{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "NaFecMlwFu0M",
        "8qeBv36fri5L"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Panda Python API\n",
        "# https://projects.saifsidhik.page/panda_robot/index.html\n",
        "# ! pip install panda-robot"
      ],
      "metadata": {
        "id": "t7ZigHs04ZnB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "import csv\n",
        "import numpy as np\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.models import load_model\n",
        "import joblib\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n"
      ],
      "metadata": {
        "id": "ouxbm40nx9cX"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MPPI-generated dataset"
      ],
      "metadata": {
        "id": "rZUrHp7UPz4f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Data processing with limit filters\n",
        "filename_input = '/content/states_from_mppi_3k.csv'\n",
        "\n",
        "input = [] # 3542\n",
        "with open(filename_input, mode='r', newline='') as file:\n",
        "    reader = csv.reader(file)\n",
        "    headers = next(reader)\n",
        "    for row in reader:\n",
        "        input.append([float(r) for r in row])\n",
        "inputs = [input[i][4:] for i in range(len(input))]\n",
        "print(len(inputs))\n",
        "print(inputs[0])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V2zT2XJCClON",
        "outputId": "551aea3e-4245-4db3-ece9-c94a5cc22e8f"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3640\n",
            "[7.449632167816162, 5.616533279418945, 8.389408111572266, 2.0859339237213135, 7.619593143463135, 6.245974540710449, -0.629223108291626, 9.01976203918457, 2.123690366744995, 0.7227600812911987]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Data processing with limit filters\n",
        "filename_input = '/content/labels_3k.csv'\n",
        "\n",
        "label = [] # 3542\n",
        "with open(filename_input, mode='r', newline='') as file:\n",
        "    reader = csv.reader(file)\n",
        "    headers = next(reader)\n",
        "    for row in reader:\n",
        "        label.append([float(r) for r in row])\n",
        "\n",
        "labels = [label[i][-1] for i in range(len(label))]\n",
        "print(len(labels))\n",
        "print(labels[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WR62JbxVXM6Y",
        "outputId": "3a595b61-1634-4f05-caea-3ad52f35076e"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3640\n",
            "0.4364641789481521\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# inputs = [input[i][4:] for i in range(len(input))]\n",
        "# filter = [[0.0, 10.0],\n",
        "#           [0.0, 10.0],\n",
        "#           [-10.0, 10.0],\n",
        "#           [-10.0, 10.0],\n",
        "#           [-20.0, 20.0],\n",
        "#           [-20.0, 20.0],\n",
        "#           [-math.pi/2, math.pi/2],\n",
        "#           [-5.0, 5.0],\n",
        "#           [-5.0, 5.0],\n",
        "#           [-math.pi/3, math.pi/3],\n",
        "#           ]\n"
      ],
      "metadata": {
        "id": "amFeAi72HdCY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# idx = []\n",
        "# for k, data in enumerate(inputs):\n",
        "#     bool_list = [(data[i] > filter[i][0] and data[i] < filter[i][1]) for i in range(len(filter))]\n",
        "#     if False not in bool_list:\n",
        "#         idx.append(k)\n",
        "# filtered_inputs = [[count,]+inputs[i] for count, i in enumerate(idx)]\n",
        "# print(len(filtered_inputs))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mr7Ad737MOZV",
        "outputId": "30b56dbd-6360-4f29-b673-47bf7d11d354"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "14818\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # Save data to a CSV file with headers\n",
        "# headers = ['data_id', 'xo', 'yo', 'vxo', 'vyo', 'xg', 'yg', 'thetag', 'vxg', 'vyg', 'omegag']\n",
        "# filename = 'filtered_states_from_mppi.csv'\n",
        "# with open(filename, mode='w', newline='') as file:\n",
        "#     writer = csv.writer(file)\n",
        "#     writer.writerow(headers)\n",
        "#     writer.writerows(filtered_inputs)"
      ],
      "metadata": {
        "id": "5Q7Ez3fCOpth"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "xo = [inputs[i][6] for i in range(len(inputs))]\n",
        "# xo = [labels[i] for i in range(len(labels))]\n",
        "\n",
        "# Create the histogram\n",
        "plt.hist(xo, bins=40, edgecolor='black')  # You can adjust the number of bins\n",
        "\n",
        "# Adding labels and title\n",
        "plt.xlabel('Value')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Histogram of Float Numbers Between 0 and 5')\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "J6Zw-HqRHE69",
        "outputId": "ccdcfca9-c4df-43be-da90-6b433689dc16"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Initial dataset"
      ],
      "metadata": {
        "id": "NaFecMlwFu0M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Data processing\n",
        "filename_input = 'data_points_O.csv'\n",
        "filename_label = 'ao_rrt-12s-varyingGoal.csv'\n",
        "\n",
        "input = [] # 3542\n",
        "with open(filename_input, mode='r', newline='') as file:\n",
        "    reader = csv.reader(file)\n",
        "    headers = next(reader)\n",
        "    for row in reader:\n",
        "        input.append([float(r) for r in row])\n",
        "\n",
        "label = []\n",
        "with open(filename_label, mode='r', newline='') as file:\n",
        "    reader = csv.reader(file)\n",
        "    headers = next(reader)\n",
        "    for row in reader:\n",
        "        label.append([float(r) for r in row])\n",
        "\n",
        "noninf_idx = []\n",
        "cutoff_hres = 4.5\n",
        "for i,l in enumerate(label):\n",
        "    if l[4] != float('inf') and l[4] < cutoff_hres: noninf_idx.append(i)\n",
        "\n",
        "inputs = [input[i][1:] for i in noninf_idx] # 2802*10\n",
        "labels = [label[i][-1] for i in noninf_idx] # 2802"
      ],
      "metadata": {
        "id": "0PNdy2IeyMtb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = np.asarray(inputs)\n",
        "labels = np.asarray(labels)\n",
        "print(inputs.shape)\n",
        "print(labels.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TSAH7MGczgHq",
        "outputId": "277fc087-b6a2-4c7d-be10-dfbd5bb61d9d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(3044, 10)\n",
            "(3044,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Create the histogram\n",
        "plt.hist(labels, bins=20, edgecolor='black')  # You can adjust the number of bins\n",
        "\n",
        "# Adding labels and title\n",
        "plt.xlabel('Value')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Histogram of Float Numbers Between 0 and 5')\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "JmRdpIH1W1BC",
        "outputId": "ff74eeea-1895-4193-f9e5-bc603c54a882"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TENSORFLOW"
      ],
      "metadata": {
        "id": "8qeBv36fri5L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming X is your input data and y is your output data\n",
        "X_train, X_test, y_train, y_test = train_test_split(inputs, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Min-Max Scaling\n",
        "min_max_scaler = MinMaxScaler()\n",
        "X_train_minmax = min_max_scaler.fit_transform(X_train)\n",
        "X_test_minmax = min_max_scaler.transform(X_test)\n",
        "joblib.dump(min_max_scaler, 'scaler_minmax.pkl')\n",
        "\n",
        "# Standardization\n",
        "standard_scaler = StandardScaler()\n",
        "X_train_standard = standard_scaler.fit_transform(X_train)\n",
        "X_test_standard = standard_scaler.transform(X_test)\n",
        "\n",
        "# # Normalize the data\n",
        "# scaler = StandardScaler()\n",
        "# X_train = scaler.fit(X_train)\n",
        "# X_test = scaler.transform(X_test[:2])"
      ],
      "metadata": {
        "id": "-yi4fyPzkg43"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "joblib.dump(min_max_scaler, 'scaler_minmax.pkl')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oZnawUnZkGH5",
        "outputId": "f644e8d7-1e0b-4716-b768-9bf0420583cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['scaler_minmax.pkl']"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # Load the scaler\n",
        "# scaler0 = joblib.load('my_scaler.pkl')\n",
        "\n",
        "# # Apply the scaler to new data\n",
        "# X_new_transformed = scaler0.transform(X_test[:2])\n",
        "# X_new_transformed"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AI9KS0OgkaWX",
        "outputId": "cb5472c6-f9a5-43c2-c363-40ec73d83228"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-1.63852813, -0.50929621,  0.08382842, -0.13254498, -1.53643876,\n",
              "        -0.48864586,  0.00356794,  1.42156081,  0.02362815, -0.37003961],\n",
              "       [-1.4017682 , -0.41107987, -0.19069578,  0.03995952, -1.37368982,\n",
              "        -0.38683071,  0.00732365,  1.2256062 ,  0.1998657 ,  0.13719381]])"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CGG6dugcxzEw",
        "outputId": "7a7012c2-aab9-4a4f-e4ad-e665be825fe4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/300\n",
            "65/65 - 2s - loss: 0.8649 - val_loss: 0.5981 - 2s/epoch - 35ms/step\n",
            "Epoch 2/300\n",
            "65/65 - 0s - loss: 0.5417 - val_loss: 0.5303 - 294ms/epoch - 5ms/step\n",
            "Epoch 3/300\n",
            "65/65 - 0s - loss: 0.4823 - val_loss: 0.4703 - 255ms/epoch - 4ms/step\n",
            "Epoch 4/300\n",
            "65/65 - 0s - loss: 0.4376 - val_loss: 0.4331 - 250ms/epoch - 4ms/step\n",
            "Epoch 5/300\n",
            "65/65 - 0s - loss: 0.4111 - val_loss: 0.4090 - 262ms/epoch - 4ms/step\n",
            "Epoch 6/300\n",
            "65/65 - 0s - loss: 0.3882 - val_loss: 0.3820 - 247ms/epoch - 4ms/step\n",
            "Epoch 7/300\n",
            "65/65 - 0s - loss: 0.3647 - val_loss: 0.3586 - 251ms/epoch - 4ms/step\n",
            "Epoch 8/300\n",
            "65/65 - 0s - loss: 0.3376 - val_loss: 0.3466 - 261ms/epoch - 4ms/step\n",
            "Epoch 9/300\n",
            "65/65 - 0s - loss: 0.3149 - val_loss: 0.3181 - 306ms/epoch - 5ms/step\n",
            "Epoch 10/300\n",
            "65/65 - 0s - loss: 0.2921 - val_loss: 0.3065 - 258ms/epoch - 4ms/step\n",
            "Epoch 11/300\n",
            "65/65 - 0s - loss: 0.2702 - val_loss: 0.3079 - 365ms/epoch - 6ms/step\n",
            "Epoch 12/300\n",
            "65/65 - 0s - loss: 0.2528 - val_loss: 0.2541 - 377ms/epoch - 6ms/step\n",
            "Epoch 13/300\n",
            "65/65 - 0s - loss: 0.2378 - val_loss: 0.2461 - 397ms/epoch - 6ms/step\n",
            "Epoch 14/300\n",
            "65/65 - 0s - loss: 0.2206 - val_loss: 0.2379 - 388ms/epoch - 6ms/step\n",
            "Epoch 15/300\n",
            "65/65 - 0s - loss: 0.2079 - val_loss: 0.2212 - 424ms/epoch - 7ms/step\n",
            "Epoch 16/300\n",
            "65/65 - 0s - loss: 0.1967 - val_loss: 0.2304 - 421ms/epoch - 6ms/step\n",
            "Epoch 17/300\n",
            "65/65 - 0s - loss: 0.1879 - val_loss: 0.2054 - 373ms/epoch - 6ms/step\n",
            "Epoch 18/300\n",
            "65/65 - 0s - loss: 0.1802 - val_loss: 0.2107 - 369ms/epoch - 6ms/step\n",
            "Epoch 19/300\n",
            "65/65 - 0s - loss: 0.1786 - val_loss: 0.2091 - 418ms/epoch - 6ms/step\n",
            "Epoch 20/300\n",
            "65/65 - 0s - loss: 0.1737 - val_loss: 0.1832 - 284ms/epoch - 4ms/step\n",
            "Epoch 21/300\n",
            "65/65 - 0s - loss: 0.1654 - val_loss: 0.1985 - 257ms/epoch - 4ms/step\n",
            "Epoch 22/300\n",
            "65/65 - 0s - loss: 0.1587 - val_loss: 0.2069 - 256ms/epoch - 4ms/step\n",
            "Epoch 23/300\n",
            "65/65 - 0s - loss: 0.1539 - val_loss: 0.1742 - 254ms/epoch - 4ms/step\n",
            "Epoch 24/300\n",
            "65/65 - 0s - loss: 0.1479 - val_loss: 0.1726 - 258ms/epoch - 4ms/step\n",
            "Epoch 25/300\n",
            "65/65 - 0s - loss: 0.1473 - val_loss: 0.1896 - 247ms/epoch - 4ms/step\n",
            "Epoch 26/300\n",
            "65/65 - 0s - loss: 0.1412 - val_loss: 0.1852 - 247ms/epoch - 4ms/step\n",
            "Epoch 27/300\n",
            "65/65 - 0s - loss: 0.1378 - val_loss: 0.2029 - 287ms/epoch - 4ms/step\n",
            "Epoch 28/300\n",
            "65/65 - 0s - loss: 0.1360 - val_loss: 0.1785 - 304ms/epoch - 5ms/step\n",
            "Epoch 29/300\n",
            "65/65 - 0s - loss: 0.1337 - val_loss: 0.1562 - 262ms/epoch - 4ms/step\n",
            "Epoch 30/300\n",
            "65/65 - 0s - loss: 0.1319 - val_loss: 0.1718 - 251ms/epoch - 4ms/step\n",
            "Epoch 31/300\n",
            "65/65 - 0s - loss: 0.1250 - val_loss: 0.1688 - 266ms/epoch - 4ms/step\n",
            "Epoch 32/300\n",
            "65/65 - 0s - loss: 0.1220 - val_loss: 0.1624 - 256ms/epoch - 4ms/step\n",
            "Epoch 33/300\n",
            "65/65 - 0s - loss: 0.1277 - val_loss: 0.2058 - 259ms/epoch - 4ms/step\n",
            "Epoch 34/300\n",
            "65/65 - 0s - loss: 0.1255 - val_loss: 0.1559 - 272ms/epoch - 4ms/step\n",
            "Epoch 35/300\n",
            "65/65 - 0s - loss: 0.1192 - val_loss: 0.1464 - 323ms/epoch - 5ms/step\n",
            "Epoch 36/300\n",
            "65/65 - 0s - loss: 0.1210 - val_loss: 0.1522 - 303ms/epoch - 5ms/step\n",
            "Epoch 37/300\n",
            "65/65 - 0s - loss: 0.1148 - val_loss: 0.1477 - 264ms/epoch - 4ms/step\n",
            "Epoch 38/300\n",
            "65/65 - 0s - loss: 0.1116 - val_loss: 0.1493 - 270ms/epoch - 4ms/step\n",
            "Epoch 39/300\n",
            "65/65 - 0s - loss: 0.1149 - val_loss: 0.1510 - 314ms/epoch - 5ms/step\n",
            "Epoch 40/300\n",
            "65/65 - 0s - loss: 0.1119 - val_loss: 0.1526 - 248ms/epoch - 4ms/step\n",
            "Epoch 41/300\n",
            "65/65 - 0s - loss: 0.1060 - val_loss: 0.1445 - 291ms/epoch - 4ms/step\n",
            "Epoch 42/300\n",
            "65/65 - 0s - loss: 0.1107 - val_loss: 0.1482 - 281ms/epoch - 4ms/step\n",
            "Epoch 43/300\n",
            "65/65 - 0s - loss: 0.1043 - val_loss: 0.1501 - 260ms/epoch - 4ms/step\n",
            "Epoch 44/300\n",
            "65/65 - 0s - loss: 0.1035 - val_loss: 0.1382 - 255ms/epoch - 4ms/step\n",
            "Epoch 45/300\n",
            "65/65 - 0s - loss: 0.0980 - val_loss: 0.1353 - 295ms/epoch - 5ms/step\n",
            "Epoch 46/300\n",
            "65/65 - 0s - loss: 0.0982 - val_loss: 0.1744 - 301ms/epoch - 5ms/step\n",
            "Epoch 47/300\n",
            "65/65 - 0s - loss: 0.0977 - val_loss: 0.1501 - 247ms/epoch - 4ms/step\n",
            "Epoch 48/300\n",
            "65/65 - 0s - loss: 0.0974 - val_loss: 0.1497 - 295ms/epoch - 5ms/step\n",
            "Epoch 49/300\n",
            "65/65 - 0s - loss: 0.0938 - val_loss: 0.1805 - 270ms/epoch - 4ms/step\n",
            "Epoch 50/300\n",
            "65/65 - 0s - loss: 0.0986 - val_loss: 0.1592 - 258ms/epoch - 4ms/step\n",
            "Epoch 51/300\n",
            "65/65 - 0s - loss: 0.0918 - val_loss: 0.1361 - 277ms/epoch - 4ms/step\n",
            "Epoch 52/300\n",
            "65/65 - 0s - loss: 0.0895 - val_loss: 0.1317 - 300ms/epoch - 5ms/step\n",
            "Epoch 53/300\n",
            "65/65 - 0s - loss: 0.0951 - val_loss: 0.1417 - 264ms/epoch - 4ms/step\n",
            "Epoch 54/300\n",
            "65/65 - 0s - loss: 0.0923 - val_loss: 0.1400 - 252ms/epoch - 4ms/step\n",
            "Epoch 55/300\n",
            "65/65 - 0s - loss: 0.0873 - val_loss: 0.1426 - 254ms/epoch - 4ms/step\n",
            "Epoch 56/300\n",
            "65/65 - 0s - loss: 0.0900 - val_loss: 0.1407 - 374ms/epoch - 6ms/step\n",
            "Epoch 57/300\n",
            "65/65 - 0s - loss: 0.0843 - val_loss: 0.1365 - 414ms/epoch - 6ms/step\n",
            "Epoch 58/300\n",
            "65/65 - 0s - loss: 0.0851 - val_loss: 0.1262 - 391ms/epoch - 6ms/step\n",
            "Epoch 59/300\n",
            "65/65 - 0s - loss: 0.0804 - val_loss: 0.1342 - 363ms/epoch - 6ms/step\n",
            "Epoch 60/300\n",
            "65/65 - 0s - loss: 0.0852 - val_loss: 0.1474 - 388ms/epoch - 6ms/step\n",
            "Epoch 61/300\n",
            "65/65 - 0s - loss: 0.0811 - val_loss: 0.1391 - 367ms/epoch - 6ms/step\n",
            "Epoch 62/300\n",
            "65/65 - 0s - loss: 0.0786 - val_loss: 0.1302 - 396ms/epoch - 6ms/step\n",
            "Epoch 63/300\n",
            "65/65 - 0s - loss: 0.0810 - val_loss: 0.1505 - 383ms/epoch - 6ms/step\n",
            "Epoch 64/300\n",
            "65/65 - 0s - loss: 0.0807 - val_loss: 0.1371 - 388ms/epoch - 6ms/step\n",
            "Epoch 65/300\n",
            "65/65 - 0s - loss: 0.0773 - val_loss: 0.1339 - 312ms/epoch - 5ms/step\n",
            "Epoch 66/300\n",
            "65/65 - 0s - loss: 0.0775 - val_loss: 0.1238 - 259ms/epoch - 4ms/step\n",
            "Epoch 67/300\n",
            "65/65 - 0s - loss: 0.0777 - val_loss: 0.1539 - 277ms/epoch - 4ms/step\n",
            "Epoch 68/300\n",
            "65/65 - 0s - loss: 0.0827 - val_loss: 0.1303 - 255ms/epoch - 4ms/step\n",
            "Epoch 69/300\n",
            "65/65 - 0s - loss: 0.0741 - val_loss: 0.1339 - 283ms/epoch - 4ms/step\n",
            "Epoch 70/300\n",
            "65/65 - 0s - loss: 0.0745 - val_loss: 0.1576 - 239ms/epoch - 4ms/step\n",
            "Epoch 71/300\n",
            "65/65 - 0s - loss: 0.0779 - val_loss: 0.1333 - 280ms/epoch - 4ms/step\n",
            "Epoch 72/300\n",
            "65/65 - 0s - loss: 0.0750 - val_loss: 0.1441 - 255ms/epoch - 4ms/step\n",
            "Epoch 73/300\n",
            "65/65 - 0s - loss: 0.0736 - val_loss: 0.1359 - 251ms/epoch - 4ms/step\n",
            "Epoch 74/300\n",
            "65/65 - 0s - loss: 0.0683 - val_loss: 0.1484 - 290ms/epoch - 4ms/step\n",
            "Epoch 75/300\n",
            "65/65 - 0s - loss: 0.0695 - val_loss: 0.1323 - 246ms/epoch - 4ms/step\n",
            "Epoch 76/300\n",
            "65/65 - 0s - loss: 0.0673 - val_loss: 0.1255 - 256ms/epoch - 4ms/step\n",
            "Epoch 77/300\n",
            "65/65 - 0s - loss: 0.0662 - val_loss: 0.1475 - 250ms/epoch - 4ms/step\n",
            "Epoch 78/300\n",
            "65/65 - 0s - loss: 0.0648 - val_loss: 0.1268 - 246ms/epoch - 4ms/step\n",
            "Epoch 79/300\n",
            "65/65 - 0s - loss: 0.0688 - val_loss: 0.1226 - 267ms/epoch - 4ms/step\n",
            "Epoch 80/300\n",
            "65/65 - 0s - loss: 0.0655 - val_loss: 0.1256 - 256ms/epoch - 4ms/step\n",
            "Epoch 81/300\n",
            "65/65 - 0s - loss: 0.0661 - val_loss: 0.1260 - 284ms/epoch - 4ms/step\n",
            "Epoch 82/300\n",
            "65/65 - 0s - loss: 0.0622 - val_loss: 0.1318 - 288ms/epoch - 4ms/step\n",
            "Epoch 83/300\n",
            "65/65 - 0s - loss: 0.0709 - val_loss: 0.1313 - 286ms/epoch - 4ms/step\n",
            "Epoch 84/300\n",
            "65/65 - 0s - loss: 0.0648 - val_loss: 0.1188 - 285ms/epoch - 4ms/step\n",
            "Epoch 85/300\n",
            "65/65 - 0s - loss: 0.0640 - val_loss: 0.1352 - 269ms/epoch - 4ms/step\n",
            "Epoch 86/300\n",
            "65/65 - 0s - loss: 0.0622 - val_loss: 0.1245 - 292ms/epoch - 4ms/step\n",
            "Epoch 87/300\n",
            "65/65 - 0s - loss: 0.0658 - val_loss: 0.1255 - 254ms/epoch - 4ms/step\n",
            "Epoch 88/300\n",
            "65/65 - 0s - loss: 0.0612 - val_loss: 0.1272 - 267ms/epoch - 4ms/step\n",
            "Epoch 89/300\n",
            "65/65 - 0s - loss: 0.0633 - val_loss: 0.1544 - 262ms/epoch - 4ms/step\n",
            "Epoch 90/300\n",
            "65/65 - 0s - loss: 0.0587 - val_loss: 0.1278 - 261ms/epoch - 4ms/step\n",
            "Epoch 91/300\n",
            "65/65 - 0s - loss: 0.0623 - val_loss: 0.1312 - 257ms/epoch - 4ms/step\n",
            "Epoch 92/300\n",
            "65/65 - 0s - loss: 0.0595 - val_loss: 0.1279 - 256ms/epoch - 4ms/step\n",
            "Epoch 93/300\n",
            "65/65 - 0s - loss: 0.0597 - val_loss: 0.1400 - 261ms/epoch - 4ms/step\n",
            "Epoch 94/300\n",
            "65/65 - 0s - loss: 0.0616 - val_loss: 0.1231 - 257ms/epoch - 4ms/step\n",
            "Epoch 95/300\n",
            "65/65 - 0s - loss: 0.0568 - val_loss: 0.1226 - 303ms/epoch - 5ms/step\n",
            "Epoch 96/300\n",
            "65/65 - 0s - loss: 0.0579 - val_loss: 0.1183 - 269ms/epoch - 4ms/step\n",
            "Epoch 97/300\n",
            "65/65 - 0s - loss: 0.0598 - val_loss: 0.1263 - 291ms/epoch - 4ms/step\n",
            "Epoch 98/300\n",
            "65/65 - 0s - loss: 0.0549 - val_loss: 0.1302 - 304ms/epoch - 5ms/step\n",
            "Epoch 99/300\n",
            "65/65 - 0s - loss: 0.0543 - val_loss: 0.1313 - 258ms/epoch - 4ms/step\n",
            "Epoch 100/300\n",
            "65/65 - 0s - loss: 0.0524 - val_loss: 0.1312 - 262ms/epoch - 4ms/step\n",
            "Epoch 101/300\n",
            "65/65 - 0s - loss: 0.0520 - val_loss: 0.1678 - 265ms/epoch - 4ms/step\n",
            "Epoch 102/300\n",
            "65/65 - 0s - loss: 0.0641 - val_loss: 0.1358 - 432ms/epoch - 7ms/step\n",
            "Epoch 103/300\n",
            "65/65 - 0s - loss: 0.0515 - val_loss: 0.1257 - 418ms/epoch - 6ms/step\n",
            "Epoch 104/300\n",
            "65/65 - 0s - loss: 0.0530 - val_loss: 0.1276 - 370ms/epoch - 6ms/step\n",
            "Epoch 105/300\n",
            "65/65 - 0s - loss: 0.0520 - val_loss: 0.1202 - 365ms/epoch - 6ms/step\n",
            "Epoch 106/300\n",
            "65/65 - 0s - loss: 0.0517 - val_loss: 0.1307 - 396ms/epoch - 6ms/step\n",
            "Epoch 107/300\n",
            "65/65 - 0s - loss: 0.0488 - val_loss: 0.1290 - 410ms/epoch - 6ms/step\n",
            "Epoch 108/300\n",
            "65/65 - 0s - loss: 0.0506 - val_loss: 0.1236 - 375ms/epoch - 6ms/step\n",
            "Epoch 109/300\n",
            "65/65 - 0s - loss: 0.0507 - val_loss: 0.1301 - 389ms/epoch - 6ms/step\n",
            "Epoch 110/300\n",
            "65/65 - 0s - loss: 0.0479 - val_loss: 0.1249 - 397ms/epoch - 6ms/step\n",
            "Epoch 111/300\n",
            "65/65 - 0s - loss: 0.0535 - val_loss: 0.1242 - 239ms/epoch - 4ms/step\n",
            "Epoch 112/300\n",
            "65/65 - 0s - loss: 0.0527 - val_loss: 0.1207 - 240ms/epoch - 4ms/step\n",
            "Epoch 113/300\n",
            "65/65 - 0s - loss: 0.0453 - val_loss: 0.1224 - 253ms/epoch - 4ms/step\n",
            "Epoch 114/300\n",
            "65/65 - 0s - loss: 0.0503 - val_loss: 0.1212 - 250ms/epoch - 4ms/step\n",
            "Epoch 115/300\n",
            "65/65 - 0s - loss: 0.0503 - val_loss: 0.1219 - 281ms/epoch - 4ms/step\n",
            "Epoch 116/300\n",
            "65/65 - 0s - loss: 0.0497 - val_loss: 0.1149 - 246ms/epoch - 4ms/step\n",
            "Epoch 117/300\n",
            "65/65 - 0s - loss: 0.0440 - val_loss: 0.1183 - 290ms/epoch - 4ms/step\n",
            "Epoch 118/300\n",
            "65/65 - 0s - loss: 0.0487 - val_loss: 0.1121 - 280ms/epoch - 4ms/step\n",
            "Epoch 119/300\n",
            "65/65 - 0s - loss: 0.0444 - val_loss: 0.1122 - 248ms/epoch - 4ms/step\n",
            "Epoch 120/300\n",
            "65/65 - 0s - loss: 0.0434 - val_loss: 0.1331 - 244ms/epoch - 4ms/step\n",
            "Epoch 121/300\n",
            "65/65 - 0s - loss: 0.0474 - val_loss: 0.1171 - 296ms/epoch - 5ms/step\n",
            "Epoch 122/300\n",
            "65/65 - 0s - loss: 0.0438 - val_loss: 0.1189 - 287ms/epoch - 4ms/step\n",
            "Epoch 123/300\n",
            "65/65 - 0s - loss: 0.0448 - val_loss: 0.1305 - 251ms/epoch - 4ms/step\n",
            "Epoch 124/300\n",
            "65/65 - 0s - loss: 0.0438 - val_loss: 0.1120 - 254ms/epoch - 4ms/step\n",
            "Epoch 125/300\n",
            "65/65 - 0s - loss: 0.0506 - val_loss: 0.1168 - 250ms/epoch - 4ms/step\n",
            "Epoch 126/300\n",
            "65/65 - 0s - loss: 0.0489 - val_loss: 0.1255 - 287ms/epoch - 4ms/step\n",
            "Epoch 127/300\n",
            "65/65 - 0s - loss: 0.0404 - val_loss: 0.1463 - 246ms/epoch - 4ms/step\n",
            "Epoch 128/300\n",
            "65/65 - 0s - loss: 0.0473 - val_loss: 0.1156 - 254ms/epoch - 4ms/step\n",
            "Epoch 129/300\n",
            "65/65 - 0s - loss: 0.0434 - val_loss: 0.1290 - 259ms/epoch - 4ms/step\n",
            "Epoch 130/300\n",
            "65/65 - 0s - loss: 0.0409 - val_loss: 0.1214 - 286ms/epoch - 4ms/step\n",
            "Epoch 131/300\n",
            "65/65 - 0s - loss: 0.0382 - val_loss: 0.1157 - 253ms/epoch - 4ms/step\n",
            "Epoch 132/300\n",
            "65/65 - 0s - loss: 0.0383 - val_loss: 0.1258 - 261ms/epoch - 4ms/step\n",
            "Epoch 133/300\n",
            "65/65 - 0s - loss: 0.0404 - val_loss: 0.1218 - 295ms/epoch - 5ms/step\n",
            "Epoch 134/300\n",
            "65/65 - 0s - loss: 0.0475 - val_loss: 0.1191 - 251ms/epoch - 4ms/step\n",
            "Epoch 135/300\n",
            "65/65 - 0s - loss: 0.0399 - val_loss: 0.1175 - 246ms/epoch - 4ms/step\n",
            "Epoch 136/300\n",
            "65/65 - 0s - loss: 0.0406 - val_loss: 0.1221 - 256ms/epoch - 4ms/step\n",
            "Epoch 137/300\n",
            "65/65 - 0s - loss: 0.0403 - val_loss: 0.1140 - 271ms/epoch - 4ms/step\n",
            "Epoch 138/300\n",
            "65/65 - 0s - loss: 0.0391 - val_loss: 0.1175 - 265ms/epoch - 4ms/step\n",
            "Epoch 139/300\n",
            "65/65 - 0s - loss: 0.0425 - val_loss: 0.1290 - 256ms/epoch - 4ms/step\n",
            "Epoch 140/300\n",
            "65/65 - 0s - loss: 0.0401 - val_loss: 0.1187 - 275ms/epoch - 4ms/step\n",
            "Epoch 141/300\n",
            "65/65 - 0s - loss: 0.0390 - val_loss: 0.1143 - 253ms/epoch - 4ms/step\n",
            "Epoch 142/300\n",
            "65/65 - 0s - loss: 0.0375 - val_loss: 0.1168 - 246ms/epoch - 4ms/step\n",
            "Epoch 143/300\n",
            "65/65 - 0s - loss: 0.0404 - val_loss: 0.1365 - 250ms/epoch - 4ms/step\n",
            "Epoch 144/300\n",
            "65/65 - 0s - loss: 0.0419 - val_loss: 0.1215 - 259ms/epoch - 4ms/step\n",
            "Epoch 145/300\n",
            "65/65 - 0s - loss: 0.0382 - val_loss: 0.1255 - 290ms/epoch - 4ms/step\n",
            "Epoch 146/300\n",
            "65/65 - 0s - loss: 0.0369 - val_loss: 0.1229 - 247ms/epoch - 4ms/step\n",
            "Epoch 147/300\n",
            "65/65 - 0s - loss: 0.0399 - val_loss: 0.1166 - 255ms/epoch - 4ms/step\n",
            "Epoch 148/300\n",
            "65/65 - 0s - loss: 0.0396 - val_loss: 0.1263 - 361ms/epoch - 6ms/step\n",
            "Epoch 149/300\n",
            "65/65 - 0s - loss: 0.0429 - val_loss: 0.1275 - 393ms/epoch - 6ms/step\n",
            "Epoch 150/300\n",
            "65/65 - 0s - loss: 0.0397 - val_loss: 0.1193 - 366ms/epoch - 6ms/step\n",
            "Epoch 151/300\n",
            "65/65 - 0s - loss: 0.0385 - val_loss: 0.1160 - 349ms/epoch - 5ms/step\n",
            "Epoch 152/300\n",
            "65/65 - 0s - loss: 0.0352 - val_loss: 0.1160 - 378ms/epoch - 6ms/step\n",
            "Epoch 153/300\n",
            "65/65 - 0s - loss: 0.0341 - val_loss: 0.1166 - 401ms/epoch - 6ms/step\n",
            "Epoch 154/300\n",
            "65/65 - 0s - loss: 0.0339 - val_loss: 0.1165 - 385ms/epoch - 6ms/step\n",
            "Epoch 155/300\n",
            "65/65 - 0s - loss: 0.0393 - val_loss: 0.1202 - 367ms/epoch - 6ms/step\n",
            "Epoch 156/300\n",
            "65/65 - 0s - loss: 0.0366 - val_loss: 0.1225 - 382ms/epoch - 6ms/step\n",
            "Epoch 157/300\n",
            "65/65 - 0s - loss: 0.0375 - val_loss: 0.1121 - 331ms/epoch - 5ms/step\n",
            "Epoch 158/300\n",
            "65/65 - 0s - loss: 0.0344 - val_loss: 0.1140 - 251ms/epoch - 4ms/step\n",
            "Epoch 159/300\n",
            "65/65 - 0s - loss: 0.0346 - val_loss: 0.1384 - 299ms/epoch - 5ms/step\n",
            "Epoch 160/300\n",
            "65/65 - 0s - loss: 0.0368 - val_loss: 0.1074 - 242ms/epoch - 4ms/step\n",
            "Epoch 161/300\n",
            "65/65 - 0s - loss: 0.0336 - val_loss: 0.1129 - 243ms/epoch - 4ms/step\n",
            "Epoch 162/300\n",
            "65/65 - 0s - loss: 0.0380 - val_loss: 0.1329 - 255ms/epoch - 4ms/step\n",
            "Epoch 163/300\n",
            "65/65 - 0s - loss: 0.0414 - val_loss: 0.1329 - 296ms/epoch - 5ms/step\n",
            "Epoch 164/300\n",
            "65/65 - 0s - loss: 0.0422 - val_loss: 0.1213 - 249ms/epoch - 4ms/step\n",
            "Epoch 165/300\n",
            "65/65 - 0s - loss: 0.0325 - val_loss: 0.1196 - 243ms/epoch - 4ms/step\n",
            "Epoch 166/300\n",
            "65/65 - 0s - loss: 0.0316 - val_loss: 0.1151 - 247ms/epoch - 4ms/step\n",
            "Epoch 167/300\n",
            "65/65 - 0s - loss: 0.0347 - val_loss: 0.1156 - 261ms/epoch - 4ms/step\n",
            "Epoch 168/300\n",
            "65/65 - 0s - loss: 0.0332 - val_loss: 0.1176 - 277ms/epoch - 4ms/step\n",
            "Epoch 169/300\n",
            "65/65 - 0s - loss: 0.0319 - val_loss: 0.1159 - 252ms/epoch - 4ms/step\n",
            "Epoch 170/300\n",
            "65/65 - 0s - loss: 0.0312 - val_loss: 0.1344 - 300ms/epoch - 5ms/step\n",
            "Epoch 171/300\n",
            "65/65 - 0s - loss: 0.0413 - val_loss: 0.1142 - 284ms/epoch - 4ms/step\n",
            "Epoch 172/300\n",
            "65/65 - 0s - loss: 0.0355 - val_loss: 0.1174 - 250ms/epoch - 4ms/step\n",
            "Epoch 173/300\n",
            "65/65 - 0s - loss: 0.0302 - val_loss: 0.1153 - 282ms/epoch - 4ms/step\n",
            "Epoch 174/300\n",
            "65/65 - 0s - loss: 0.0301 - val_loss: 0.1108 - 264ms/epoch - 4ms/step\n",
            "Epoch 175/300\n",
            "65/65 - 0s - loss: 0.0326 - val_loss: 0.1126 - 246ms/epoch - 4ms/step\n",
            "Epoch 176/300\n",
            "65/65 - 0s - loss: 0.0341 - val_loss: 0.1180 - 241ms/epoch - 4ms/step\n",
            "Epoch 177/300\n",
            "65/65 - 0s - loss: 0.0363 - val_loss: 0.1332 - 281ms/epoch - 4ms/step\n",
            "Epoch 178/300\n",
            "65/65 - 0s - loss: 0.0327 - val_loss: 0.1357 - 271ms/epoch - 4ms/step\n",
            "Epoch 179/300\n",
            "65/65 - 0s - loss: 0.0332 - val_loss: 0.1174 - 247ms/epoch - 4ms/step\n",
            "Epoch 180/300\n",
            "65/65 - 0s - loss: 0.0317 - val_loss: 0.1240 - 252ms/epoch - 4ms/step\n",
            "Epoch 181/300\n",
            "65/65 - 0s - loss: 0.0303 - val_loss: 0.1151 - 296ms/epoch - 5ms/step\n",
            "Epoch 182/300\n",
            "65/65 - 0s - loss: 0.0339 - val_loss: 0.1118 - 265ms/epoch - 4ms/step\n",
            "Epoch 183/300\n",
            "65/65 - 0s - loss: 0.0330 - val_loss: 0.1160 - 291ms/epoch - 4ms/step\n",
            "Epoch 184/300\n",
            "65/65 - 0s - loss: 0.0310 - val_loss: 0.1137 - 257ms/epoch - 4ms/step\n",
            "Epoch 185/300\n",
            "65/65 - 0s - loss: 0.0301 - val_loss: 0.1062 - 261ms/epoch - 4ms/step\n",
            "Epoch 186/300\n",
            "65/65 - 0s - loss: 0.0312 - val_loss: 0.1071 - 245ms/epoch - 4ms/step\n",
            "Epoch 187/300\n",
            "65/65 - 0s - loss: 0.0304 - val_loss: 0.1136 - 245ms/epoch - 4ms/step\n",
            "Epoch 188/300\n",
            "65/65 - 0s - loss: 0.0294 - val_loss: 0.1189 - 255ms/epoch - 4ms/step\n",
            "Epoch 189/300\n",
            "65/65 - 0s - loss: 0.0307 - val_loss: 0.1058 - 260ms/epoch - 4ms/step\n",
            "Epoch 190/300\n",
            "65/65 - 0s - loss: 0.0324 - val_loss: 0.1165 - 251ms/epoch - 4ms/step\n",
            "Epoch 191/300\n",
            "65/65 - 0s - loss: 0.0289 - val_loss: 0.1143 - 245ms/epoch - 4ms/step\n",
            "Epoch 192/300\n",
            "65/65 - 0s - loss: 0.0291 - val_loss: 0.1158 - 274ms/epoch - 4ms/step\n",
            "Epoch 193/300\n",
            "65/65 - 0s - loss: 0.0307 - val_loss: 0.1124 - 264ms/epoch - 4ms/step\n",
            "Epoch 194/300\n",
            "65/65 - 0s - loss: 0.0305 - val_loss: 0.1083 - 254ms/epoch - 4ms/step\n",
            "Epoch 195/300\n",
            "65/65 - 0s - loss: 0.0286 - val_loss: 0.1089 - 396ms/epoch - 6ms/step\n",
            "Epoch 196/300\n",
            "65/65 - 0s - loss: 0.0272 - val_loss: 0.1083 - 411ms/epoch - 6ms/step\n",
            "Epoch 197/300\n",
            "65/65 - 0s - loss: 0.0274 - val_loss: 0.1112 - 364ms/epoch - 6ms/step\n",
            "Epoch 198/300\n",
            "65/65 - 0s - loss: 0.0291 - val_loss: 0.1234 - 362ms/epoch - 6ms/step\n",
            "Epoch 199/300\n",
            "65/65 - 0s - loss: 0.0312 - val_loss: 0.1045 - 380ms/epoch - 6ms/step\n",
            "Epoch 200/300\n",
            "65/65 - 0s - loss: 0.0360 - val_loss: 0.1136 - 448ms/epoch - 7ms/step\n",
            "Epoch 201/300\n",
            "65/65 - 0s - loss: 0.0286 - val_loss: 0.1015 - 440ms/epoch - 7ms/step\n",
            "Epoch 202/300\n",
            "65/65 - 0s - loss: 0.0274 - val_loss: 0.1219 - 375ms/epoch - 6ms/step\n",
            "Epoch 203/300\n",
            "65/65 - 0s - loss: 0.0293 - val_loss: 0.1076 - 430ms/epoch - 7ms/step\n",
            "Epoch 204/300\n",
            "65/65 - 0s - loss: 0.0270 - val_loss: 0.1103 - 349ms/epoch - 5ms/step\n",
            "Epoch 205/300\n",
            "65/65 - 0s - loss: 0.0291 - val_loss: 0.1134 - 288ms/epoch - 4ms/step\n",
            "Epoch 206/300\n",
            "65/65 - 0s - loss: 0.0367 - val_loss: 0.1022 - 268ms/epoch - 4ms/step\n",
            "Epoch 207/300\n",
            "65/65 - 0s - loss: 0.0281 - val_loss: 0.1051 - 252ms/epoch - 4ms/step\n",
            "Epoch 208/300\n",
            "65/65 - 0s - loss: 0.0279 - val_loss: 0.1084 - 252ms/epoch - 4ms/step\n",
            "Epoch 209/300\n",
            "65/65 - 0s - loss: 0.0325 - val_loss: 0.1083 - 245ms/epoch - 4ms/step\n",
            "Epoch 210/300\n",
            "65/65 - 0s - loss: 0.0299 - val_loss: 0.1043 - 252ms/epoch - 4ms/step\n",
            "Epoch 211/300\n",
            "65/65 - 0s - loss: 0.0265 - val_loss: 0.1121 - 248ms/epoch - 4ms/step\n",
            "Epoch 212/300\n",
            "65/65 - 0s - loss: 0.0286 - val_loss: 0.1151 - 293ms/epoch - 5ms/step\n",
            "Epoch 213/300\n",
            "65/65 - 0s - loss: 0.0268 - val_loss: 0.1021 - 291ms/epoch - 4ms/step\n",
            "Epoch 214/300\n",
            "65/65 - 0s - loss: 0.0253 - val_loss: 0.1166 - 245ms/epoch - 4ms/step\n",
            "Epoch 215/300\n",
            "65/65 - 0s - loss: 0.0246 - val_loss: 0.1173 - 249ms/epoch - 4ms/step\n",
            "Epoch 216/300\n",
            "65/65 - 0s - loss: 0.0252 - val_loss: 0.1107 - 255ms/epoch - 4ms/step\n",
            "Epoch 217/300\n",
            "65/65 - 0s - loss: 0.0273 - val_loss: 0.1191 - 258ms/epoch - 4ms/step\n",
            "Epoch 218/300\n",
            "65/65 - 0s - loss: 0.0259 - val_loss: 0.1174 - 251ms/epoch - 4ms/step\n",
            "Epoch 219/300\n",
            "65/65 - 0s - loss: 0.0250 - val_loss: 0.1068 - 253ms/epoch - 4ms/step\n",
            "Epoch 220/300\n",
            "65/65 - 0s - loss: 0.0251 - val_loss: 0.1077 - 249ms/epoch - 4ms/step\n",
            "Epoch 221/300\n",
            "65/65 - 0s - loss: 0.0265 - val_loss: 0.1091 - 260ms/epoch - 4ms/step\n",
            "Epoch 222/300\n",
            "65/65 - 0s - loss: 0.0257 - val_loss: 0.1113 - 290ms/epoch - 4ms/step\n",
            "Epoch 223/300\n",
            "65/65 - 0s - loss: 0.0266 - val_loss: 0.1157 - 260ms/epoch - 4ms/step\n",
            "Epoch 224/300\n",
            "65/65 - 0s - loss: 0.0265 - val_loss: 0.1163 - 294ms/epoch - 5ms/step\n",
            "Epoch 225/300\n",
            "65/65 - 0s - loss: 0.0257 - val_loss: 0.1152 - 290ms/epoch - 4ms/step\n",
            "Epoch 226/300\n",
            "65/65 - 0s - loss: 0.0243 - val_loss: 0.1104 - 247ms/epoch - 4ms/step\n",
            "Epoch 227/300\n",
            "65/65 - 0s - loss: 0.0255 - val_loss: 0.1140 - 304ms/epoch - 5ms/step\n",
            "Epoch 228/300\n",
            "65/65 - 0s - loss: 0.0249 - val_loss: 0.1329 - 250ms/epoch - 4ms/step\n",
            "Epoch 229/300\n",
            "65/65 - 0s - loss: 0.0278 - val_loss: 0.1035 - 255ms/epoch - 4ms/step\n",
            "Epoch 230/300\n",
            "65/65 - 0s - loss: 0.0240 - val_loss: 0.1139 - 245ms/epoch - 4ms/step\n",
            "Epoch 231/300\n",
            "65/65 - 0s - loss: 0.0254 - val_loss: 0.1129 - 247ms/epoch - 4ms/step\n",
            "Epoch 232/300\n",
            "65/65 - 0s - loss: 0.0237 - val_loss: 0.1174 - 286ms/epoch - 4ms/step\n",
            "Epoch 233/300\n",
            "65/65 - 0s - loss: 0.0214 - val_loss: 0.1085 - 261ms/epoch - 4ms/step\n",
            "Epoch 234/300\n",
            "65/65 - 0s - loss: 0.0228 - val_loss: 0.1177 - 294ms/epoch - 5ms/step\n",
            "Epoch 235/300\n",
            "65/65 - 0s - loss: 0.0233 - val_loss: 0.1180 - 254ms/epoch - 4ms/step\n",
            "Epoch 236/300\n",
            "65/65 - 0s - loss: 0.0223 - val_loss: 0.1086 - 251ms/epoch - 4ms/step\n",
            "Epoch 237/300\n",
            "65/65 - 0s - loss: 0.0244 - val_loss: 0.1203 - 252ms/epoch - 4ms/step\n",
            "Epoch 238/300\n",
            "65/65 - 0s - loss: 0.0239 - val_loss: 0.1046 - 303ms/epoch - 5ms/step\n",
            "Epoch 239/300\n",
            "65/65 - 0s - loss: 0.0253 - val_loss: 0.1121 - 260ms/epoch - 4ms/step\n",
            "Epoch 240/300\n",
            "65/65 - 0s - loss: 0.0244 - val_loss: 0.1067 - 292ms/epoch - 4ms/step\n",
            "Epoch 241/300\n",
            "65/65 - 0s - loss: 0.0241 - val_loss: 0.1156 - 333ms/epoch - 5ms/step\n",
            "Epoch 242/300\n",
            "65/65 - 0s - loss: 0.0257 - val_loss: 0.1148 - 401ms/epoch - 6ms/step\n",
            "Epoch 243/300\n",
            "65/65 - 0s - loss: 0.0258 - val_loss: 0.1082 - 421ms/epoch - 6ms/step\n",
            "Epoch 244/300\n",
            "65/65 - 0s - loss: 0.0274 - val_loss: 0.1110 - 407ms/epoch - 6ms/step\n",
            "Epoch 245/300\n",
            "65/65 - 0s - loss: 0.0244 - val_loss: 0.1262 - 396ms/epoch - 6ms/step\n",
            "Epoch 246/300\n",
            "65/65 - 0s - loss: 0.0250 - val_loss: 0.1173 - 398ms/epoch - 6ms/step\n",
            "Epoch 247/300\n",
            "65/65 - 0s - loss: 0.0273 - val_loss: 0.1075 - 406ms/epoch - 6ms/step\n",
            "Epoch 248/300\n",
            "65/65 - 0s - loss: 0.0241 - val_loss: 0.1306 - 347ms/epoch - 5ms/step\n",
            "Epoch 249/300\n",
            "65/65 - 0s - loss: 0.0249 - val_loss: 0.1181 - 370ms/epoch - 6ms/step\n",
            "Epoch 250/300\n",
            "65/65 - 0s - loss: 0.0218 - val_loss: 0.1089 - 422ms/epoch - 6ms/step\n",
            "Epoch 251/300\n",
            "65/65 - 0s - loss: 0.0214 - val_loss: 0.1112 - 245ms/epoch - 4ms/step\n",
            "Epoch 252/300\n",
            "65/65 - 0s - loss: 0.0202 - val_loss: 0.1101 - 280ms/epoch - 4ms/step\n",
            "Epoch 253/300\n",
            "65/65 - 0s - loss: 0.0204 - val_loss: 0.1123 - 257ms/epoch - 4ms/step\n",
            "Epoch 254/300\n",
            "65/65 - 0s - loss: 0.0230 - val_loss: 0.1200 - 256ms/epoch - 4ms/step\n",
            "Epoch 255/300\n",
            "65/65 - 0s - loss: 0.0259 - val_loss: 0.1307 - 242ms/epoch - 4ms/step\n",
            "Epoch 256/300\n",
            "65/65 - 0s - loss: 0.0254 - val_loss: 0.1092 - 284ms/epoch - 4ms/step\n",
            "Epoch 257/300\n",
            "65/65 - 0s - loss: 0.0222 - val_loss: 0.1278 - 251ms/epoch - 4ms/step\n",
            "Epoch 258/300\n",
            "65/65 - 0s - loss: 0.0227 - val_loss: 0.1105 - 247ms/epoch - 4ms/step\n",
            "Epoch 259/300\n",
            "65/65 - 0s - loss: 0.0197 - val_loss: 0.1146 - 282ms/epoch - 4ms/step\n",
            "Epoch 260/300\n",
            "65/65 - 0s - loss: 0.0240 - val_loss: 0.1142 - 248ms/epoch - 4ms/step\n",
            "Epoch 261/300\n",
            "65/65 - 0s - loss: 0.0198 - val_loss: 0.1134 - 250ms/epoch - 4ms/step\n",
            "Epoch 262/300\n",
            "65/65 - 0s - loss: 0.0205 - val_loss: 0.1198 - 280ms/epoch - 4ms/step\n",
            "Epoch 263/300\n",
            "65/65 - 0s - loss: 0.0204 - val_loss: 0.1120 - 285ms/epoch - 4ms/step\n",
            "Epoch 264/300\n",
            "65/65 - 0s - loss: 0.0202 - val_loss: 0.1050 - 251ms/epoch - 4ms/step\n",
            "Epoch 265/300\n",
            "65/65 - 0s - loss: 0.0231 - val_loss: 0.1121 - 292ms/epoch - 4ms/step\n",
            "Epoch 266/300\n",
            "65/65 - 0s - loss: 0.0228 - val_loss: 0.1082 - 243ms/epoch - 4ms/step\n",
            "Epoch 267/300\n",
            "65/65 - 0s - loss: 0.0212 - val_loss: 0.1067 - 253ms/epoch - 4ms/step\n",
            "Epoch 268/300\n",
            "65/65 - 0s - loss: 0.0232 - val_loss: 0.1150 - 259ms/epoch - 4ms/step\n",
            "Epoch 269/300\n",
            "65/65 - 0s - loss: 0.0239 - val_loss: 0.1125 - 283ms/epoch - 4ms/step\n",
            "Epoch 270/300\n",
            "65/65 - 0s - loss: 0.0229 - val_loss: 0.1090 - 246ms/epoch - 4ms/step\n",
            "Epoch 271/300\n",
            "65/65 - 0s - loss: 0.0189 - val_loss: 0.1119 - 247ms/epoch - 4ms/step\n",
            "Epoch 272/300\n",
            "65/65 - 0s - loss: 0.0245 - val_loss: 0.1179 - 264ms/epoch - 4ms/step\n",
            "Epoch 273/300\n",
            "65/65 - 0s - loss: 0.0222 - val_loss: 0.1129 - 287ms/epoch - 4ms/step\n",
            "Epoch 274/300\n",
            "65/65 - 0s - loss: 0.0209 - val_loss: 0.1067 - 246ms/epoch - 4ms/step\n",
            "Epoch 275/300\n",
            "65/65 - 0s - loss: 0.0205 - val_loss: 0.1083 - 253ms/epoch - 4ms/step\n",
            "Epoch 276/300\n",
            "65/65 - 0s - loss: 0.0200 - val_loss: 0.1324 - 262ms/epoch - 4ms/step\n",
            "Epoch 277/300\n",
            "65/65 - 0s - loss: 0.0206 - val_loss: 0.1083 - 246ms/epoch - 4ms/step\n",
            "Epoch 278/300\n",
            "65/65 - 0s - loss: 0.0201 - val_loss: 0.1112 - 248ms/epoch - 4ms/step\n",
            "Epoch 279/300\n",
            "65/65 - 0s - loss: 0.0209 - val_loss: 0.1185 - 285ms/epoch - 4ms/step\n",
            "Epoch 280/300\n",
            "65/65 - 0s - loss: 0.0221 - val_loss: 0.1065 - 288ms/epoch - 4ms/step\n",
            "Epoch 281/300\n",
            "65/65 - 0s - loss: 0.0234 - val_loss: 0.1107 - 285ms/epoch - 4ms/step\n",
            "Epoch 282/300\n",
            "65/65 - 0s - loss: 0.0192 - val_loss: 0.1082 - 254ms/epoch - 4ms/step\n",
            "Epoch 283/300\n",
            "65/65 - 0s - loss: 0.0211 - val_loss: 0.1048 - 272ms/epoch - 4ms/step\n",
            "Epoch 284/300\n",
            "65/65 - 0s - loss: 0.0197 - val_loss: 0.1252 - 265ms/epoch - 4ms/step\n",
            "Epoch 285/300\n",
            "65/65 - 0s - loss: 0.0193 - val_loss: 0.1061 - 253ms/epoch - 4ms/step\n",
            "Epoch 286/300\n",
            "65/65 - 0s - loss: 0.0166 - val_loss: 0.1124 - 247ms/epoch - 4ms/step\n",
            "Epoch 287/300\n",
            "65/65 - 0s - loss: 0.0180 - val_loss: 0.1118 - 267ms/epoch - 4ms/step\n",
            "Epoch 288/300\n",
            "65/65 - 0s - loss: 0.0183 - val_loss: 0.1198 - 402ms/epoch - 6ms/step\n",
            "Epoch 289/300\n",
            "65/65 - 0s - loss: 0.0208 - val_loss: 0.1220 - 383ms/epoch - 6ms/step\n",
            "Epoch 290/300\n",
            "65/65 - 0s - loss: 0.0187 - val_loss: 0.1084 - 378ms/epoch - 6ms/step\n",
            "Epoch 291/300\n",
            "65/65 - 0s - loss: 0.0193 - val_loss: 0.1158 - 393ms/epoch - 6ms/step\n",
            "Epoch 292/300\n",
            "65/65 - 0s - loss: 0.0203 - val_loss: 0.1083 - 398ms/epoch - 6ms/step\n",
            "Epoch 293/300\n",
            "65/65 - 0s - loss: 0.0173 - val_loss: 0.1179 - 377ms/epoch - 6ms/step\n",
            "Epoch 294/300\n",
            "65/65 - 0s - loss: 0.0181 - val_loss: 0.1086 - 392ms/epoch - 6ms/step\n",
            "Epoch 295/300\n",
            "65/65 - 0s - loss: 0.0181 - val_loss: 0.1143 - 414ms/epoch - 6ms/step\n",
            "Epoch 296/300\n",
            "65/65 - 0s - loss: 0.0181 - val_loss: 0.1073 - 407ms/epoch - 6ms/step\n",
            "Epoch 297/300\n",
            "65/65 - 0s - loss: 0.0175 - val_loss: 0.1196 - 307ms/epoch - 5ms/step\n",
            "Epoch 298/300\n",
            "65/65 - 0s - loss: 0.0201 - val_loss: 0.1235 - 262ms/epoch - 4ms/step\n",
            "Epoch 299/300\n",
            "65/65 - 0s - loss: 0.0198 - val_loss: 0.1057 - 252ms/epoch - 4ms/step\n",
            "Epoch 300/300\n",
            "65/65 - 0s - loss: 0.0213 - val_loss: 0.1101 - 260ms/epoch - 4ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7fd251e62230>"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ],
      "source": [
        "# Build the model\n",
        "hidden_layer_neurals = 64\n",
        "model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Dense(hidden_layer_neurals, activation='relu', input_shape=(10,)),\n",
        "    tf.keras.layers.Dense(hidden_layer_neurals, activation='relu'),\n",
        "    tf.keras.layers.Dense(hidden_layer_neurals, activation='relu'),\n",
        "    tf.keras.layers.Dense(hidden_layer_neurals, activation='relu'),\n",
        "    tf.keras.layers.Dense(hidden_layer_neurals, activation='relu'),\n",
        "    tf.keras.layers.Dense(1, activation='linear')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "# Define your desired learning rate\n",
        "learning_rate = 2e-4\n",
        "adam_optimizer = Adam(learning_rate=learning_rate)\n",
        "model.compile(optimizer=adam_optimizer, loss='mean_squared_error')\n",
        "\n",
        "# # Train the model\n",
        "model.fit(X_train_minmax, y_train, validation_split=0.15, epochs=300, batch_size=32, verbose=2)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model\n",
        "\n",
        "test_loss = model.evaluate(X_test_minmax, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hizV8PYg0Zod",
        "outputId": "e8dd3549-5706-4087-812a-414e9a492cf7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "20/20 [==============================] - 0s 2ms/step - loss: 0.0998\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Generate predictions\n",
        "y_pred = model.predict(X_test_minmax)\n",
        "\n",
        "# Flatten y_pred to ensure it's in the same shape as y_test\n",
        "y_pred = y_pred.flatten()\n",
        "\n",
        "# Print true and predicted values\n",
        "for true, pred in zip(y_test, y_pred):\n",
        "    print(f\"True: {true}, Predicted: {pred}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ahR0Q4b_b_4",
        "outputId": "040285a8-1f99-4c5f-e32a-74976258cfc7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "20/20 [==============================] - 0s 3ms/step\n",
            "True: 3.0178682662762513, Predicted: 2.9539005756378174\n",
            "True: 0.34011344442588587, Predicted: 0.462912380695343\n",
            "True: 0.4888605092503627, Predicted: 0.46087661385536194\n",
            "True: 0.5187183288106738, Predicted: 0.5197278261184692\n",
            "True: 3.839619957911904, Predicted: 4.492387294769287\n",
            "True: 0.5437148188748948, Predicted: 0.5917494297027588\n",
            "True: 1.8904210069442406, Predicted: 1.8316186666488647\n",
            "True: 0.16692362756604606, Predicted: 0.2620367109775543\n",
            "True: 0.6165579585404503, Predicted: 0.6703601479530334\n",
            "True: 0.4256696180779985, Predicted: 1.259719967842102\n",
            "True: 0.2265400860152856, Predicted: -0.07402719557285309\n",
            "True: 0.3635165964266504, Predicted: 0.457239031791687\n",
            "True: 3.1286093587060138, Predicted: 3.123720645904541\n",
            "True: 0.37906718659229477, Predicted: 0.37145134806632996\n",
            "True: 0.5592932747010888, Predicted: 0.5612027049064636\n",
            "True: 0.9191880242335625, Predicted: 1.0597026348114014\n",
            "True: 0.5398662690392555, Predicted: 0.5737130045890808\n",
            "True: 0.947616845543993, Predicted: 0.8408961892127991\n",
            "True: 0.5976291318793855, Predicted: 0.62075275182724\n",
            "True: 0.5444679227980618, Predicted: 0.5658093690872192\n",
            "True: 0.36442501587431986, Predicted: 0.2361379861831665\n",
            "True: 0.3213966364663681, Predicted: 0.4025286138057709\n",
            "True: 0.39719217855370564, Predicted: 0.42751574516296387\n",
            "True: 0.46339406881878753, Predicted: 0.5141991972923279\n",
            "True: 0.4998711346774508, Predicted: 0.4973873496055603\n",
            "True: 0.6621818370389729, Predicted: 0.5760633945465088\n",
            "True: 0.3466140390859927, Predicted: 0.24018928408622742\n",
            "True: 0.5304353586542719, Predicted: 0.6755796074867249\n",
            "True: 0.6349281278131933, Predicted: 0.6055775284767151\n",
            "True: 4.009078370790966, Predicted: 0.6365982294082642\n",
            "True: 0.15693528156527262, Predicted: 0.13684457540512085\n",
            "True: 0.005760578790451412, Predicted: 0.1725917011499405\n",
            "True: 0.6012484725763904, Predicted: 0.5712727308273315\n",
            "True: 2.550113897264019, Predicted: 2.837585926055908\n",
            "True: 0.5979547634140958, Predicted: 0.4567400813102722\n",
            "True: 0.5147429939518088, Predicted: 0.49170899391174316\n",
            "True: 0.5489264960751795, Predicted: 0.7236393094062805\n",
            "True: 0.4418033158814417, Predicted: 0.5043516755104065\n",
            "True: 0.4124673847122135, Predicted: 0.6174694299697876\n",
            "True: 0.40102232178488234, Predicted: 0.3617843985557556\n",
            "True: 0.42481285414657927, Predicted: 0.24945449829101562\n",
            "True: 0.32066235380614466, Predicted: 0.22441759705543518\n",
            "True: 0.5114085626179508, Predicted: 0.5618292093276978\n",
            "True: 2.8078526311468717, Predicted: 3.1554415225982666\n",
            "True: 0.1745484529195676, Predicted: 0.3025878667831421\n",
            "True: 3.285942394938939, Predicted: 2.981917381286621\n",
            "True: 0.43155547391679716, Predicted: 0.4759776294231415\n",
            "True: 0.5457043260666736, Predicted: 0.5801572799682617\n",
            "True: 3.7029384353296853, Predicted: 2.9861161708831787\n",
            "True: 0.47613036038078593, Predicted: 0.4169211685657501\n",
            "True: 3.902199776495614, Predicted: 4.558155536651611\n",
            "True: 1.8334584544155872, Predicted: 2.377272844314575\n",
            "True: 0.26580122325567723, Predicted: -0.02053128555417061\n",
            "True: 0.5963953520268879, Predicted: 0.67310631275177\n",
            "True: 0.5608153560764636, Predicted: 0.6619210839271545\n",
            "True: 0.8222657677885976, Predicted: 1.0848169326782227\n",
            "True: 0.2652822699803816, Predicted: 0.3244360685348511\n",
            "True: 0.5055572379073607, Predicted: 0.4382801055908203\n",
            "True: 0.5663086327461045, Predicted: 0.507904589176178\n",
            "True: 0.3499933377620261, Predicted: 0.4118845462799072\n",
            "True: 3.114378859658725, Predicted: 3.7408061027526855\n",
            "True: 0.6456398873024529, Predicted: 0.6451042890548706\n",
            "True: 0.3997232724780272, Predicted: 0.28333190083503723\n",
            "True: 0.5703849287756964, Predicted: 0.7441393733024597\n",
            "True: 0.6103283617523672, Predicted: 0.7054727673530579\n",
            "True: 0.9966142010332308, Predicted: 1.2253410816192627\n",
            "True: 0.9691263352627072, Predicted: 1.1038275957107544\n",
            "True: 0.42264960442993743, Predicted: 0.47204020619392395\n",
            "True: 0.5447169992602588, Predicted: 0.26469987630844116\n",
            "True: 0.5752680159344936, Predicted: 0.6169838905334473\n",
            "True: 0.003632533849790467, Predicted: 0.3600457012653351\n",
            "True: 0.006014552492107068, Predicted: 0.050957705825567245\n",
            "True: 0.40956650398751326, Predicted: 0.4964238405227661\n",
            "True: 0.5047437795579498, Predicted: 0.6285306811332703\n",
            "True: 0.5158447086637097, Predicted: 1.3403353691101074\n",
            "True: 0.6887522842245746, Predicted: 0.9127582907676697\n",
            "True: 0.5860988329210595, Predicted: 0.4507445991039276\n",
            "True: 0.5739638359127148, Predicted: 0.6706158518791199\n",
            "True: 0.31614828853927457, Predicted: 0.34707942605018616\n",
            "True: 0.43838793637638185, Predicted: 0.42959946393966675\n",
            "True: 0.5827640311558833, Predicted: 0.535490095615387\n",
            "True: 0.4629608001607466, Predicted: 0.5495949387550354\n",
            "True: 0.5062471112686066, Predicted: 0.5212545990943909\n",
            "True: 0.5760668643206351, Predicted: 0.5818135738372803\n",
            "True: 0.5146541515916518, Predicted: 0.4466070830821991\n",
            "True: 0.630198244235264, Predicted: 0.37563660740852356\n",
            "True: 3.097019043293701, Predicted: 2.9627459049224854\n",
            "True: 1.1693592383058196, Predicted: 1.0876502990722656\n",
            "True: 0.5004326371553228, Predicted: 0.9546458721160889\n",
            "True: 0.5396738572860376, Predicted: 1.3187336921691895\n",
            "True: 0.493212953583548, Predicted: 0.391147255897522\n",
            "True: 0.5291180080024881, Predicted: 0.6199533343315125\n",
            "True: 0.4811047148332485, Predicted: 0.6255080103874207\n",
            "True: 0.3820149566945108, Predicted: 0.3698756694793701\n",
            "True: 0.5560640077799889, Predicted: 0.45821520686149597\n",
            "True: 0.49995782287342827, Predicted: 0.6277008652687073\n",
            "True: 0.6043538411393561, Predicted: 0.5707365274429321\n",
            "True: 0.265581178327119, Predicted: 0.34313732385635376\n",
            "True: 0.4766963441986063, Predicted: 0.3465057909488678\n",
            "True: 0.5674555867814141, Predicted: 0.6158217191696167\n",
            "True: 0.6528894864365454, Predicted: 0.7391430735588074\n",
            "True: 0.06706591656360081, Predicted: 0.3187830448150635\n",
            "True: 0.5905983821662687, Predicted: 0.6013023257255554\n",
            "True: 0.44724779891869043, Predicted: 0.34679239988327026\n",
            "True: 0.004935881579399667, Predicted: 0.18681921064853668\n",
            "True: 0.32330762392429957, Predicted: 0.3987836539745331\n",
            "True: 0.5271019720593643, Predicted: 1.0329885482788086\n",
            "True: 0.6348981782513815, Predicted: 0.420523464679718\n",
            "True: 0.48595428075306696, Predicted: 0.5374298095703125\n",
            "True: 0.5424922242989205, Predicted: 0.7890400290489197\n",
            "True: 0.592971588922771, Predicted: 0.683635950088501\n",
            "True: 0.5586420825204634, Predicted: 0.5279372334480286\n",
            "True: 1.1142485820771038, Predicted: 0.8150177597999573\n",
            "True: 0.3352893391326924, Predicted: 0.33351796865463257\n",
            "True: 3.045423356890952, Predicted: 3.0899369716644287\n",
            "True: 0.5127106140345801, Predicted: 0.3356059491634369\n",
            "True: 0.5584410769679805, Predicted: 0.7528403401374817\n",
            "True: 0.572191263552938, Predicted: 0.59011310338974\n",
            "True: 0.5063630744781207, Predicted: 0.1659659743309021\n",
            "True: 0.5011340068668914, Predicted: 0.8975936770439148\n",
            "True: 0.5525271771455922, Predicted: 0.50509113073349\n",
            "True: 0.25116046661893104, Predicted: 0.012560289353132248\n",
            "True: 0.35531697135750884, Predicted: 0.4086328148841858\n",
            "True: 0.5698185087828924, Predicted: 0.7220752835273743\n",
            "True: 0.4539330672596794, Predicted: 0.7379298806190491\n",
            "True: 0.41705718636424377, Predicted: 0.3625463545322418\n",
            "True: 0.454809004385321, Predicted: 0.6356999278068542\n",
            "True: 0.5090534486983491, Predicted: 0.5823807120323181\n",
            "True: 0.5413102434580368, Predicted: 0.6418117880821228\n",
            "True: 0.007451671921194336, Predicted: 0.38195133209228516\n",
            "True: 0.5439178084168016, Predicted: 0.5966451168060303\n",
            "True: 2.1704656577845727, Predicted: 2.427945613861084\n",
            "True: 0.5671899479207911, Predicted: 0.48613500595092773\n",
            "True: 0.455413790206956, Predicted: 0.42707186937332153\n",
            "True: 0.44365758690689494, Predicted: 0.42808517813682556\n",
            "True: 0.4119664524547543, Predicted: 0.4399811327457428\n",
            "True: 0.5350664435014026, Predicted: 0.5806378126144409\n",
            "True: 0.6187307326434571, Predicted: 0.6631409525871277\n",
            "True: 1.7382992658786716, Predicted: 0.7405110597610474\n",
            "True: 0.5285594844504782, Predicted: 0.5960707664489746\n",
            "True: 0.5633012271351573, Predicted: 0.5418705344200134\n",
            "True: 0.5142505474316261, Predicted: 0.554799497127533\n",
            "True: 0.06415932260159048, Predicted: 0.15703056752681732\n",
            "True: 0.5658981640144848, Predicted: 0.5110117197036743\n",
            "True: 0.5559150979793952, Predicted: 0.4799751043319702\n",
            "True: 1.7385343409415026, Predicted: 2.252531051635742\n",
            "True: 0.4235421969573525, Predicted: 0.33725109696388245\n",
            "True: 0.4764426598290648, Predicted: 0.6656543016433716\n",
            "True: 0.5734915972219224, Predicted: 0.7136861681938171\n",
            "True: 0.4318881193501102, Predicted: 0.44664841890335083\n",
            "True: 0.516779037386176, Predicted: 0.49445417523384094\n",
            "True: 0.46713493394744154, Predicted: 0.48183223605155945\n",
            "True: 0.5305347008950302, Predicted: 0.5470377206802368\n",
            "True: 0.35101555722338845, Predicted: 0.46402308344841003\n",
            "True: 0.5008671264664958, Predicted: 0.47887471318244934\n",
            "True: 0.7176217325329808, Predicted: 0.797036349773407\n",
            "True: 0.9444968281103342, Predicted: 1.8895798921585083\n",
            "True: 1.4905785195246772, Predicted: 1.3977735042572021\n",
            "True: 1.4952519904060333, Predicted: 2.4627814292907715\n",
            "True: 0.5254460199436873, Predicted: 0.6431744694709778\n",
            "True: 1.4217714929342395, Predicted: 1.453420877456665\n",
            "True: 0.5777936451179756, Predicted: 0.3108094334602356\n",
            "True: 0.4096657733642221, Predicted: 0.4442669153213501\n",
            "True: 3.4343159750066334, Predicted: 2.9885611534118652\n",
            "True: 0.33512677843290917, Predicted: 0.34576845169067383\n",
            "True: 0.46721815805939393, Predicted: 0.54450923204422\n",
            "True: 0.5770828951900283, Predicted: 0.06335997581481934\n",
            "True: 0.5960003815985626, Predicted: 0.6212850213050842\n",
            "True: 0.9753516128291212, Predicted: 1.2155399322509766\n",
            "True: 0.1278841712708297, Predicted: 0.2703092694282532\n",
            "True: 0.5413124374571668, Predicted: 0.6421092748641968\n",
            "True: 1.0012355580724621, Predicted: 1.1505988836288452\n",
            "True: 0.4728111656453197, Predicted: 0.47492700815200806\n",
            "True: 3.6424377950700384, Predicted: 3.231081247329712\n",
            "True: 0.26012978798270003, Predicted: 0.37859460711479187\n",
            "True: 0.4776504231547176, Predicted: 0.34254586696624756\n",
            "True: 1.2387867084696458, Predicted: 1.2799458503723145\n",
            "True: 0.3875632720594947, Predicted: 0.2995542883872986\n",
            "True: 0.4450743595453788, Predicted: 0.4462015628814697\n",
            "True: 0.3175030586512923, Predicted: 0.3062497079372406\n",
            "True: 0.08607564735048949, Predicted: 0.049877237528562546\n",
            "True: 0.47224947560154296, Predicted: 0.49522116780281067\n",
            "True: 0.3227890665664677, Predicted: 0.372336745262146\n",
            "True: 0.5076096522723875, Predicted: 0.6994267106056213\n",
            "True: 0.4874685363293496, Predicted: 0.3297099173069\n",
            "True: 0.16031446188002527, Predicted: 0.2771020233631134\n",
            "True: 0.4073320324309529, Predicted: 0.3266630470752716\n",
            "True: 0.05218723056727326, Predicted: 0.10866868495941162\n",
            "True: 0.5123546830578027, Predicted: 0.5209525227546692\n",
            "True: 0.5689551350333061, Predicted: 0.4493134319782257\n",
            "True: 0.31557608142525917, Predicted: 0.45667821168899536\n",
            "True: 0.5143174836404412, Predicted: 0.59116131067276\n",
            "True: 0.5512890776211052, Predicted: 0.49255669116973877\n",
            "True: 4.421803738678234, Predicted: 4.064488887786865\n",
            "True: 0.4164943808263621, Predicted: 0.4104132354259491\n",
            "True: 0.38950308069068074, Predicted: 0.23031337559223175\n",
            "True: 1.1776533542655794, Predicted: 2.0650713443756104\n",
            "True: 0.5619731189756225, Predicted: 0.58343505859375\n",
            "True: 0.5345842097282487, Predicted: 0.5963069796562195\n",
            "True: 3.0680036191982856, Predicted: 3.4633524417877197\n",
            "True: 1.2451634475197089, Predicted: 1.0217498540878296\n",
            "True: 0.5234595505983579, Predicted: 0.5143002271652222\n",
            "True: 0.8636330397856207, Predicted: 0.9322780966758728\n",
            "True: 0.6039996559713253, Predicted: 0.6676198840141296\n",
            "True: 0.5528056262635658, Predicted: 0.63300621509552\n",
            "True: 0.8822395418720328, Predicted: 0.9010770320892334\n",
            "True: 0.42073205896346955, Predicted: 0.5857909321784973\n",
            "True: 0.12608009626470387, Predicted: 0.0936090499162674\n",
            "True: 0.6496401050369173, Predicted: 0.5792980790138245\n",
            "True: 0.46980904567703546, Predicted: 0.5105956196784973\n",
            "True: 0.1517574065910375, Predicted: 0.10840663313865662\n",
            "True: 1.4010140667762543, Predicted: 1.2983124256134033\n",
            "True: 0.5677807306479226, Predicted: 0.7547587752342224\n",
            "True: 0.6251798771489965, Predicted: 0.6257231831550598\n",
            "True: 0.755938902511106, Predicted: 0.8404566645622253\n",
            "True: 0.6238999729143934, Predicted: 0.7027813196182251\n",
            "True: 0.43536542326035116, Predicted: 0.8267373442649841\n",
            "True: 0.41965237075550277, Predicted: 0.6343104839324951\n",
            "True: 0.3574600268477557, Predicted: 0.5213713645935059\n",
            "True: 2.829365370309198, Predicted: 1.7588860988616943\n",
            "True: 0.3881112828730292, Predicted: 0.33272895216941833\n",
            "True: 0.41673140467336933, Predicted: 0.5498165488243103\n",
            "True: 0.580261962568283, Predicted: 0.4775503873825073\n",
            "True: 0.4111934403599161, Predicted: 0.42959272861480713\n",
            "True: 0.7451181751056992, Predicted: 0.6180260181427002\n",
            "True: 0.46271758957111137, Predicted: 0.39372384548187256\n",
            "True: 0.5634257858230397, Predicted: 0.5816939473152161\n",
            "True: 0.8048141194348469, Predicted: 0.7501196265220642\n",
            "True: 0.5016212235471288, Predicted: 0.604103147983551\n",
            "True: 0.558375263402458, Predicted: 0.5326734781265259\n",
            "True: 0.3968243105282249, Predicted: 0.37454068660736084\n",
            "True: 0.495910371677825, Predicted: 0.553673267364502\n",
            "True: 1.6720428240775649, Predicted: 1.9031113386154175\n",
            "True: 0.3498680544181406, Predicted: 0.3556368052959442\n",
            "True: 0.0293155585160025, Predicted: 0.06608043611049652\n",
            "True: 0.005520389526246448, Predicted: 0.09794433414936066\n",
            "True: 0.5754547199987408, Predicted: 0.5452761054039001\n",
            "True: 0.5447100157507454, Predicted: 0.47571292519569397\n",
            "True: 0.3295297231155877, Predicted: 0.5407922267913818\n",
            "True: 0.4011206593634006, Predicted: 0.6660667657852173\n",
            "True: 0.12101355907246902, Predicted: 0.08617204427719116\n",
            "True: 0.41795147858528114, Predicted: 0.47655707597732544\n",
            "True: 1.1878806851389438, Predicted: 0.9182986617088318\n",
            "True: 1.2553674010002056, Predicted: 0.8372998237609863\n",
            "True: 0.0053716631470254, Predicted: 0.30239078402519226\n",
            "True: 0.16526243331380952, Predicted: 0.18518032133579254\n",
            "True: 0.38781247989496304, Predicted: 0.24155890941619873\n",
            "True: 1.7266619786850408, Predicted: 1.707505702972412\n",
            "True: 0.5574733933515582, Predicted: 0.5980014801025391\n",
            "True: 0.8250648732111642, Predicted: 0.7625746130943298\n",
            "True: 0.56966199787398, Predicted: 0.6347293257713318\n",
            "True: 0.5074450376062797, Predicted: 0.5525640845298767\n",
            "True: 0.49399247311342614, Predicted: 0.5635839104652405\n",
            "True: 0.6734506811993722, Predicted: 0.6877236366271973\n",
            "True: 0.36329870198927333, Predicted: 0.47406408190727234\n",
            "True: 0.3965031355849322, Predicted: 0.47640830278396606\n",
            "True: 0.6171975513739832, Predicted: 0.4141731262207031\n",
            "True: 0.586769788741315, Predicted: 0.5888215899467468\n",
            "True: 0.32870773395874064, Predicted: 0.6312790513038635\n",
            "True: 0.3301120582868426, Predicted: 0.20553943514823914\n",
            "True: 0.9212499555013718, Predicted: 0.9225639700889587\n",
            "True: 3.029963087332907, Predicted: 3.5760066509246826\n",
            "True: 0.6524319251569635, Predicted: 0.6549216508865356\n",
            "True: 0.27473797821356094, Predicted: 0.24306538701057434\n",
            "True: 0.5355594641258705, Predicted: 0.8089414238929749\n",
            "True: 0.32405875892246555, Predicted: 0.41860857605934143\n",
            "True: 0.6530161247548498, Predicted: 0.9331390857696533\n",
            "True: 0.4304205555075136, Predicted: 0.4366718828678131\n",
            "True: 0.3445888679950133, Predicted: 0.29275497794151306\n",
            "True: 0.8796005121390759, Predicted: 0.7782304286956787\n",
            "True: 0.9083751608952247, Predicted: 0.858556866645813\n",
            "True: 0.418327051991724, Predicted: 0.28758418560028076\n",
            "True: 0.19418604559285954, Predicted: 0.34131842851638794\n",
            "True: 0.505852034512233, Predicted: 0.5344070792198181\n",
            "True: 0.584108078490298, Predicted: 0.420865923166275\n",
            "True: 0.5722976627703975, Predicted: 0.5071088671684265\n",
            "True: 0.474457677905189, Predicted: 0.7380852699279785\n",
            "True: 0.40973611707906954, Predicted: 0.3452851176261902\n",
            "True: 0.5207149161149576, Predicted: 0.5359952449798584\n",
            "True: 0.5730028915591172, Predicted: 0.4432469308376312\n",
            "True: 0.004136710048815613, Predicted: 0.07042458653450012\n",
            "True: 2.2097515146182864, Predicted: 1.3689316511154175\n",
            "True: 0.32213555471655836, Predicted: 0.33392754197120667\n",
            "True: 0.8093768152981499, Predicted: 0.9432935118675232\n",
            "True: 2.22471054008744, Predicted: 2.481154680252075\n",
            "True: 0.705298910100876, Predicted: 1.1310395002365112\n",
            "True: 0.41342320292718787, Predicted: 0.2511448264122009\n",
            "True: 0.36438957123129845, Predicted: 0.3814355134963989\n",
            "True: 0.9199297464446872, Predicted: 0.5172074437141418\n",
            "True: 0.7846069305127013, Predicted: 0.8308830857276917\n",
            "True: 0.5807892519050166, Predicted: 0.6211600303649902\n",
            "True: 0.7813258864427746, Predicted: 2.337684392929077\n",
            "True: 2.9139098100623824, Predicted: 3.8942174911499023\n",
            "True: 0.27130634068841103, Predicted: 0.20105330646038055\n",
            "True: 2.8799042801008383, Predicted: 3.0207910537719727\n",
            "True: 0.6660771192030668, Predicted: 0.2619078755378723\n",
            "True: 0.5102619710246241, Predicted: 0.5878974199295044\n",
            "True: 0.0027958591789994337, Predicted: 0.6205277442932129\n",
            "True: 0.31302821000370795, Predicted: 0.3007577359676361\n",
            "True: 0.5626152229305537, Predicted: 0.710077702999115\n",
            "True: 0.6806247882133637, Predicted: 0.6372914910316467\n",
            "True: 0.19188758461207694, Predicted: -0.06347277760505676\n",
            "True: 0.6158579378830831, Predicted: 0.6467631459236145\n",
            "True: 0.6141722316858543, Predicted: 0.5234315991401672\n",
            "True: 0.5439247334644753, Predicted: 0.4557035267353058\n",
            "True: 1.5334786536375882, Predicted: 1.2309726476669312\n",
            "True: 1.153617433868687, Predicted: 0.9380078911781311\n",
            "True: 0.37312404859154724, Predicted: 0.42885541915893555\n",
            "True: 0.38717273365063765, Predicted: 0.8852762579917908\n",
            "True: 0.09505126522922763, Predicted: 0.2791026532649994\n",
            "True: 0.5303134591494891, Predicted: 0.6109424233436584\n",
            "True: 0.22389825633537375, Predicted: 0.23741382360458374\n",
            "True: 0.27090045512905503, Predicted: -0.057499561458826065\n",
            "True: 0.35807548903780806, Predicted: 0.4452512860298157\n",
            "True: 0.5458907931246092, Predicted: 0.5969387292861938\n",
            "True: 1.2106073688232672, Predicted: 2.0219767093658447\n",
            "True: 0.48286386629271866, Predicted: 0.38410767912864685\n",
            "True: 0.5177213748955252, Predicted: 0.5024383664131165\n",
            "True: 0.5821739186729791, Predicted: 0.7387872338294983\n",
            "True: 0.5121860107286177, Predicted: 0.47190535068511963\n",
            "True: 0.2998186066581788, Predicted: 0.5403624773025513\n",
            "True: 0.5489467101644766, Predicted: 0.7095787525177002\n",
            "True: 0.6071773030618068, Predicted: 0.9421486258506775\n",
            "True: 0.5978601267729612, Predicted: 0.6015687584877014\n",
            "True: 0.5841359603211388, Predicted: 0.5635645985603333\n",
            "True: 1.1415976883070755, Predicted: 1.2481006383895874\n",
            "True: 0.5582587594133436, Predicted: 0.5501449108123779\n",
            "True: 0.5747693500490207, Predicted: 0.6162101030349731\n",
            "True: 0.543369745920991, Predicted: 0.9105328917503357\n",
            "True: 3.992126885487972, Predicted: 3.880585193634033\n",
            "True: 0.39073461492401773, Predicted: 0.4012530446052551\n",
            "True: 0.6245771921809543, Predicted: 0.5454003810882568\n",
            "True: 1.318379824613935, Predicted: 1.1985719203948975\n",
            "True: 0.7276519770713993, Predicted: 0.6934264898300171\n",
            "True: 0.4255000186520082, Predicted: 0.45944106578826904\n",
            "True: 0.5141292637781116, Predicted: 0.45902779698371887\n",
            "True: 0.3988537303427012, Predicted: 0.4170469045639038\n",
            "True: 2.5961618216314393, Predicted: 2.1416726112365723\n",
            "True: 0.2833853495804853, Predicted: 0.3406858742237091\n",
            "True: 0.5539440687461469, Predicted: 0.5966160893440247\n",
            "True: 3.2672756981650375, Predicted: 3.0760951042175293\n",
            "True: 0.3667323113591937, Predicted: 0.5893036723136902\n",
            "True: 2.37562012452399, Predicted: 2.9311413764953613\n",
            "True: 0.34292130610077287, Predicted: 0.5292340517044067\n",
            "True: 1.063811137041237, Predicted: 1.6892685890197754\n",
            "True: 0.5528027975962169, Predicted: 0.49784231185913086\n",
            "True: 0.7286015586174522, Predicted: 0.7552935481071472\n",
            "True: 0.41785491969415484, Predicted: 0.6041886806488037\n",
            "True: 0.41551423149780414, Predicted: 0.47072672843933105\n",
            "True: 0.5424987741791034, Predicted: 0.6369004845619202\n",
            "True: 0.41012447014271836, Predicted: 0.5440858006477356\n",
            "True: 0.15405166191861103, Predicted: -0.12496200203895569\n",
            "True: 0.42123298223484995, Predicted: 0.37776878476142883\n",
            "True: 0.5116119762542414, Predicted: 0.4751417338848114\n",
            "True: 0.737193512085931, Predicted: 0.7946605086326599\n",
            "True: 0.49935446434116965, Predicted: 0.7587886452674866\n",
            "True: 0.6073158890189307, Predicted: 0.7517766356468201\n",
            "True: 0.616853731822046, Predicted: 0.6327407956123352\n",
            "True: 0.3874500432683276, Predicted: 0.35516148805618286\n",
            "True: 0.35396049106203215, Predicted: 0.46753811836242676\n",
            "True: 2.0791462623551684, Predicted: 2.34997296333313\n",
            "True: 0.4671331473013026, Predicted: 0.540302574634552\n",
            "True: 0.5845496615533631, Predicted: 0.8672379851341248\n",
            "True: 0.5718996945779558, Predicted: 0.6268906593322754\n",
            "True: 0.5421494257576649, Predicted: 0.5987641215324402\n",
            "True: 0.5681457474250358, Predicted: 0.5183797478675842\n",
            "True: 0.3270999286606835, Predicted: 0.2238362431526184\n",
            "True: 0.9094261124303096, Predicted: 1.4664016962051392\n",
            "True: 0.8669224757348308, Predicted: 0.7761871218681335\n",
            "True: 0.5534780082759685, Predicted: 0.5779929161071777\n",
            "True: 0.43780019537514914, Predicted: 0.49072587490081787\n",
            "True: 3.805697053846136, Predicted: 3.6285505294799805\n",
            "True: 0.5682172076372134, Predicted: 0.8366155028343201\n",
            "True: 0.5395508913492147, Predicted: 0.6225999593734741\n",
            "True: 0.5845249304955428, Predicted: 0.5590217709541321\n",
            "True: 0.5787084662869124, Predicted: 0.5068362951278687\n",
            "True: 0.690218181672059, Predicted: 0.38886621594429016\n",
            "True: 0.35308433880249457, Predicted: 0.2718559503555298\n",
            "True: 2.3102164917633994, Predicted: 2.733551263809204\n",
            "True: 1.524294227837417, Predicted: 1.0621681213378906\n",
            "True: 0.48892162925920674, Predicted: 0.4879830479621887\n",
            "True: 0.6487109913435524, Predicted: 0.5592418313026428\n",
            "True: 0.8082491348419513, Predicted: 0.8832906484603882\n",
            "True: 0.33882159582695104, Predicted: 0.1510809361934662\n",
            "True: 0.3358181186142875, Predicted: 0.4723649322986603\n",
            "True: 1.1920327965505453, Predicted: 0.9248912930488586\n",
            "True: 1.1908896007428988, Predicted: 1.5451549291610718\n",
            "True: 0.7469600152016266, Predicted: 0.7402750849723816\n",
            "True: 0.3718849988925145, Predicted: 0.6358886361122131\n",
            "True: 0.3462869249929549, Predicted: 0.2877035439014435\n",
            "True: 0.5586190371548804, Predicted: 0.6174136400222778\n",
            "True: 0.5390845634626795, Predicted: 0.49888113141059875\n",
            "True: 0.5169840374356431, Predicted: 0.6650394201278687\n",
            "True: 0.6190379165105564, Predicted: 0.8759711384773254\n",
            "True: 0.2254158611847807, Predicted: 0.22467532753944397\n",
            "True: 0.7084809839306503, Predicted: 0.9777874946594238\n",
            "True: 1.6147726322010545, Predicted: 1.9340351819992065\n",
            "True: 0.44701035879979956, Predicted: 0.5296630263328552\n",
            "True: 0.6025654476554595, Predicted: 0.5931281447410583\n",
            "True: 0.5309343759772696, Predicted: 0.620378851890564\n",
            "True: 0.6304568534858884, Predicted: 0.5264347791671753\n",
            "True: 0.5584492144049215, Predicted: 0.6263359189033508\n",
            "True: 0.6027132852528538, Predicted: 1.0225918292999268\n",
            "True: 0.40799685183466006, Predicted: 0.6443737149238586\n",
            "True: 0.6351587930393794, Predicted: 0.5686188340187073\n",
            "True: 0.39255237638781343, Predicted: 0.47820690274238586\n",
            "True: 1.1636328534317562, Predicted: 0.9630045294761658\n",
            "True: 0.406580000759409, Predicted: 0.3504100441932678\n",
            "True: 0.3399859260526826, Predicted: 0.3216416835784912\n",
            "True: 3.5792331183445762, Predicted: 3.284034490585327\n",
            "True: 0.5443153090458585, Predicted: 0.5378430485725403\n",
            "True: 0.5648422786754692, Predicted: 0.4955856204032898\n",
            "True: 0.5956192347434321, Predicted: 0.6204070448875427\n",
            "True: 3.98111803780072, Predicted: 4.840734004974365\n",
            "True: 0.5317901108312656, Predicted: 0.636486291885376\n",
            "True: 0.5530233442136157, Predicted: 0.4085354506969452\n",
            "True: 0.005018649712650877, Predicted: 0.05566274747252464\n",
            "True: 0.47334024774266154, Predicted: 0.5401774644851685\n",
            "True: 0.3261808137539814, Predicted: 0.3551784157752991\n",
            "True: 0.5497814895600651, Predicted: 0.5376847982406616\n",
            "True: 0.6869632451901988, Predicted: 0.7522699236869812\n",
            "True: 0.5349457269910716, Predicted: 0.5903544425964355\n",
            "True: 1.2111472607182456, Predicted: 1.3888784646987915\n",
            "True: 1.071220806251477, Predicted: 0.9026190638542175\n",
            "True: 0.16115016480062785, Predicted: 0.17376452684402466\n",
            "True: 0.4704861038996926, Predicted: 0.5815140008926392\n",
            "True: 0.5131818656232503, Predicted: 0.6260650753974915\n",
            "True: 0.6354547690070012, Predicted: 1.0074002742767334\n",
            "True: 1.5802320275005688, Predicted: 2.589759588241577\n",
            "True: 2.1410888612587535, Predicted: 2.117036819458008\n",
            "True: 0.5988361605215892, Predicted: 1.1857160329818726\n",
            "True: 0.4682948039291134, Predicted: 0.6158921122550964\n",
            "True: 0.5182735825034865, Predicted: 0.4252147674560547\n",
            "True: 0.5955890368484735, Predicted: 0.41141414642333984\n",
            "True: 1.0753221066736482, Predicted: 1.0499790906906128\n",
            "True: 0.531027603207394, Predicted: 0.5896134972572327\n",
            "True: 0.7128542354729969, Predicted: 0.8613784909248352\n",
            "True: 4.157204290792338, Predicted: 1.851047158241272\n",
            "True: 0.46984376016996443, Predicted: 0.5282981991767883\n",
            "True: 0.3520168090698665, Predicted: 0.34624332189559937\n",
            "True: 0.0030066338970676275, Predicted: 0.02153293415904045\n",
            "True: 0.5078993740149549, Predicted: 0.5428306460380554\n",
            "True: 0.48547483417278137, Predicted: 0.4013080596923828\n",
            "True: 0.5865718303194678, Predicted: 0.7186529636383057\n",
            "True: 1.3994067089982836, Predicted: 1.0612716674804688\n",
            "True: 2.5529600930906717, Predicted: 2.742025852203369\n",
            "True: 0.5848661340969626, Predicted: 0.42015835642814636\n",
            "True: 1.0738589176505626, Predicted: 0.8763670921325684\n",
            "True: 0.3752594783865661, Predicted: 0.39183923602104187\n",
            "True: 0.40710056826780344, Predicted: 0.26804330945014954\n",
            "True: 0.7111795162434519, Predicted: 0.5708277821540833\n",
            "True: 0.38295129303467557, Predicted: 0.5277159214019775\n",
            "True: 0.5011637471293104, Predicted: 0.5349324941635132\n",
            "True: 0.9853194004049903, Predicted: 1.858600378036499\n",
            "True: 0.34919655527658683, Predicted: 0.2770818769931793\n",
            "True: 0.5043972320592826, Predicted: 0.5138124823570251\n",
            "True: 0.4003664150703572, Predicted: 0.4222702383995056\n",
            "True: 0.5163612245137101, Predicted: 0.5524488091468811\n",
            "True: 0.5203827303302575, Predicted: 0.5649228096008301\n",
            "True: 0.6226265793908854, Predicted: 0.6612246632575989\n",
            "True: 0.6168529250283754, Predicted: 0.7052244544029236\n",
            "True: 0.525541717506945, Predicted: 0.5995456576347351\n",
            "True: 0.6229339376633912, Predicted: 0.6703732013702393\n",
            "True: 0.39693025734745446, Predicted: 0.054670240730047226\n",
            "True: 0.5197546131830914, Predicted: 0.48469632863998413\n",
            "True: 0.46334621518009383, Predicted: 0.43707484006881714\n",
            "True: 0.46849498073678375, Predicted: 0.46259167790412903\n",
            "True: 0.3948369906229112, Predicted: 0.36126092076301575\n",
            "True: 1.1223231051795364, Predicted: 0.9170196056365967\n",
            "True: 0.8923359629371589, Predicted: 0.4288950562477112\n",
            "True: 0.18784592863750443, Predicted: 0.17189282178878784\n",
            "True: 0.34390050945951944, Predicted: 0.3218896985054016\n",
            "True: 0.7354558899037368, Predicted: 0.6725897789001465\n",
            "True: 0.5068303247447268, Predicted: 0.48616549372673035\n",
            "True: 0.6015867051946128, Predicted: 0.5962327122688293\n",
            "True: 3.2138220506180835, Predicted: 1.0478066205978394\n",
            "True: 0.7444095893113067, Predicted: 0.6317458152770996\n",
            "True: 2.8804359914610367, Predicted: 2.231461524963379\n",
            "True: 0.3125670043081603, Predicted: 0.2819049656391144\n",
            "True: 0.1316926070633454, Predicted: 0.28924107551574707\n",
            "True: 0.4760445174475848, Predicted: 0.558339536190033\n",
            "True: 0.521397810691949, Predicted: 0.30433231592178345\n",
            "True: 0.5967962075877945, Predicted: 0.6791606545448303\n",
            "True: 0.7058135724288216, Predicted: 0.8226692080497742\n",
            "True: 0.18735624469349138, Predicted: 0.23808170855045319\n",
            "True: 0.8167587235765217, Predicted: 0.6310052275657654\n",
            "True: 0.7552900914169896, Predicted: 1.3773040771484375\n",
            "True: 0.35460476300199095, Predicted: 0.3500504493713379\n",
            "True: 0.9438723750393884, Predicted: 0.7528076767921448\n",
            "True: 0.475800411007348, Predicted: 0.45532163977622986\n",
            "True: 2.672595340003873, Predicted: 2.895861864089966\n",
            "True: 0.5491008514559714, Predicted: 0.6117991209030151\n",
            "True: 0.3752581014325554, Predicted: 0.3812999129295349\n",
            "True: 0.4596535176894643, Predicted: 0.3778977394104004\n",
            "True: 0.843653446082625, Predicted: 1.0608161687850952\n",
            "True: 3.8914179511566185, Predicted: 3.5030710697174072\n",
            "True: 0.7074752878937729, Predicted: 0.6103914380073547\n",
            "True: 0.19326960269017096, Predicted: 0.14508333802223206\n",
            "True: 0.6654615932144765, Predicted: 0.8209862112998962\n",
            "True: 0.33143294539862306, Predicted: 0.306379497051239\n",
            "True: 0.4608314968530211, Predicted: 0.5609848499298096\n",
            "True: 0.3615708731591389, Predicted: 0.43039005994796753\n",
            "True: 1.1789670649879471, Predicted: 1.015645146369934\n",
            "True: 0.4972549695882965, Predicted: 0.45027485489845276\n",
            "True: 0.47593142811024536, Predicted: 0.44191083312034607\n",
            "True: 0.0054497603380347865, Predicted: 0.08934615552425385\n",
            "True: 0.9102542258816745, Predicted: 0.7050458192825317\n",
            "True: 0.37695138728546324, Predicted: 0.4087868630886078\n",
            "True: 0.6068899561208037, Predicted: 0.783619225025177\n",
            "True: 1.5372976675715955, Predicted: 1.1848833560943604\n",
            "True: 0.6688507021557214, Predicted: 1.708997130393982\n",
            "True: 0.16327224273239482, Predicted: 0.17985281348228455\n",
            "True: 0.448769124197937, Predicted: 0.4602465033531189\n",
            "True: 0.5206520659922663, Predicted: 0.6223415732383728\n",
            "True: 0.5385356120464756, Predicted: 0.6368868947029114\n",
            "True: 0.5962179644015803, Predicted: 0.9119473099708557\n",
            "True: 1.5776933065467718, Predicted: 1.267526626586914\n",
            "True: 0.6554701980476372, Predicted: 0.6014910340309143\n",
            "True: 0.4127350695879768, Predicted: 0.3202959895133972\n",
            "True: 2.7191990263418235, Predicted: 2.969010353088379\n",
            "True: 0.006966256910859557, Predicted: -0.10454273223876953\n",
            "True: 1.9430816754909255, Predicted: 2.7873694896698\n",
            "True: 0.005179657608992576, Predicted: 0.14060544967651367\n",
            "True: 0.4927602398535579, Predicted: 0.5156094431877136\n",
            "True: 0.4077012082073199, Predicted: 0.405938982963562\n",
            "True: 0.5113666826418796, Predicted: 0.4578450322151184\n",
            "True: 0.5234989042682543, Predicted: 0.48957526683807373\n",
            "True: 0.6238556083952549, Predicted: 0.7883885502815247\n",
            "True: 2.8598155652471675, Predicted: 2.8244714736938477\n",
            "True: 0.5726988184307397, Predicted: 0.580230176448822\n",
            "True: 0.891251886208861, Predicted: 0.413531094789505\n",
            "True: 2.197668344040642, Predicted: 2.416792869567871\n",
            "True: 3.5016059982620193, Predicted: 2.978449583053589\n",
            "True: 0.5813322900207085, Predicted: 0.6337288618087769\n",
            "True: 0.448063289759941, Predicted: 0.46863141655921936\n",
            "True: 0.7291589367497857, Predicted: 0.6531623005867004\n",
            "True: 1.476135422053748, Predicted: 1.4145982265472412\n",
            "True: 0.8488120340440543, Predicted: 0.6807827949523926\n",
            "True: 0.9744279467459542, Predicted: 1.0672245025634766\n",
            "True: 0.5962173804977966, Predicted: 0.8792573809623718\n",
            "True: 0.5007403699717118, Predicted: 0.3374480605125427\n",
            "True: 0.5643646963510478, Predicted: 0.6434445977210999\n",
            "True: 0.5673890654957199, Predicted: 0.6279406547546387\n",
            "True: 0.6107577361125615, Predicted: 0.31475213170051575\n",
            "True: 0.3965871881339581, Predicted: 0.3866209387779236\n",
            "True: 0.5989043582801681, Predicted: 0.19622203707695007\n",
            "True: 0.008037997243595892, Predicted: -0.37271928787231445\n",
            "True: 0.006102619577323208, Predicted: 0.3618537187576294\n",
            "True: 0.5784453912958065, Predicted: 0.648620069026947\n",
            "True: 0.4090499589927471, Predicted: 0.4377552270889282\n",
            "True: 3.960113290091421, Predicted: 3.375054121017456\n",
            "True: 0.46026496554561175, Predicted: 0.34689250588417053\n",
            "True: 0.470733526610215, Predicted: 0.4612998366355896\n",
            "True: 0.7264296742449678, Predicted: 0.7413303256034851\n",
            "True: 0.28491284014886736, Predicted: 0.35892802476882935\n",
            "True: 0.0071511361637617565, Predicted: -0.0034322328865528107\n",
            "True: 0.8963499435522967, Predicted: 0.8703394532203674\n",
            "True: 3.538136518325004, Predicted: 3.1326823234558105\n",
            "True: 0.5119412439278836, Predicted: 0.6294732689857483\n",
            "True: 0.5602086375594613, Predicted: 0.6189365386962891\n",
            "True: 0.7267605206197338, Predicted: 0.6161009669303894\n",
            "True: 0.5336038309409923, Predicted: 0.4581730365753174\n",
            "True: 3.682172754719707, Predicted: 4.121590614318848\n",
            "True: 3.301950087110185, Predicted: 3.874753952026367\n",
            "True: 0.3619583903209908, Predicted: 0.23381678760051727\n",
            "True: 0.8797346592753317, Predicted: 0.9947763085365295\n",
            "True: 0.3866075619674395, Predicted: 0.3234596252441406\n",
            "True: 0.44800107858626753, Predicted: 0.5245905518531799\n",
            "True: 0.10821256136601773, Predicted: 0.17160336673259735\n",
            "True: 0.5482439257226331, Predicted: 0.5923544764518738\n",
            "True: 0.5210917933938366, Predicted: 0.5064516067504883\n",
            "True: 0.4800378660747556, Predicted: 0.6113993525505066\n",
            "True: 1.422306654469969, Predicted: 1.3712738752365112\n",
            "True: 1.8127033542235635, Predicted: 1.779198408126831\n",
            "True: 0.5112533074744269, Predicted: 0.6927081346511841\n",
            "True: 0.6271080578807635, Predicted: 0.47767892479896545\n",
            "True: 0.3379623690950358, Predicted: 0.8427460789680481\n",
            "True: 0.4745513989891442, Predicted: 0.6566323041915894\n",
            "True: 0.502074662613169, Predicted: 0.5892260670661926\n",
            "True: 0.5599513322089322, Predicted: 0.589426577091217\n",
            "True: 1.7994781067480479, Predicted: 0.3230852782726288\n",
            "True: 1.489668581198676, Predicted: 1.2858039140701294\n",
            "True: 0.8204007805829351, Predicted: 0.7204580903053284\n",
            "True: 0.72351139792817, Predicted: 0.8472781777381897\n",
            "True: 1.5590491039929246, Predicted: 2.1505978107452393\n",
            "True: 0.3374721493373578, Predicted: 0.18868839740753174\n",
            "True: 0.3658708611084559, Predicted: 0.48406216502189636\n",
            "True: 0.540075001274014, Predicted: 0.5445098876953125\n",
            "True: 2.3925870109568144, Predicted: 2.672912120819092\n",
            "True: 0.2752193289940888, Predicted: 0.10882295668125153\n",
            "True: 0.003450789291310434, Predicted: 0.18602582812309265\n",
            "True: 0.12036224429586317, Predicted: 0.2010709047317505\n",
            "True: 0.6464920172407943, Predicted: 0.5622996091842651\n",
            "True: 0.6908210462593407, Predicted: 0.5442578792572021\n",
            "True: 2.273148037789624, Predicted: 2.3323605060577393\n",
            "True: 0.4945764065327153, Predicted: 0.5890650153160095\n",
            "True: 0.2225576682405711, Predicted: 0.22652938961982727\n",
            "True: 0.5697367273730228, Predicted: 0.16415385901927948\n",
            "True: 0.5454741446870325, Predicted: 0.37571126222610474\n",
            "True: 0.5558939145519897, Predicted: 0.40597835183143616\n",
            "True: 0.4378575896515877, Predicted: 0.35164421796798706\n",
            "True: 0.5151545498184201, Predicted: 0.6070356965065002\n",
            "True: 0.5542778166071753, Predicted: 0.639948844909668\n",
            "True: 0.6599962079721712, Predicted: 0.6297489404678345\n",
            "True: 0.2914887652060093, Predicted: 0.39959877729415894\n",
            "True: 0.5589837038905836, Predicted: 1.0550142526626587\n",
            "True: 0.52843666314775, Predicted: 0.4553813338279724\n",
            "True: 0.8113528750979347, Predicted: 0.8318104147911072\n",
            "True: 1.3869325057956703, Predicted: 1.4160078763961792\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# model.save_weights('cage_metric_model.ckpt')\n",
        "model.save('model_varyingGoal_cutoffLabels.h5')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0jrlkc1pZs9Z",
        "outputId": "b9eb3caa-8baa-4e89-b69a-30c5b539f4dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the entire model\n",
        "new_model = load_model('path_to_my_model.h5')\n",
        "\n",
        "# Use the model for inference\n",
        "i = (X_test[0:1]).tolist()\n",
        "print(i)\n",
        "prediction = new_model.predict(i)\n",
        "print(prediction)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VEer6ssAafeO",
        "outputId": "f48aaf4e-cb7e-4812-fea4-be9d5e316e23"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[-1.6385281345710567, -0.5092962134283774, 0.08382842145923423, -0.13254497591122066, -1.5364387572726412, -0.4886458616509667, 0.003567941207741127, 1.4215608118319583, 0.023628152616274812, -0.3700396101036746]]\n",
            "1/1 [==============================] - 0s 234ms/step\n",
            "[[1.4909074]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TORCH"
      ],
      "metadata": {
        "id": "Dz2YtNA-rmA7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
        "from torch.utils.data import DataLoader, TensorDataset\n"
      ],
      "metadata": {
        "id": "qsPLUJLtruO8"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop\n",
        "def train_loop(dataloader, model, loss_fn, optimizer):\n",
        "    total_loss = 0\n",
        "    num_batch = len(dataloader)\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "        # Compute prediction and loss\n",
        "        pred = model(X)\n",
        "        loss = loss_fn(pred, y.unsqueeze(1))\n",
        "\n",
        "        # Backpropagation\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    return total_loss / batch\n",
        "\n",
        "def val_loop(dataloader, model, loss_fn):\n",
        "    num_batch = len(dataloader)\n",
        "    total_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for X, y in dataloader:\n",
        "            pred = model(X)\n",
        "            loss = loss_fn(pred, y.unsqueeze(1))\n",
        "            total_loss += loss.item()\n",
        "    return total_loss / num_batch\n",
        "\n",
        "# Define the model\n",
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(NeuralNetwork, self).__init__()\n",
        "        self.linear_relu_stack = nn.Sequential(\n",
        "            nn.Linear(10, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.linear_relu_stack(x)\n"
      ],
      "metadata": {
        "id": "lWTf4Ml2shTD"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming 'inputs' and 'labels' are your data\n",
        "X_train, X_test, y_train, y_test = train_test_split(inputs, labels, test_size=0.15, random_state=42)\n",
        "\n",
        "# Min-Max Scaling\n",
        "min_max_scaler = MinMaxScaler()\n",
        "X_train_minmax = min_max_scaler.fit_transform(X_train)\n",
        "X_test_minmax = min_max_scaler.transform(X_test)\n",
        "joblib.dump(min_max_scaler, 'scaler_minmax.pkl')\n",
        "\n",
        "# Standardization\n",
        "standard_scaler = StandardScaler()\n",
        "X_train_standard = standard_scaler.fit_transform(X_train)\n",
        "X_test_standard = standard_scaler.transform(X_test)\n",
        "# joblib.dump(min_max_scaler, 'scaler_standard.pkl')\n"
      ],
      "metadata": {
        "id": "rlv06qozsHQd"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_test[0])\n",
        "X_test_minmax[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bw8ZsdBGVsLJ",
        "outputId": "d5c09874-c73f-4380-c334-5470ece473e9"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2.0221686363220215, 7.916991233825684, 0.06067521497607231, -0.39584794640541077, 1.9203073978424072, 8.321775436401367, -0.027220193296670914, -0.3103671371936798, -0.42248594760894775, -0.12225266546010971]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.20086576, 0.76980991, 0.48601316, 0.28220192, 0.167497  ,\n",
              "       0.81556834, 0.48300312, 0.46832746, 0.27966413, 0.45368815])"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_val, y_train, y_val = train_test_split(X_train_minmax, y_train, test_size=0.2, random_state=42)  # 0.25 x 0.8 = 0.2\n",
        "\n",
        "# Convert arrays to PyTorch tensors\n",
        "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
        "X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
        "y_val_tensor = torch.tensor(y_val, dtype=torch.float32)\n",
        "X_test_tensor = torch.tensor(X_test_minmax, dtype=torch.float32)\n",
        "y_test_tensor = torch.tensor(y_test, dtype=torch.float32)\n",
        "\n",
        "# Create datasets and dataloaders\n",
        "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
        "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
        "\n",
        "batch_size = 32\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "model = NeuralNetwork()\n",
        "\n",
        "# Define the optimizer and loss function\n",
        "learning_rate = 5e-5\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "loss_fn = nn.MSELoss()\n"
      ],
      "metadata": {
        "id": "bT_W1AD0rpVI"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training the model\n",
        "epochs = 400\n",
        "for epoch in range(epochs):\n",
        "    train_loss = train_loop(train_loader, model, loss_fn, optimizer)\n",
        "    val_loss = val_loop(val_loader, model, loss_fn)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}\\n-------------------------------\")\n",
        "    print(f\"Training Loss: {train_loss:.4f}\")\n",
        "    print(f\"Validation Loss: {val_loss:.4f}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "os0eQM0Hxv_I",
        "outputId": "31999987-8b47-4357-8d1c-f16d3df18cb5"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "Training Loss: 10.6788\n",
            "Validation Loss: 11.6092\n",
            "\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "Training Loss: 10.4299\n",
            "Validation Loss: 10.8288\n",
            "\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "Training Loss: 9.8690\n",
            "Validation Loss: 9.9753\n",
            "\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "Training Loss: 8.6270\n",
            "Validation Loss: 8.5931\n",
            "\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "Training Loss: 7.3604\n",
            "Validation Loss: 7.4291\n",
            "\n",
            "Epoch 6\n",
            "-------------------------------\n",
            "Training Loss: 6.9437\n",
            "Validation Loss: 7.1810\n",
            "\n",
            "Epoch 7\n",
            "-------------------------------\n",
            "Training Loss: 6.8118\n",
            "Validation Loss: 7.1872\n",
            "\n",
            "Epoch 8\n",
            "-------------------------------\n",
            "Training Loss: 6.6655\n",
            "Validation Loss: 6.9619\n",
            "\n",
            "Epoch 9\n",
            "-------------------------------\n",
            "Training Loss: 6.5227\n",
            "Validation Loss: 6.8769\n",
            "\n",
            "Epoch 10\n",
            "-------------------------------\n",
            "Training Loss: 6.3411\n",
            "Validation Loss: 6.4773\n",
            "\n",
            "Epoch 11\n",
            "-------------------------------\n",
            "Training Loss: 6.0576\n",
            "Validation Loss: 6.6037\n",
            "\n",
            "Epoch 12\n",
            "-------------------------------\n",
            "Training Loss: 5.7887\n",
            "Validation Loss: 6.0614\n",
            "\n",
            "Epoch 13\n",
            "-------------------------------\n",
            "Training Loss: 5.4632\n",
            "Validation Loss: 5.8596\n",
            "\n",
            "Epoch 14\n",
            "-------------------------------\n",
            "Training Loss: 5.1322\n",
            "Validation Loss: 5.1760\n",
            "\n",
            "Epoch 15\n",
            "-------------------------------\n",
            "Training Loss: 4.7672\n",
            "Validation Loss: 4.7401\n",
            "\n",
            "Epoch 16\n",
            "-------------------------------\n",
            "Training Loss: 4.4270\n",
            "Validation Loss: 4.5217\n",
            "\n",
            "Epoch 17\n",
            "-------------------------------\n",
            "Training Loss: 4.1745\n",
            "Validation Loss: 4.3162\n",
            "\n",
            "Epoch 18\n",
            "-------------------------------\n",
            "Training Loss: 4.0102\n",
            "Validation Loss: 4.1877\n",
            "\n",
            "Epoch 19\n",
            "-------------------------------\n",
            "Training Loss: 3.8996\n",
            "Validation Loss: 4.0839\n",
            "\n",
            "Epoch 20\n",
            "-------------------------------\n",
            "Training Loss: 3.7920\n",
            "Validation Loss: 3.9464\n",
            "\n",
            "Epoch 21\n",
            "-------------------------------\n",
            "Training Loss: 3.7407\n",
            "Validation Loss: 4.0369\n",
            "\n",
            "Epoch 22\n",
            "-------------------------------\n",
            "Training Loss: 3.7230\n",
            "Validation Loss: 3.9806\n",
            "\n",
            "Epoch 23\n",
            "-------------------------------\n",
            "Training Loss: 3.5910\n",
            "Validation Loss: 3.7595\n",
            "\n",
            "Epoch 24\n",
            "-------------------------------\n",
            "Training Loss: 3.5373\n",
            "Validation Loss: 3.7052\n",
            "\n",
            "Epoch 25\n",
            "-------------------------------\n",
            "Training Loss: 3.4995\n",
            "Validation Loss: 3.7241\n",
            "\n",
            "Epoch 26\n",
            "-------------------------------\n",
            "Training Loss: 3.4308\n",
            "Validation Loss: 3.6468\n",
            "\n",
            "Epoch 27\n",
            "-------------------------------\n",
            "Training Loss: 3.3714\n",
            "Validation Loss: 3.5959\n",
            "\n",
            "Epoch 28\n",
            "-------------------------------\n",
            "Training Loss: 3.3359\n",
            "Validation Loss: 3.5405\n",
            "\n",
            "Epoch 29\n",
            "-------------------------------\n",
            "Training Loss: 3.2490\n",
            "Validation Loss: 3.5307\n",
            "\n",
            "Epoch 30\n",
            "-------------------------------\n",
            "Training Loss: 3.1867\n",
            "Validation Loss: 3.4813\n",
            "\n",
            "Epoch 31\n",
            "-------------------------------\n",
            "Training Loss: 3.1639\n",
            "Validation Loss: 3.4396\n",
            "\n",
            "Epoch 32\n",
            "-------------------------------\n",
            "Training Loss: 3.0843\n",
            "Validation Loss: 3.2777\n",
            "\n",
            "Epoch 33\n",
            "-------------------------------\n",
            "Training Loss: 3.0131\n",
            "Validation Loss: 3.2033\n",
            "\n",
            "Epoch 34\n",
            "-------------------------------\n",
            "Training Loss: 2.9352\n",
            "Validation Loss: 3.3055\n",
            "\n",
            "Epoch 35\n",
            "-------------------------------\n",
            "Training Loss: 2.9067\n",
            "Validation Loss: 3.1368\n",
            "\n",
            "Epoch 36\n",
            "-------------------------------\n",
            "Training Loss: 2.8432\n",
            "Validation Loss: 3.0707\n",
            "\n",
            "Epoch 37\n",
            "-------------------------------\n",
            "Training Loss: 2.7654\n",
            "Validation Loss: 3.0129\n",
            "\n",
            "Epoch 38\n",
            "-------------------------------\n",
            "Training Loss: 2.7142\n",
            "Validation Loss: 2.9366\n",
            "\n",
            "Epoch 39\n",
            "-------------------------------\n",
            "Training Loss: 2.6175\n",
            "Validation Loss: 2.8319\n",
            "\n",
            "Epoch 40\n",
            "-------------------------------\n",
            "Training Loss: 2.5892\n",
            "Validation Loss: 2.8181\n",
            "\n",
            "Epoch 41\n",
            "-------------------------------\n",
            "Training Loss: 2.5279\n",
            "Validation Loss: 2.7799\n",
            "\n",
            "Epoch 42\n",
            "-------------------------------\n",
            "Training Loss: 2.4431\n",
            "Validation Loss: 2.7228\n",
            "\n",
            "Epoch 43\n",
            "-------------------------------\n",
            "Training Loss: 2.3905\n",
            "Validation Loss: 2.5371\n",
            "\n",
            "Epoch 44\n",
            "-------------------------------\n",
            "Training Loss: 2.3687\n",
            "Validation Loss: 2.5009\n",
            "\n",
            "Epoch 45\n",
            "-------------------------------\n",
            "Training Loss: 2.2411\n",
            "Validation Loss: 2.4823\n",
            "\n",
            "Epoch 46\n",
            "-------------------------------\n",
            "Training Loss: 2.1907\n",
            "Validation Loss: 2.4145\n",
            "\n",
            "Epoch 47\n",
            "-------------------------------\n",
            "Training Loss: 2.1210\n",
            "Validation Loss: 2.2260\n",
            "\n",
            "Epoch 48\n",
            "-------------------------------\n",
            "Training Loss: 2.0695\n",
            "Validation Loss: 2.2726\n",
            "\n",
            "Epoch 49\n",
            "-------------------------------\n",
            "Training Loss: 2.0280\n",
            "Validation Loss: 2.1565\n",
            "\n",
            "Epoch 50\n",
            "-------------------------------\n",
            "Training Loss: 1.9261\n",
            "Validation Loss: 2.1232\n",
            "\n",
            "Epoch 51\n",
            "-------------------------------\n",
            "Training Loss: 1.8840\n",
            "Validation Loss: 1.9848\n",
            "\n",
            "Epoch 52\n",
            "-------------------------------\n",
            "Training Loss: 1.8280\n",
            "Validation Loss: 1.9481\n",
            "\n",
            "Epoch 53\n",
            "-------------------------------\n",
            "Training Loss: 1.7628\n",
            "Validation Loss: 1.8442\n",
            "\n",
            "Epoch 54\n",
            "-------------------------------\n",
            "Training Loss: 1.6918\n",
            "Validation Loss: 1.7468\n",
            "\n",
            "Epoch 55\n",
            "-------------------------------\n",
            "Training Loss: 1.6373\n",
            "Validation Loss: 1.7011\n",
            "\n",
            "Epoch 56\n",
            "-------------------------------\n",
            "Training Loss: 1.5622\n",
            "Validation Loss: 1.7030\n",
            "\n",
            "Epoch 57\n",
            "-------------------------------\n",
            "Training Loss: 1.5338\n",
            "Validation Loss: 1.6633\n",
            "\n",
            "Epoch 58\n",
            "-------------------------------\n",
            "Training Loss: 1.4619\n",
            "Validation Loss: 1.6193\n",
            "\n",
            "Epoch 59\n",
            "-------------------------------\n",
            "Training Loss: 1.3965\n",
            "Validation Loss: 1.5631\n",
            "\n",
            "Epoch 60\n",
            "-------------------------------\n",
            "Training Loss: 1.3353\n",
            "Validation Loss: 1.4556\n",
            "\n",
            "Epoch 61\n",
            "-------------------------------\n",
            "Training Loss: 1.2906\n",
            "Validation Loss: 1.3302\n",
            "\n",
            "Epoch 62\n",
            "-------------------------------\n",
            "Training Loss: 1.2416\n",
            "Validation Loss: 1.3105\n",
            "\n",
            "Epoch 63\n",
            "-------------------------------\n",
            "Training Loss: 1.2118\n",
            "Validation Loss: 1.2335\n",
            "\n",
            "Epoch 64\n",
            "-------------------------------\n",
            "Training Loss: 1.1471\n",
            "Validation Loss: 1.1941\n",
            "\n",
            "Epoch 65\n",
            "-------------------------------\n",
            "Training Loss: 1.0959\n",
            "Validation Loss: 1.1716\n",
            "\n",
            "Epoch 66\n",
            "-------------------------------\n",
            "Training Loss: 1.0711\n",
            "Validation Loss: 1.1035\n",
            "\n",
            "Epoch 67\n",
            "-------------------------------\n",
            "Training Loss: 1.0139\n",
            "Validation Loss: 1.0535\n",
            "\n",
            "Epoch 68\n",
            "-------------------------------\n",
            "Training Loss: 0.9850\n",
            "Validation Loss: 1.0900\n",
            "\n",
            "Epoch 69\n",
            "-------------------------------\n",
            "Training Loss: 0.9413\n",
            "Validation Loss: 1.0278\n",
            "\n",
            "Epoch 70\n",
            "-------------------------------\n",
            "Training Loss: 0.9106\n",
            "Validation Loss: 0.9559\n",
            "\n",
            "Epoch 71\n",
            "-------------------------------\n",
            "Training Loss: 0.8974\n",
            "Validation Loss: 1.0528\n",
            "\n",
            "Epoch 72\n",
            "-------------------------------\n",
            "Training Loss: 0.8558\n",
            "Validation Loss: 0.9191\n",
            "\n",
            "Epoch 73\n",
            "-------------------------------\n",
            "Training Loss: 0.8184\n",
            "Validation Loss: 0.8581\n",
            "\n",
            "Epoch 74\n",
            "-------------------------------\n",
            "Training Loss: 0.7873\n",
            "Validation Loss: 0.8529\n",
            "\n",
            "Epoch 75\n",
            "-------------------------------\n",
            "Training Loss: 0.7619\n",
            "Validation Loss: 0.8031\n",
            "\n",
            "Epoch 76\n",
            "-------------------------------\n",
            "Training Loss: 0.7327\n",
            "Validation Loss: 0.8372\n",
            "\n",
            "Epoch 77\n",
            "-------------------------------\n",
            "Training Loss: 0.7276\n",
            "Validation Loss: 0.7841\n",
            "\n",
            "Epoch 78\n",
            "-------------------------------\n",
            "Training Loss: 0.7036\n",
            "Validation Loss: 0.8351\n",
            "\n",
            "Epoch 79\n",
            "-------------------------------\n",
            "Training Loss: 0.6879\n",
            "Validation Loss: 0.7370\n",
            "\n",
            "Epoch 80\n",
            "-------------------------------\n",
            "Training Loss: 0.6547\n",
            "Validation Loss: 0.7961\n",
            "\n",
            "Epoch 81\n",
            "-------------------------------\n",
            "Training Loss: 0.6425\n",
            "Validation Loss: 0.7208\n",
            "\n",
            "Epoch 82\n",
            "-------------------------------\n",
            "Training Loss: 0.6275\n",
            "Validation Loss: 0.6860\n",
            "\n",
            "Epoch 83\n",
            "-------------------------------\n",
            "Training Loss: 0.6019\n",
            "Validation Loss: 0.6697\n",
            "\n",
            "Epoch 84\n",
            "-------------------------------\n",
            "Training Loss: 0.5883\n",
            "Validation Loss: 0.6860\n",
            "\n",
            "Epoch 85\n",
            "-------------------------------\n",
            "Training Loss: 0.5760\n",
            "Validation Loss: 0.6302\n",
            "\n",
            "Epoch 86\n",
            "-------------------------------\n",
            "Training Loss: 0.5665\n",
            "Validation Loss: 0.6373\n",
            "\n",
            "Epoch 87\n",
            "-------------------------------\n",
            "Training Loss: 0.5475\n",
            "Validation Loss: 0.6299\n",
            "\n",
            "Epoch 88\n",
            "-------------------------------\n",
            "Training Loss: 0.5381\n",
            "Validation Loss: 0.6000\n",
            "\n",
            "Epoch 89\n",
            "-------------------------------\n",
            "Training Loss: 0.5458\n",
            "Validation Loss: 0.6228\n",
            "\n",
            "Epoch 90\n",
            "-------------------------------\n",
            "Training Loss: 0.5206\n",
            "Validation Loss: 0.5800\n",
            "\n",
            "Epoch 91\n",
            "-------------------------------\n",
            "Training Loss: 0.5146\n",
            "Validation Loss: 0.5781\n",
            "\n",
            "Epoch 92\n",
            "-------------------------------\n",
            "Training Loss: 0.4880\n",
            "Validation Loss: 0.5673\n",
            "\n",
            "Epoch 93\n",
            "-------------------------------\n",
            "Training Loss: 0.4824\n",
            "Validation Loss: 0.5625\n",
            "\n",
            "Epoch 94\n",
            "-------------------------------\n",
            "Training Loss: 0.4795\n",
            "Validation Loss: 0.5518\n",
            "\n",
            "Epoch 95\n",
            "-------------------------------\n",
            "Training Loss: 0.4712\n",
            "Validation Loss: 0.5358\n",
            "\n",
            "Epoch 96\n",
            "-------------------------------\n",
            "Training Loss: 0.4637\n",
            "Validation Loss: 0.5400\n",
            "\n",
            "Epoch 97\n",
            "-------------------------------\n",
            "Training Loss: 0.4496\n",
            "Validation Loss: 0.5412\n",
            "\n",
            "Epoch 98\n",
            "-------------------------------\n",
            "Training Loss: 0.4362\n",
            "Validation Loss: 0.5296\n",
            "\n",
            "Epoch 99\n",
            "-------------------------------\n",
            "Training Loss: 0.4321\n",
            "Validation Loss: 0.5765\n",
            "\n",
            "Epoch 100\n",
            "-------------------------------\n",
            "Training Loss: 0.4347\n",
            "Validation Loss: 0.4985\n",
            "\n",
            "Epoch 101\n",
            "-------------------------------\n",
            "Training Loss: 0.4182\n",
            "Validation Loss: 0.5382\n",
            "\n",
            "Epoch 102\n",
            "-------------------------------\n",
            "Training Loss: 0.4224\n",
            "Validation Loss: 0.4890\n",
            "\n",
            "Epoch 103\n",
            "-------------------------------\n",
            "Training Loss: 0.4042\n",
            "Validation Loss: 0.4765\n",
            "\n",
            "Epoch 104\n",
            "-------------------------------\n",
            "Training Loss: 0.4072\n",
            "Validation Loss: 0.4954\n",
            "\n",
            "Epoch 105\n",
            "-------------------------------\n",
            "Training Loss: 0.4027\n",
            "Validation Loss: 0.5510\n",
            "\n",
            "Epoch 106\n",
            "-------------------------------\n",
            "Training Loss: 0.4007\n",
            "Validation Loss: 0.4754\n",
            "\n",
            "Epoch 107\n",
            "-------------------------------\n",
            "Training Loss: 0.4101\n",
            "Validation Loss: 0.4572\n",
            "\n",
            "Epoch 108\n",
            "-------------------------------\n",
            "Training Loss: 0.3887\n",
            "Validation Loss: 0.4465\n",
            "\n",
            "Epoch 109\n",
            "-------------------------------\n",
            "Training Loss: 0.3890\n",
            "Validation Loss: 0.4544\n",
            "\n",
            "Epoch 110\n",
            "-------------------------------\n",
            "Training Loss: 0.3826\n",
            "Validation Loss: 0.4380\n",
            "\n",
            "Epoch 111\n",
            "-------------------------------\n",
            "Training Loss: 0.3795\n",
            "Validation Loss: 0.4378\n",
            "\n",
            "Epoch 112\n",
            "-------------------------------\n",
            "Training Loss: 0.3788\n",
            "Validation Loss: 0.4486\n",
            "\n",
            "Epoch 113\n",
            "-------------------------------\n",
            "Training Loss: 0.3807\n",
            "Validation Loss: 0.4271\n",
            "\n",
            "Epoch 114\n",
            "-------------------------------\n",
            "Training Loss: 0.3694\n",
            "Validation Loss: 0.4543\n",
            "\n",
            "Epoch 115\n",
            "-------------------------------\n",
            "Training Loss: 0.3628\n",
            "Validation Loss: 0.4565\n",
            "\n",
            "Epoch 116\n",
            "-------------------------------\n",
            "Training Loss: 0.3637\n",
            "Validation Loss: 0.4152\n",
            "\n",
            "Epoch 117\n",
            "-------------------------------\n",
            "Training Loss: 0.3598\n",
            "Validation Loss: 0.4921\n",
            "\n",
            "Epoch 118\n",
            "-------------------------------\n",
            "Training Loss: 0.3545\n",
            "Validation Loss: 0.4360\n",
            "\n",
            "Epoch 119\n",
            "-------------------------------\n",
            "Training Loss: 0.3496\n",
            "Validation Loss: 0.4184\n",
            "\n",
            "Epoch 120\n",
            "-------------------------------\n",
            "Training Loss: 0.3482\n",
            "Validation Loss: 0.4103\n",
            "\n",
            "Epoch 121\n",
            "-------------------------------\n",
            "Training Loss: 0.3438\n",
            "Validation Loss: 0.4077\n",
            "\n",
            "Epoch 122\n",
            "-------------------------------\n",
            "Training Loss: 0.3557\n",
            "Validation Loss: 0.4281\n",
            "\n",
            "Epoch 123\n",
            "-------------------------------\n",
            "Training Loss: 0.3390\n",
            "Validation Loss: 0.4190\n",
            "\n",
            "Epoch 124\n",
            "-------------------------------\n",
            "Training Loss: 0.3371\n",
            "Validation Loss: 0.4315\n",
            "\n",
            "Epoch 125\n",
            "-------------------------------\n",
            "Training Loss: 0.3367\n",
            "Validation Loss: 0.3974\n",
            "\n",
            "Epoch 126\n",
            "-------------------------------\n",
            "Training Loss: 0.3310\n",
            "Validation Loss: 0.3869\n",
            "\n",
            "Epoch 127\n",
            "-------------------------------\n",
            "Training Loss: 0.3323\n",
            "Validation Loss: 0.3944\n",
            "\n",
            "Epoch 128\n",
            "-------------------------------\n",
            "Training Loss: 0.3363\n",
            "Validation Loss: 0.3987\n",
            "\n",
            "Epoch 129\n",
            "-------------------------------\n",
            "Training Loss: 0.3238\n",
            "Validation Loss: 0.3964\n",
            "\n",
            "Epoch 130\n",
            "-------------------------------\n",
            "Training Loss: 0.3291\n",
            "Validation Loss: 0.3877\n",
            "\n",
            "Epoch 131\n",
            "-------------------------------\n",
            "Training Loss: 0.3204\n",
            "Validation Loss: 0.4393\n",
            "\n",
            "Epoch 132\n",
            "-------------------------------\n",
            "Training Loss: 0.3266\n",
            "Validation Loss: 0.3864\n",
            "\n",
            "Epoch 133\n",
            "-------------------------------\n",
            "Training Loss: 0.3317\n",
            "Validation Loss: 0.3724\n",
            "\n",
            "Epoch 134\n",
            "-------------------------------\n",
            "Training Loss: 0.3192\n",
            "Validation Loss: 0.3862\n",
            "\n",
            "Epoch 135\n",
            "-------------------------------\n",
            "Training Loss: 0.3212\n",
            "Validation Loss: 0.3673\n",
            "\n",
            "Epoch 136\n",
            "-------------------------------\n",
            "Training Loss: 0.3137\n",
            "Validation Loss: 0.3633\n",
            "\n",
            "Epoch 137\n",
            "-------------------------------\n",
            "Training Loss: 0.3135\n",
            "Validation Loss: 0.3984\n",
            "\n",
            "Epoch 138\n",
            "-------------------------------\n",
            "Training Loss: 0.3164\n",
            "Validation Loss: 0.3677\n",
            "\n",
            "Epoch 139\n",
            "-------------------------------\n",
            "Training Loss: 0.3119\n",
            "Validation Loss: 0.3616\n",
            "\n",
            "Epoch 140\n",
            "-------------------------------\n",
            "Training Loss: 0.3228\n",
            "Validation Loss: 0.4237\n",
            "\n",
            "Epoch 141\n",
            "-------------------------------\n",
            "Training Loss: 0.3161\n",
            "Validation Loss: 0.3887\n",
            "\n",
            "Epoch 142\n",
            "-------------------------------\n",
            "Training Loss: 0.3062\n",
            "Validation Loss: 0.3682\n",
            "\n",
            "Epoch 143\n",
            "-------------------------------\n",
            "Training Loss: 0.3164\n",
            "Validation Loss: 0.3588\n",
            "\n",
            "Epoch 144\n",
            "-------------------------------\n",
            "Training Loss: 0.3167\n",
            "Validation Loss: 0.3824\n",
            "\n",
            "Epoch 145\n",
            "-------------------------------\n",
            "Training Loss: 0.3066\n",
            "Validation Loss: 0.3696\n",
            "\n",
            "Epoch 146\n",
            "-------------------------------\n",
            "Training Loss: 0.3139\n",
            "Validation Loss: 0.3605\n",
            "\n",
            "Epoch 147\n",
            "-------------------------------\n",
            "Training Loss: 0.3038\n",
            "Validation Loss: 0.3600\n",
            "\n",
            "Epoch 148\n",
            "-------------------------------\n",
            "Training Loss: 0.3054\n",
            "Validation Loss: 0.3663\n",
            "\n",
            "Epoch 149\n",
            "-------------------------------\n",
            "Training Loss: 0.3006\n",
            "Validation Loss: 0.3585\n",
            "\n",
            "Epoch 150\n",
            "-------------------------------\n",
            "Training Loss: 0.2990\n",
            "Validation Loss: 0.3617\n",
            "\n",
            "Epoch 151\n",
            "-------------------------------\n",
            "Training Loss: 0.3142\n",
            "Validation Loss: 0.3453\n",
            "\n",
            "Epoch 152\n",
            "-------------------------------\n",
            "Training Loss: 0.3018\n",
            "Validation Loss: 0.3613\n",
            "\n",
            "Epoch 153\n",
            "-------------------------------\n",
            "Training Loss: 0.3110\n",
            "Validation Loss: 0.3445\n",
            "\n",
            "Epoch 154\n",
            "-------------------------------\n",
            "Training Loss: 0.3013\n",
            "Validation Loss: 0.3779\n",
            "\n",
            "Epoch 155\n",
            "-------------------------------\n",
            "Training Loss: 0.2951\n",
            "Validation Loss: 0.3417\n",
            "\n",
            "Epoch 156\n",
            "-------------------------------\n",
            "Training Loss: 0.3040\n",
            "Validation Loss: 0.3460\n",
            "\n",
            "Epoch 157\n",
            "-------------------------------\n",
            "Training Loss: 0.3052\n",
            "Validation Loss: 0.3421\n",
            "\n",
            "Epoch 158\n",
            "-------------------------------\n",
            "Training Loss: 0.2946\n",
            "Validation Loss: 0.3339\n",
            "\n",
            "Epoch 159\n",
            "-------------------------------\n",
            "Training Loss: 0.3069\n",
            "Validation Loss: 0.4064\n",
            "\n",
            "Epoch 160\n",
            "-------------------------------\n",
            "Training Loss: 0.2977\n",
            "Validation Loss: 0.3563\n",
            "\n",
            "Epoch 161\n",
            "-------------------------------\n",
            "Training Loss: 0.3008\n",
            "Validation Loss: 0.3471\n",
            "\n",
            "Epoch 162\n",
            "-------------------------------\n",
            "Training Loss: 0.3020\n",
            "Validation Loss: 0.3452\n",
            "\n",
            "Epoch 163\n",
            "-------------------------------\n",
            "Training Loss: 0.2898\n",
            "Validation Loss: 0.3416\n",
            "\n",
            "Epoch 164\n",
            "-------------------------------\n",
            "Training Loss: 0.2879\n",
            "Validation Loss: 0.3429\n",
            "\n",
            "Epoch 165\n",
            "-------------------------------\n",
            "Training Loss: 0.2927\n",
            "Validation Loss: 0.3706\n",
            "\n",
            "Epoch 166\n",
            "-------------------------------\n",
            "Training Loss: 0.2960\n",
            "Validation Loss: 0.3307\n",
            "\n",
            "Epoch 167\n",
            "-------------------------------\n",
            "Training Loss: 0.2903\n",
            "Validation Loss: 0.3781\n",
            "\n",
            "Epoch 168\n",
            "-------------------------------\n",
            "Training Loss: 0.2937\n",
            "Validation Loss: 0.3724\n",
            "\n",
            "Epoch 169\n",
            "-------------------------------\n",
            "Training Loss: 0.3012\n",
            "Validation Loss: 0.3580\n",
            "\n",
            "Epoch 170\n",
            "-------------------------------\n",
            "Training Loss: 0.2910\n",
            "Validation Loss: 0.4083\n",
            "\n",
            "Epoch 171\n",
            "-------------------------------\n",
            "Training Loss: 0.3035\n",
            "Validation Loss: 0.3356\n",
            "\n",
            "Epoch 172\n",
            "-------------------------------\n",
            "Training Loss: 0.2935\n",
            "Validation Loss: 0.3431\n",
            "\n",
            "Epoch 173\n",
            "-------------------------------\n",
            "Training Loss: 0.2935\n",
            "Validation Loss: 0.3653\n",
            "\n",
            "Epoch 174\n",
            "-------------------------------\n",
            "Training Loss: 0.2845\n",
            "Validation Loss: 0.3342\n",
            "\n",
            "Epoch 175\n",
            "-------------------------------\n",
            "Training Loss: 0.2825\n",
            "Validation Loss: 0.3320\n",
            "\n",
            "Epoch 176\n",
            "-------------------------------\n",
            "Training Loss: 0.2957\n",
            "Validation Loss: 0.3462\n",
            "\n",
            "Epoch 177\n",
            "-------------------------------\n",
            "Training Loss: 0.2955\n",
            "Validation Loss: 0.3286\n",
            "\n",
            "Epoch 178\n",
            "-------------------------------\n",
            "Training Loss: 0.2856\n",
            "Validation Loss: 0.3266\n",
            "\n",
            "Epoch 179\n",
            "-------------------------------\n",
            "Training Loss: 0.2885\n",
            "Validation Loss: 0.3373\n",
            "\n",
            "Epoch 180\n",
            "-------------------------------\n",
            "Training Loss: 0.2790\n",
            "Validation Loss: 0.3234\n",
            "\n",
            "Epoch 181\n",
            "-------------------------------\n",
            "Training Loss: 0.2813\n",
            "Validation Loss: 0.3374\n",
            "\n",
            "Epoch 182\n",
            "-------------------------------\n",
            "Training Loss: 0.2847\n",
            "Validation Loss: 0.3301\n",
            "\n",
            "Epoch 183\n",
            "-------------------------------\n",
            "Training Loss: 0.2966\n",
            "Validation Loss: 0.3179\n",
            "\n",
            "Epoch 184\n",
            "-------------------------------\n",
            "Training Loss: 0.2793\n",
            "Validation Loss: 0.3359\n",
            "\n",
            "Epoch 185\n",
            "-------------------------------\n",
            "Training Loss: 0.2835\n",
            "Validation Loss: 0.3413\n",
            "\n",
            "Epoch 186\n",
            "-------------------------------\n",
            "Training Loss: 0.2784\n",
            "Validation Loss: 0.3231\n",
            "\n",
            "Epoch 187\n",
            "-------------------------------\n",
            "Training Loss: 0.2735\n",
            "Validation Loss: 0.4061\n",
            "\n",
            "Epoch 188\n",
            "-------------------------------\n",
            "Training Loss: 0.2851\n",
            "Validation Loss: 0.3359\n",
            "\n",
            "Epoch 189\n",
            "-------------------------------\n",
            "Training Loss: 0.2835\n",
            "Validation Loss: 0.3201\n",
            "\n",
            "Epoch 190\n",
            "-------------------------------\n",
            "Training Loss: 0.2832\n",
            "Validation Loss: 0.3281\n",
            "\n",
            "Epoch 191\n",
            "-------------------------------\n",
            "Training Loss: 0.2882\n",
            "Validation Loss: 0.3248\n",
            "\n",
            "Epoch 192\n",
            "-------------------------------\n",
            "Training Loss: 0.2808\n",
            "Validation Loss: 0.3635\n",
            "\n",
            "Epoch 193\n",
            "-------------------------------\n",
            "Training Loss: 0.2802\n",
            "Validation Loss: 0.3340\n",
            "\n",
            "Epoch 194\n",
            "-------------------------------\n",
            "Training Loss: 0.2772\n",
            "Validation Loss: 0.3319\n",
            "\n",
            "Epoch 195\n",
            "-------------------------------\n",
            "Training Loss: 0.2763\n",
            "Validation Loss: 0.3257\n",
            "\n",
            "Epoch 196\n",
            "-------------------------------\n",
            "Training Loss: 0.2773\n",
            "Validation Loss: 0.3217\n",
            "\n",
            "Epoch 197\n",
            "-------------------------------\n",
            "Training Loss: 0.2767\n",
            "Validation Loss: 0.3144\n",
            "\n",
            "Epoch 198\n",
            "-------------------------------\n",
            "Training Loss: 0.2786\n",
            "Validation Loss: 0.3181\n",
            "\n",
            "Epoch 199\n",
            "-------------------------------\n",
            "Training Loss: 0.2685\n",
            "Validation Loss: 0.3170\n",
            "\n",
            "Epoch 200\n",
            "-------------------------------\n",
            "Training Loss: 0.2791\n",
            "Validation Loss: 0.3137\n",
            "\n",
            "Epoch 201\n",
            "-------------------------------\n",
            "Training Loss: 0.2690\n",
            "Validation Loss: 0.3265\n",
            "\n",
            "Epoch 202\n",
            "-------------------------------\n",
            "Training Loss: 0.2676\n",
            "Validation Loss: 0.3555\n",
            "\n",
            "Epoch 203\n",
            "-------------------------------\n",
            "Training Loss: 0.2661\n",
            "Validation Loss: 0.3115\n",
            "\n",
            "Epoch 204\n",
            "-------------------------------\n",
            "Training Loss: 0.2674\n",
            "Validation Loss: 0.4173\n",
            "\n",
            "Epoch 205\n",
            "-------------------------------\n",
            "Training Loss: 0.2701\n",
            "Validation Loss: 0.3138\n",
            "\n",
            "Epoch 206\n",
            "-------------------------------\n",
            "Training Loss: 0.2648\n",
            "Validation Loss: 0.3338\n",
            "\n",
            "Epoch 207\n",
            "-------------------------------\n",
            "Training Loss: 0.2693\n",
            "Validation Loss: 0.3047\n",
            "\n",
            "Epoch 208\n",
            "-------------------------------\n",
            "Training Loss: 0.2676\n",
            "Validation Loss: 0.3808\n",
            "\n",
            "Epoch 209\n",
            "-------------------------------\n",
            "Training Loss: 0.2830\n",
            "Validation Loss: 0.3428\n",
            "\n",
            "Epoch 210\n",
            "-------------------------------\n",
            "Training Loss: 0.2709\n",
            "Validation Loss: 0.3088\n",
            "\n",
            "Epoch 211\n",
            "-------------------------------\n",
            "Training Loss: 0.2774\n",
            "Validation Loss: 0.3114\n",
            "\n",
            "Epoch 212\n",
            "-------------------------------\n",
            "Training Loss: 0.2800\n",
            "Validation Loss: 0.3200\n",
            "\n",
            "Epoch 213\n",
            "-------------------------------\n",
            "Training Loss: 0.2716\n",
            "Validation Loss: 0.3031\n",
            "\n",
            "Epoch 214\n",
            "-------------------------------\n",
            "Training Loss: 0.2703\n",
            "Validation Loss: 0.3305\n",
            "\n",
            "Epoch 215\n",
            "-------------------------------\n",
            "Training Loss: 0.2689\n",
            "Validation Loss: 0.3298\n",
            "\n",
            "Epoch 216\n",
            "-------------------------------\n",
            "Training Loss: 0.2684\n",
            "Validation Loss: 0.3125\n",
            "\n",
            "Epoch 217\n",
            "-------------------------------\n",
            "Training Loss: 0.2630\n",
            "Validation Loss: 0.3193\n",
            "\n",
            "Epoch 218\n",
            "-------------------------------\n",
            "Training Loss: 0.2671\n",
            "Validation Loss: 0.3054\n",
            "\n",
            "Epoch 219\n",
            "-------------------------------\n",
            "Training Loss: 0.2673\n",
            "Validation Loss: 0.3275\n",
            "\n",
            "Epoch 220\n",
            "-------------------------------\n",
            "Training Loss: 0.2672\n",
            "Validation Loss: 0.3211\n",
            "\n",
            "Epoch 221\n",
            "-------------------------------\n",
            "Training Loss: 0.2600\n",
            "Validation Loss: 0.3094\n",
            "\n",
            "Epoch 222\n",
            "-------------------------------\n",
            "Training Loss: 0.2636\n",
            "Validation Loss: 0.3068\n",
            "\n",
            "Epoch 223\n",
            "-------------------------------\n",
            "Training Loss: 0.2649\n",
            "Validation Loss: 0.3057\n",
            "\n",
            "Epoch 224\n",
            "-------------------------------\n",
            "Training Loss: 0.2616\n",
            "Validation Loss: 0.3026\n",
            "\n",
            "Epoch 225\n",
            "-------------------------------\n",
            "Training Loss: 0.2593\n",
            "Validation Loss: 0.3065\n",
            "\n",
            "Epoch 226\n",
            "-------------------------------\n",
            "Training Loss: 0.2681\n",
            "Validation Loss: 0.3551\n",
            "\n",
            "Epoch 227\n",
            "-------------------------------\n",
            "Training Loss: 0.2679\n",
            "Validation Loss: 0.3008\n",
            "\n",
            "Epoch 228\n",
            "-------------------------------\n",
            "Training Loss: 0.2682\n",
            "Validation Loss: 0.3118\n",
            "\n",
            "Epoch 229\n",
            "-------------------------------\n",
            "Training Loss: 0.2628\n",
            "Validation Loss: 0.3656\n",
            "\n",
            "Epoch 230\n",
            "-------------------------------\n",
            "Training Loss: 0.2682\n",
            "Validation Loss: 0.3115\n",
            "\n",
            "Epoch 231\n",
            "-------------------------------\n",
            "Training Loss: 0.2621\n",
            "Validation Loss: 0.3000\n",
            "\n",
            "Epoch 232\n",
            "-------------------------------\n",
            "Training Loss: 0.2578\n",
            "Validation Loss: 0.2921\n",
            "\n",
            "Epoch 233\n",
            "-------------------------------\n",
            "Training Loss: 0.2573\n",
            "Validation Loss: 0.2940\n",
            "\n",
            "Epoch 234\n",
            "-------------------------------\n",
            "Training Loss: 0.2636\n",
            "Validation Loss: 0.3098\n",
            "\n",
            "Epoch 235\n",
            "-------------------------------\n",
            "Training Loss: 0.2568\n",
            "Validation Loss: 0.2925\n",
            "\n",
            "Epoch 236\n",
            "-------------------------------\n",
            "Training Loss: 0.2627\n",
            "Validation Loss: 0.2918\n",
            "\n",
            "Epoch 237\n",
            "-------------------------------\n",
            "Training Loss: 0.2589\n",
            "Validation Loss: 0.3105\n",
            "\n",
            "Epoch 238\n",
            "-------------------------------\n",
            "Training Loss: 0.2596\n",
            "Validation Loss: 0.3232\n",
            "\n",
            "Epoch 239\n",
            "-------------------------------\n",
            "Training Loss: 0.2609\n",
            "Validation Loss: 0.2990\n",
            "\n",
            "Epoch 240\n",
            "-------------------------------\n",
            "Training Loss: 0.2628\n",
            "Validation Loss: 0.2961\n",
            "\n",
            "Epoch 241\n",
            "-------------------------------\n",
            "Training Loss: 0.2697\n",
            "Validation Loss: 0.3071\n",
            "\n",
            "Epoch 242\n",
            "-------------------------------\n",
            "Training Loss: 0.2584\n",
            "Validation Loss: 0.2920\n",
            "\n",
            "Epoch 243\n",
            "-------------------------------\n",
            "Training Loss: 0.2579\n",
            "Validation Loss: 0.3021\n",
            "\n",
            "Epoch 244\n",
            "-------------------------------\n",
            "Training Loss: 0.2584\n",
            "Validation Loss: 0.3101\n",
            "\n",
            "Epoch 245\n",
            "-------------------------------\n",
            "Training Loss: 0.2536\n",
            "Validation Loss: 0.3107\n",
            "\n",
            "Epoch 246\n",
            "-------------------------------\n",
            "Training Loss: 0.2630\n",
            "Validation Loss: 0.2962\n",
            "\n",
            "Epoch 247\n",
            "-------------------------------\n",
            "Training Loss: 0.2599\n",
            "Validation Loss: 0.3065\n",
            "\n",
            "Epoch 248\n",
            "-------------------------------\n",
            "Training Loss: 0.2555\n",
            "Validation Loss: 0.3217\n",
            "\n",
            "Epoch 249\n",
            "-------------------------------\n",
            "Training Loss: 0.2544\n",
            "Validation Loss: 0.3010\n",
            "\n",
            "Epoch 250\n",
            "-------------------------------\n",
            "Training Loss: 0.2603\n",
            "Validation Loss: 0.3040\n",
            "\n",
            "Epoch 251\n",
            "-------------------------------\n",
            "Training Loss: 0.2629\n",
            "Validation Loss: 0.2899\n",
            "\n",
            "Epoch 252\n",
            "-------------------------------\n",
            "Training Loss: 0.2514\n",
            "Validation Loss: 0.3145\n",
            "\n",
            "Epoch 253\n",
            "-------------------------------\n",
            "Training Loss: 0.2588\n",
            "Validation Loss: 0.2962\n",
            "\n",
            "Epoch 254\n",
            "-------------------------------\n",
            "Training Loss: 0.2530\n",
            "Validation Loss: 0.2867\n",
            "\n",
            "Epoch 255\n",
            "-------------------------------\n",
            "Training Loss: 0.2640\n",
            "Validation Loss: 0.2845\n",
            "\n",
            "Epoch 256\n",
            "-------------------------------\n",
            "Training Loss: 0.2516\n",
            "Validation Loss: 0.2835\n",
            "\n",
            "Epoch 257\n",
            "-------------------------------\n",
            "Training Loss: 0.2511\n",
            "Validation Loss: 0.2972\n",
            "\n",
            "Epoch 258\n",
            "-------------------------------\n",
            "Training Loss: 0.2532\n",
            "Validation Loss: 0.2943\n",
            "\n",
            "Epoch 259\n",
            "-------------------------------\n",
            "Training Loss: 0.2525\n",
            "Validation Loss: 0.2996\n",
            "\n",
            "Epoch 260\n",
            "-------------------------------\n",
            "Training Loss: 0.2565\n",
            "Validation Loss: 0.2863\n",
            "\n",
            "Epoch 261\n",
            "-------------------------------\n",
            "Training Loss: 0.2486\n",
            "Validation Loss: 0.2913\n",
            "\n",
            "Epoch 262\n",
            "-------------------------------\n",
            "Training Loss: 0.2496\n",
            "Validation Loss: 0.3086\n",
            "\n",
            "Epoch 263\n",
            "-------------------------------\n",
            "Training Loss: 0.2582\n",
            "Validation Loss: 0.3081\n",
            "\n",
            "Epoch 264\n",
            "-------------------------------\n",
            "Training Loss: 0.2483\n",
            "Validation Loss: 0.3181\n",
            "\n",
            "Epoch 265\n",
            "-------------------------------\n",
            "Training Loss: 0.2505\n",
            "Validation Loss: 0.2932\n",
            "\n",
            "Epoch 266\n",
            "-------------------------------\n",
            "Training Loss: 0.2550\n",
            "Validation Loss: 0.2810\n",
            "\n",
            "Epoch 267\n",
            "-------------------------------\n",
            "Training Loss: 0.2486\n",
            "Validation Loss: 0.2909\n",
            "\n",
            "Epoch 268\n",
            "-------------------------------\n",
            "Training Loss: 0.2574\n",
            "Validation Loss: 0.2868\n",
            "\n",
            "Epoch 269\n",
            "-------------------------------\n",
            "Training Loss: 0.2512\n",
            "Validation Loss: 0.2806\n",
            "\n",
            "Epoch 270\n",
            "-------------------------------\n",
            "Training Loss: 0.2467\n",
            "Validation Loss: 0.3292\n",
            "\n",
            "Epoch 271\n",
            "-------------------------------\n",
            "Training Loss: 0.2462\n",
            "Validation Loss: 0.3101\n",
            "\n",
            "Epoch 272\n",
            "-------------------------------\n",
            "Training Loss: 0.2642\n",
            "Validation Loss: 0.2820\n",
            "\n",
            "Epoch 273\n",
            "-------------------------------\n",
            "Training Loss: 0.2519\n",
            "Validation Loss: 0.2895\n",
            "\n",
            "Epoch 274\n",
            "-------------------------------\n",
            "Training Loss: 0.2466\n",
            "Validation Loss: 0.3094\n",
            "\n",
            "Epoch 275\n",
            "-------------------------------\n",
            "Training Loss: 0.2514\n",
            "Validation Loss: 0.2860\n",
            "\n",
            "Epoch 276\n",
            "-------------------------------\n",
            "Training Loss: 0.2499\n",
            "Validation Loss: 0.2821\n",
            "\n",
            "Epoch 277\n",
            "-------------------------------\n",
            "Training Loss: 0.2488\n",
            "Validation Loss: 0.2853\n",
            "\n",
            "Epoch 278\n",
            "-------------------------------\n",
            "Training Loss: 0.2479\n",
            "Validation Loss: 0.2851\n",
            "\n",
            "Epoch 279\n",
            "-------------------------------\n",
            "Training Loss: 0.2427\n",
            "Validation Loss: 0.2817\n",
            "\n",
            "Epoch 280\n",
            "-------------------------------\n",
            "Training Loss: 0.2550\n",
            "Validation Loss: 0.2834\n",
            "\n",
            "Epoch 281\n",
            "-------------------------------\n",
            "Training Loss: 0.2398\n",
            "Validation Loss: 0.2774\n",
            "\n",
            "Epoch 282\n",
            "-------------------------------\n",
            "Training Loss: 0.2475\n",
            "Validation Loss: 0.2901\n",
            "\n",
            "Epoch 283\n",
            "-------------------------------\n",
            "Training Loss: 0.2586\n",
            "Validation Loss: 0.3140\n",
            "\n",
            "Epoch 284\n",
            "-------------------------------\n",
            "Training Loss: 0.2606\n",
            "Validation Loss: 0.2807\n",
            "\n",
            "Epoch 285\n",
            "-------------------------------\n",
            "Training Loss: 0.2518\n",
            "Validation Loss: 0.2758\n",
            "\n",
            "Epoch 286\n",
            "-------------------------------\n",
            "Training Loss: 0.2540\n",
            "Validation Loss: 0.2940\n",
            "\n",
            "Epoch 287\n",
            "-------------------------------\n",
            "Training Loss: 0.2472\n",
            "Validation Loss: 0.2785\n",
            "\n",
            "Epoch 288\n",
            "-------------------------------\n",
            "Training Loss: 0.2470\n",
            "Validation Loss: 0.2799\n",
            "\n",
            "Epoch 289\n",
            "-------------------------------\n",
            "Training Loss: 0.2513\n",
            "Validation Loss: 0.2810\n",
            "\n",
            "Epoch 290\n",
            "-------------------------------\n",
            "Training Loss: 0.2498\n",
            "Validation Loss: 0.2947\n",
            "\n",
            "Epoch 291\n",
            "-------------------------------\n",
            "Training Loss: 0.2432\n",
            "Validation Loss: 0.2891\n",
            "\n",
            "Epoch 292\n",
            "-------------------------------\n",
            "Training Loss: 0.2476\n",
            "Validation Loss: 0.2918\n",
            "\n",
            "Epoch 293\n",
            "-------------------------------\n",
            "Training Loss: 0.2455\n",
            "Validation Loss: 0.2841\n",
            "\n",
            "Epoch 294\n",
            "-------------------------------\n",
            "Training Loss: 0.2467\n",
            "Validation Loss: 0.2727\n",
            "\n",
            "Epoch 295\n",
            "-------------------------------\n",
            "Training Loss: 0.2378\n",
            "Validation Loss: 0.2712\n",
            "\n",
            "Epoch 296\n",
            "-------------------------------\n",
            "Training Loss: 0.2438\n",
            "Validation Loss: 0.2942\n",
            "\n",
            "Epoch 297\n",
            "-------------------------------\n",
            "Training Loss: 0.2451\n",
            "Validation Loss: 0.2894\n",
            "\n",
            "Epoch 298\n",
            "-------------------------------\n",
            "Training Loss: 0.2428\n",
            "Validation Loss: 0.2875\n",
            "\n",
            "Epoch 299\n",
            "-------------------------------\n",
            "Training Loss: 0.2409\n",
            "Validation Loss: 0.2757\n",
            "\n",
            "Epoch 300\n",
            "-------------------------------\n",
            "Training Loss: 0.2424\n",
            "Validation Loss: 0.3323\n",
            "\n",
            "Epoch 301\n",
            "-------------------------------\n",
            "Training Loss: 0.2579\n",
            "Validation Loss: 0.2785\n",
            "\n",
            "Epoch 302\n",
            "-------------------------------\n",
            "Training Loss: 0.2488\n",
            "Validation Loss: 0.3058\n",
            "\n",
            "Epoch 303\n",
            "-------------------------------\n",
            "Training Loss: 0.2462\n",
            "Validation Loss: 0.2884\n",
            "\n",
            "Epoch 304\n",
            "-------------------------------\n",
            "Training Loss: 0.2409\n",
            "Validation Loss: 0.3070\n",
            "\n",
            "Epoch 305\n",
            "-------------------------------\n",
            "Training Loss: 0.2399\n",
            "Validation Loss: 0.2772\n",
            "\n",
            "Epoch 306\n",
            "-------------------------------\n",
            "Training Loss: 0.2397\n",
            "Validation Loss: 0.2896\n",
            "\n",
            "Epoch 307\n",
            "-------------------------------\n",
            "Training Loss: 0.2364\n",
            "Validation Loss: 0.2915\n",
            "\n",
            "Epoch 308\n",
            "-------------------------------\n",
            "Training Loss: 0.2460\n",
            "Validation Loss: 0.2780\n",
            "\n",
            "Epoch 309\n",
            "-------------------------------\n",
            "Training Loss: 0.2408\n",
            "Validation Loss: 0.3012\n",
            "\n",
            "Epoch 310\n",
            "-------------------------------\n",
            "Training Loss: 0.2423\n",
            "Validation Loss: 0.2823\n",
            "\n",
            "Epoch 311\n",
            "-------------------------------\n",
            "Training Loss: 0.2432\n",
            "Validation Loss: 0.2874\n",
            "\n",
            "Epoch 312\n",
            "-------------------------------\n",
            "Training Loss: 0.2397\n",
            "Validation Loss: 0.2867\n",
            "\n",
            "Epoch 313\n",
            "-------------------------------\n",
            "Training Loss: 0.2484\n",
            "Validation Loss: 0.3270\n",
            "\n",
            "Epoch 314\n",
            "-------------------------------\n",
            "Training Loss: 0.2426\n",
            "Validation Loss: 0.2787\n",
            "\n",
            "Epoch 315\n",
            "-------------------------------\n",
            "Training Loss: 0.2406\n",
            "Validation Loss: 0.2879\n",
            "\n",
            "Epoch 316\n",
            "-------------------------------\n",
            "Training Loss: 0.2436\n",
            "Validation Loss: 0.2704\n",
            "\n",
            "Epoch 317\n",
            "-------------------------------\n",
            "Training Loss: 0.2443\n",
            "Validation Loss: 0.2688\n",
            "\n",
            "Epoch 318\n",
            "-------------------------------\n",
            "Training Loss: 0.2392\n",
            "Validation Loss: 0.2717\n",
            "\n",
            "Epoch 319\n",
            "-------------------------------\n",
            "Training Loss: 0.2417\n",
            "Validation Loss: 0.2687\n",
            "\n",
            "Epoch 320\n",
            "-------------------------------\n",
            "Training Loss: 0.2402\n",
            "Validation Loss: 0.2669\n",
            "\n",
            "Epoch 321\n",
            "-------------------------------\n",
            "Training Loss: 0.2427\n",
            "Validation Loss: 0.3384\n",
            "\n",
            "Epoch 322\n",
            "-------------------------------\n",
            "Training Loss: 0.2488\n",
            "Validation Loss: 0.2678\n",
            "\n",
            "Epoch 323\n",
            "-------------------------------\n",
            "Training Loss: 0.2347\n",
            "Validation Loss: 0.2670\n",
            "\n",
            "Epoch 324\n",
            "-------------------------------\n",
            "Training Loss: 0.2446\n",
            "Validation Loss: 0.2985\n",
            "\n",
            "Epoch 325\n",
            "-------------------------------\n",
            "Training Loss: 0.2341\n",
            "Validation Loss: 0.2921\n",
            "\n",
            "Epoch 326\n",
            "-------------------------------\n",
            "Training Loss: 0.2441\n",
            "Validation Loss: 0.3069\n",
            "\n",
            "Epoch 327\n",
            "-------------------------------\n",
            "Training Loss: 0.2464\n",
            "Validation Loss: 0.2795\n",
            "\n",
            "Epoch 328\n",
            "-------------------------------\n",
            "Training Loss: 0.2354\n",
            "Validation Loss: 0.2753\n",
            "\n",
            "Epoch 329\n",
            "-------------------------------\n",
            "Training Loss: 0.2526\n",
            "Validation Loss: 0.2829\n",
            "\n",
            "Epoch 330\n",
            "-------------------------------\n",
            "Training Loss: 0.2440\n",
            "Validation Loss: 0.2820\n",
            "\n",
            "Epoch 331\n",
            "-------------------------------\n",
            "Training Loss: 0.2394\n",
            "Validation Loss: 0.2934\n",
            "\n",
            "Epoch 332\n",
            "-------------------------------\n",
            "Training Loss: 0.2326\n",
            "Validation Loss: 0.2894\n",
            "\n",
            "Epoch 333\n",
            "-------------------------------\n",
            "Training Loss: 0.2484\n",
            "Validation Loss: 0.2970\n",
            "\n",
            "Epoch 334\n",
            "-------------------------------\n",
            "Training Loss: 0.2561\n",
            "Validation Loss: 0.2759\n",
            "\n",
            "Epoch 335\n",
            "-------------------------------\n",
            "Training Loss: 0.2413\n",
            "Validation Loss: 0.2673\n",
            "\n",
            "Epoch 336\n",
            "-------------------------------\n",
            "Training Loss: 0.2332\n",
            "Validation Loss: 0.2677\n",
            "\n",
            "Epoch 337\n",
            "-------------------------------\n",
            "Training Loss: 0.2430\n",
            "Validation Loss: 0.2651\n",
            "\n",
            "Epoch 338\n",
            "-------------------------------\n",
            "Training Loss: 0.2339\n",
            "Validation Loss: 0.2682\n",
            "\n",
            "Epoch 339\n",
            "-------------------------------\n",
            "Training Loss: 0.2363\n",
            "Validation Loss: 0.2685\n",
            "\n",
            "Epoch 340\n",
            "-------------------------------\n",
            "Training Loss: 0.2365\n",
            "Validation Loss: 0.2896\n",
            "\n",
            "Epoch 341\n",
            "-------------------------------\n",
            "Training Loss: 0.2342\n",
            "Validation Loss: 0.2822\n",
            "\n",
            "Epoch 342\n",
            "-------------------------------\n",
            "Training Loss: 0.2390\n",
            "Validation Loss: 0.2786\n",
            "\n",
            "Epoch 343\n",
            "-------------------------------\n",
            "Training Loss: 0.2370\n",
            "Validation Loss: 0.2839\n",
            "\n",
            "Epoch 344\n",
            "-------------------------------\n",
            "Training Loss: 0.2379\n",
            "Validation Loss: 0.2812\n",
            "\n",
            "Epoch 345\n",
            "-------------------------------\n",
            "Training Loss: 0.2399\n",
            "Validation Loss: 0.2935\n",
            "\n",
            "Epoch 346\n",
            "-------------------------------\n",
            "Training Loss: 0.2350\n",
            "Validation Loss: 0.2829\n",
            "\n",
            "Epoch 347\n",
            "-------------------------------\n",
            "Training Loss: 0.2335\n",
            "Validation Loss: 0.2712\n",
            "\n",
            "Epoch 348\n",
            "-------------------------------\n",
            "Training Loss: 0.2340\n",
            "Validation Loss: 0.2787\n",
            "\n",
            "Epoch 349\n",
            "-------------------------------\n",
            "Training Loss: 0.2332\n",
            "Validation Loss: 0.2790\n",
            "\n",
            "Epoch 350\n",
            "-------------------------------\n",
            "Training Loss: 0.2433\n",
            "Validation Loss: 0.2708\n",
            "\n",
            "Epoch 351\n",
            "-------------------------------\n",
            "Training Loss: 0.2383\n",
            "Validation Loss: 0.2734\n",
            "\n",
            "Epoch 352\n",
            "-------------------------------\n",
            "Training Loss: 0.2314\n",
            "Validation Loss: 0.2719\n",
            "\n",
            "Epoch 353\n",
            "-------------------------------\n",
            "Training Loss: 0.2352\n",
            "Validation Loss: 0.2748\n",
            "\n",
            "Epoch 354\n",
            "-------------------------------\n",
            "Training Loss: 0.2336\n",
            "Validation Loss: 0.2710\n",
            "\n",
            "Epoch 355\n",
            "-------------------------------\n",
            "Training Loss: 0.2306\n",
            "Validation Loss: 0.2666\n",
            "\n",
            "Epoch 356\n",
            "-------------------------------\n",
            "Training Loss: 0.2465\n",
            "Validation Loss: 0.2747\n",
            "\n",
            "Epoch 357\n",
            "-------------------------------\n",
            "Training Loss: 0.2334\n",
            "Validation Loss: 0.2644\n",
            "\n",
            "Epoch 358\n",
            "-------------------------------\n",
            "Training Loss: 0.2353\n",
            "Validation Loss: 0.2611\n",
            "\n",
            "Epoch 359\n",
            "-------------------------------\n",
            "Training Loss: 0.2304\n",
            "Validation Loss: 0.2654\n",
            "\n",
            "Epoch 360\n",
            "-------------------------------\n",
            "Training Loss: 0.2365\n",
            "Validation Loss: 0.2640\n",
            "\n",
            "Epoch 361\n",
            "-------------------------------\n",
            "Training Loss: 0.2364\n",
            "Validation Loss: 0.2956\n",
            "\n",
            "Epoch 362\n",
            "-------------------------------\n",
            "Training Loss: 0.2293\n",
            "Validation Loss: 0.2626\n",
            "\n",
            "Epoch 363\n",
            "-------------------------------\n",
            "Training Loss: 0.2431\n",
            "Validation Loss: 0.2752\n",
            "\n",
            "Epoch 364\n",
            "-------------------------------\n",
            "Training Loss: 0.2445\n",
            "Validation Loss: 0.2631\n",
            "\n",
            "Epoch 365\n",
            "-------------------------------\n",
            "Training Loss: 0.2372\n",
            "Validation Loss: 0.2622\n",
            "\n",
            "Epoch 366\n",
            "-------------------------------\n",
            "Training Loss: 0.2310\n",
            "Validation Loss: 0.3160\n",
            "\n",
            "Epoch 367\n",
            "-------------------------------\n",
            "Training Loss: 0.2350\n",
            "Validation Loss: 0.2627\n",
            "\n",
            "Epoch 368\n",
            "-------------------------------\n",
            "Training Loss: 0.2256\n",
            "Validation Loss: 0.2615\n",
            "\n",
            "Epoch 369\n",
            "-------------------------------\n",
            "Training Loss: 0.2316\n",
            "Validation Loss: 0.2710\n",
            "\n",
            "Epoch 370\n",
            "-------------------------------\n",
            "Training Loss: 0.2378\n",
            "Validation Loss: 0.2573\n",
            "\n",
            "Epoch 371\n",
            "-------------------------------\n",
            "Training Loss: 0.2420\n",
            "Validation Loss: 0.3434\n",
            "\n",
            "Epoch 372\n",
            "-------------------------------\n",
            "Training Loss: 0.2331\n",
            "Validation Loss: 0.2790\n",
            "\n",
            "Epoch 373\n",
            "-------------------------------\n",
            "Training Loss: 0.2346\n",
            "Validation Loss: 0.2663\n",
            "\n",
            "Epoch 374\n",
            "-------------------------------\n",
            "Training Loss: 0.2343\n",
            "Validation Loss: 0.2585\n",
            "\n",
            "Epoch 375\n",
            "-------------------------------\n",
            "Training Loss: 0.2432\n",
            "Validation Loss: 0.2665\n",
            "\n",
            "Epoch 376\n",
            "-------------------------------\n",
            "Training Loss: 0.2260\n",
            "Validation Loss: 0.2695\n",
            "\n",
            "Epoch 377\n",
            "-------------------------------\n",
            "Training Loss: 0.2294\n",
            "Validation Loss: 0.2747\n",
            "\n",
            "Epoch 378\n",
            "-------------------------------\n",
            "Training Loss: 0.2313\n",
            "Validation Loss: 0.2956\n",
            "\n",
            "Epoch 379\n",
            "-------------------------------\n",
            "Training Loss: 0.2269\n",
            "Validation Loss: 0.2575\n",
            "\n",
            "Epoch 380\n",
            "-------------------------------\n",
            "Training Loss: 0.2293\n",
            "Validation Loss: 0.2646\n",
            "\n",
            "Epoch 381\n",
            "-------------------------------\n",
            "Training Loss: 0.2331\n",
            "Validation Loss: 0.2875\n",
            "\n",
            "Epoch 382\n",
            "-------------------------------\n",
            "Training Loss: 0.2307\n",
            "Validation Loss: 0.2683\n",
            "\n",
            "Epoch 383\n",
            "-------------------------------\n",
            "Training Loss: 0.2279\n",
            "Validation Loss: 0.2791\n",
            "\n",
            "Epoch 384\n",
            "-------------------------------\n",
            "Training Loss: 0.2424\n",
            "Validation Loss: 0.2667\n",
            "\n",
            "Epoch 385\n",
            "-------------------------------\n",
            "Training Loss: 0.2303\n",
            "Validation Loss: 0.2851\n",
            "\n",
            "Epoch 386\n",
            "-------------------------------\n",
            "Training Loss: 0.2321\n",
            "Validation Loss: 0.2701\n",
            "\n",
            "Epoch 387\n",
            "-------------------------------\n",
            "Training Loss: 0.2269\n",
            "Validation Loss: 0.2856\n",
            "\n",
            "Epoch 388\n",
            "-------------------------------\n",
            "Training Loss: 0.2354\n",
            "Validation Loss: 0.2942\n",
            "\n",
            "Epoch 389\n",
            "-------------------------------\n",
            "Training Loss: 0.2248\n",
            "Validation Loss: 0.2848\n",
            "\n",
            "Epoch 390\n",
            "-------------------------------\n",
            "Training Loss: 0.2275\n",
            "Validation Loss: 0.2695\n",
            "\n",
            "Epoch 391\n",
            "-------------------------------\n",
            "Training Loss: 0.2307\n",
            "Validation Loss: 0.2645\n",
            "\n",
            "Epoch 392\n",
            "-------------------------------\n",
            "Training Loss: 0.2249\n",
            "Validation Loss: 0.2717\n",
            "\n",
            "Epoch 393\n",
            "-------------------------------\n",
            "Training Loss: 0.2306\n",
            "Validation Loss: 0.2601\n",
            "\n",
            "Epoch 394\n",
            "-------------------------------\n",
            "Training Loss: 0.2325\n",
            "Validation Loss: 0.3009\n",
            "\n",
            "Epoch 395\n",
            "-------------------------------\n",
            "Training Loss: 0.2390\n",
            "Validation Loss: 0.2791\n",
            "\n",
            "Epoch 396\n",
            "-------------------------------\n",
            "Training Loss: 0.2489\n",
            "Validation Loss: 0.2679\n",
            "\n",
            "Epoch 397\n",
            "-------------------------------\n",
            "Training Loss: 0.2243\n",
            "Validation Loss: 0.2666\n",
            "\n",
            "Epoch 398\n",
            "-------------------------------\n",
            "Training Loss: 0.2249\n",
            "Validation Loss: 0.2701\n",
            "\n",
            "Epoch 399\n",
            "-------------------------------\n",
            "Training Loss: 0.2236\n",
            "Validation Loss: 0.2636\n",
            "\n",
            "Epoch 400\n",
            "-------------------------------\n",
            "Training Loss: 0.2265\n",
            "Validation Loss: 0.2596\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_test_tensor.shape"
      ],
      "metadata": {
        "id": "SOLgg62S8gqn",
        "outputId": "46d976b5-ed5a-49aa-8190-5a6921901a18",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([457, 10])"
            ]
          },
          "metadata": {},
          "execution_count": 107
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    y_pred = model(X_test_tensor)\n",
        "    test_loss = loss_fn(y_pred, y_test_tensor.unsqueeze(1)).item()\n",
        "    print(f\"Test Loss: {test_loss}\")\n",
        "\n",
        "# Save the model\n",
        "torch.save(model.state_dict(), 'model_varyingGoal_cutoffLabels.pth')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wOQLkQuuygek",
        "outputId": "8b387ce9-b914-435f-d785-e176b7d4d569"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss: 0.2407832145690918\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print true and predicted values\n",
        "for true, pred in zip(y_test, y_pred.flatten()):\n",
        "    print(f\"True: {true}, Predicted: {pred.item()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_jsimULgsfGF",
        "outputId": "164ae5b5-1574-4e0b-8dde-4c7b724d9d48"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True: 2.6080872664629258, Predicted: 3.2285778522491455\n",
            "True: 0.823978691464602, Predicted: 0.7089773416519165\n",
            "True: 0.39565513623575005, Predicted: 0.40904441475868225\n",
            "True: 2.619610451603125, Predicted: 2.15590763092041\n",
            "True: 0.3217962259796415, Predicted: 0.4705003499984741\n",
            "True: 5.5278815456270936, Predicted: 5.442183017730713\n",
            "True: 1.0603261293826078, Predicted: 1.7497986555099487\n",
            "True: 1.2833907800028237, Predicted: 0.8010441064834595\n",
            "True: 0.2986843949229902, Predicted: 0.25182268023490906\n",
            "True: 6.298947937290897, Predicted: 6.8272600173950195\n",
            "True: 10.32787460071454, Predicted: 8.834748268127441\n",
            "True: 0.7983679453131569, Predicted: 0.4430859684944153\n",
            "True: 0.21777429147870514, Predicted: 0.33289965987205505\n",
            "True: 1.4368291428947917, Predicted: 0.9261153340339661\n",
            "True: 0.34171374314316105, Predicted: 0.4323623776435852\n",
            "True: 0.2762206279024149, Predicted: 0.37879711389541626\n",
            "True: 0.4782652084057518, Predicted: 0.40979093313217163\n",
            "True: 0.5289743733898675, Predicted: 0.4773462414741516\n",
            "True: 2.0800797931153197, Predicted: 1.9614187479019165\n",
            "True: 1.3300952950355838, Predicted: 1.6146879196166992\n",
            "True: 6.72559354408103, Predicted: 5.833242893218994\n",
            "True: 0.34282514638284856, Predicted: 0.41355228424072266\n",
            "True: 5.356748243640393, Predicted: 3.757157802581787\n",
            "True: 6.555150158853477, Predicted: 7.716948509216309\n",
            "True: 0.27966526981684153, Predicted: 0.2593880891799927\n",
            "True: 1.7921986312748603, Predicted: 1.9756360054016113\n",
            "True: 0.46262065428641774, Predicted: 0.5554137229919434\n",
            "True: 0.7014092113859847, Predicted: 0.4433688521385193\n",
            "True: 2.5603605994493055, Predicted: 2.8834104537963867\n",
            "True: 0.48656397512665706, Predicted: 0.5441414713859558\n",
            "True: 0.7254485781103782, Predicted: 0.40892231464385986\n",
            "True: 0.016093792129721864, Predicted: -0.06748536974191666\n",
            "True: 0.36271929548331944, Predicted: 0.40763533115386963\n",
            "True: 0.45074127235906614, Predicted: 0.34121087193489075\n",
            "True: 6.519027606224726, Predicted: 8.487873077392578\n",
            "True: 1.6586682940845012, Predicted: 1.276815414428711\n",
            "True: 2.1219024480639965, Predicted: 1.0338743925094604\n",
            "True: 2.4801282176738915, Predicted: 1.9448074102401733\n",
            "True: 4.013831873409414, Predicted: 3.68577241897583\n",
            "True: 1.7597703275143788, Predicted: 1.5173684358596802\n",
            "True: 0.5951083440997922, Predicted: 0.7190138697624207\n",
            "True: 0.7655097601206686, Predicted: 1.553135871887207\n",
            "True: 0.9370903467287645, Predicted: 0.7141856551170349\n",
            "True: 0.917299422758562, Predicted: 0.8516563177108765\n",
            "True: 0.6201324720483498, Predicted: 0.5331282019615173\n",
            "True: 0.604289147667861, Predicted: 0.569603443145752\n",
            "True: 9.650298870652435, Predicted: 9.565726280212402\n",
            "True: 3.277217980185424, Predicted: 2.5942187309265137\n",
            "True: 0.4974404604976598, Predicted: 0.4561930000782013\n",
            "True: 0.1257770327357534, Predicted: 0.1734803020954132\n",
            "True: 1.4148762231301772, Predicted: 1.958202600479126\n",
            "True: 6.034251794212131, Predicted: 6.068480491638184\n",
            "True: 0.38968299821401176, Predicted: 0.3149774670600891\n",
            "True: 1.888568697727739, Predicted: 1.1637040376663208\n",
            "True: 0.7866441285389377, Predicted: 0.687366247177124\n",
            "True: 3.1190596579531418, Predicted: 2.7195088863372803\n",
            "True: 4.615672299989627, Predicted: 4.575704097747803\n",
            "True: 8.778711102126035, Predicted: 7.728977680206299\n",
            "True: 0.37188688327002606, Predicted: 0.3365916311740875\n",
            "True: 0.5252448893192744, Predicted: 0.4665290117263794\n",
            "True: 4.098515501346583, Predicted: 3.8763296604156494\n",
            "True: 0.4253953476337878, Predicted: 0.37986424565315247\n",
            "True: 0.27407599492800333, Predicted: 0.42106449604034424\n",
            "True: 0.21379291365915434, Predicted: 0.20214636623859406\n",
            "True: 0.921446799299354, Predicted: 0.35691002011299133\n",
            "True: 0.7030464524266694, Predicted: 0.9362079501152039\n",
            "True: 1.0914418491515803, Predicted: 0.8340540528297424\n",
            "True: 12.178444507892483, Predicted: 10.520452499389648\n",
            "True: 0.78755744682835, Predicted: 0.837528645992279\n",
            "True: 0.8894738730114062, Predicted: 0.6897955536842346\n",
            "True: 0.3933914265258784, Predicted: 0.3271799087524414\n",
            "True: 0.4386830658527579, Predicted: 0.30140677094459534\n",
            "True: 3.531736553959406, Predicted: 3.8582639694213867\n",
            "True: 1.877043873068597, Predicted: 2.137503147125244\n",
            "True: 0.1238177677114375, Predicted: 0.18592974543571472\n",
            "True: 0.06369761203482724, Predicted: 0.1477353572845459\n",
            "True: 0.10277548737967974, Predicted: 0.15793177485466003\n",
            "True: 0.6377346820403845, Predicted: 0.8611335158348083\n",
            "True: 0.8928198274480068, Predicted: 0.7794405221939087\n",
            "True: 7.273929799405671, Predicted: 6.380222797393799\n",
            "True: 1.5867301265742135, Predicted: 0.8061167597770691\n",
            "True: 1.0171371228246624, Predicted: 1.0036380290985107\n",
            "True: 7.826032744804788, Predicted: 7.823141574859619\n",
            "True: 0.5544983416548567, Predicted: 0.5485674142837524\n",
            "True: 0.4740908593407506, Predicted: 1.1094609498977661\n",
            "True: 0.7208855365487183, Predicted: 0.7727397084236145\n",
            "True: 0.5508599704512879, Predicted: 0.5193220973014832\n",
            "True: 1.4231348339935457, Predicted: 0.9881178140640259\n",
            "True: 0.10164204602739246, Predicted: 0.1535697877407074\n",
            "True: 0.348437308953761, Predicted: 0.5687211155891418\n",
            "True: 3.1705473076011796, Predicted: 2.614259958267212\n",
            "True: 0.5261375500445823, Predicted: 0.40150293707847595\n",
            "True: 0.5522115702154954, Predicted: 0.5742862820625305\n",
            "True: 0.6121237438378068, Predicted: 0.6837348937988281\n",
            "True: 7.420046997912816, Predicted: 7.5918684005737305\n",
            "True: 1.1115046184474453, Predicted: 0.9638493657112122\n",
            "True: 1.5989514471438984, Predicted: 2.281477928161621\n",
            "True: 3.460368487142557, Predicted: 3.922691822052002\n",
            "True: 1.8805527083392124, Predicted: 1.824316382408142\n",
            "True: 5.935814767657263, Predicted: 7.416962623596191\n",
            "True: 5.926716090058145, Predicted: 5.680624961853027\n",
            "True: 0.2847864386002408, Predicted: 0.3503704369068146\n",
            "True: 0.36253195014659706, Predicted: 0.46541982889175415\n",
            "True: 4.7495344795775845, Predicted: 4.4281511306762695\n",
            "True: 0.30586581457140777, Predicted: 0.3999636471271515\n",
            "True: 0.7959859970917229, Predicted: 1.0029340982437134\n",
            "True: 4.551618440145555, Predicted: 5.152766227722168\n",
            "True: 0.3302157969170277, Predicted: 0.3345949649810791\n",
            "True: 0.7271751213730306, Predicted: 0.827947199344635\n",
            "True: 1.755772908948677, Predicted: 2.229414701461792\n",
            "True: 0.17745339846663144, Predicted: 0.19021406769752502\n",
            "True: 0.8566769354549517, Predicted: 0.46674174070358276\n",
            "True: 1.4253393571739592, Predicted: 1.5723795890808105\n",
            "True: 4.4076352401491095, Predicted: 4.406156539916992\n",
            "True: 0.3268100351770597, Predicted: 0.4583108127117157\n",
            "True: 2.463434896958805, Predicted: 2.177091598510742\n",
            "True: 3.8271898260989383, Predicted: 2.931339979171753\n",
            "True: 0.6506426555528064, Predicted: 0.8477575182914734\n",
            "True: 7.905913616896944, Predicted: 8.416038513183594\n",
            "True: 3.8863050640590524, Predicted: 3.007930040359497\n",
            "True: 0.5584177487153956, Predicted: 0.29394465684890747\n",
            "True: 0.7500902044693787, Predicted: 0.8818578124046326\n",
            "True: 0.3784726933738766, Predicted: 0.2982766628265381\n",
            "True: 3.742874206553257, Predicted: 3.7159039974212646\n",
            "True: 8.633543347378204, Predicted: 8.284916877746582\n",
            "True: 6.33596762835445, Predicted: 5.637801170349121\n",
            "True: 5.156433676950083, Predicted: 5.024435043334961\n",
            "True: 0.872110757518417, Predicted: 1.350781798362732\n",
            "True: 0.492214047689169, Predicted: 0.5206185579299927\n",
            "True: 0.08141793108738235, Predicted: 0.0823904499411583\n",
            "True: 0.23372594589362497, Predicted: 0.8987789750099182\n",
            "True: 0.3911527210766154, Predicted: 0.8017256855964661\n",
            "True: 2.5741386955428722, Predicted: 4.310420036315918\n",
            "True: 1.8216600849776026, Predicted: 1.7637577056884766\n",
            "True: 0.9122097008306366, Predicted: 0.6920817494392395\n",
            "True: 0.45447234938440373, Predicted: 0.5637992024421692\n",
            "True: 1.7753340962495292, Predicted: 1.5675294399261475\n",
            "True: 3.3553647882774023, Predicted: 2.8286430835723877\n",
            "True: 0.5504808998339212, Predicted: 0.7264022827148438\n",
            "True: 0.04585346711082715, Predicted: 0.042860500514507294\n",
            "True: 0.03488860650152102, Predicted: 0.1537763774394989\n",
            "True: 0.6774721393061282, Predicted: 0.7829203009605408\n",
            "True: 0.1060565490313248, Predicted: 0.04708079248666763\n",
            "True: 2.462451238393568, Predicted: 3.282522439956665\n",
            "True: 0.24732707106419713, Predicted: 0.2640214264392853\n",
            "True: 7.1158946488486325, Predicted: 6.293066024780273\n",
            "True: 0.4296999368466086, Predicted: 0.7598730325698853\n",
            "True: 0.23435002133841065, Predicted: 0.5122130513191223\n",
            "True: 4.795745445901506, Predicted: 4.221407413482666\n",
            "True: 0.01188080436606221, Predicted: 0.25242868065834045\n",
            "True: 0.1763677620305316, Predicted: 0.39297574758529663\n",
            "True: 1.047900218953622, Predicted: 0.770393431186676\n",
            "True: 5.734998048650638, Predicted: 5.866403579711914\n",
            "True: 7.650740396317468, Predicted: 5.852025032043457\n",
            "True: 0.9171439963629344, Predicted: 0.7349547743797302\n",
            "True: 0.32317017951528126, Predicted: 0.7510961890220642\n",
            "True: 2.8016265977744976, Predicted: 3.0712332725524902\n",
            "True: 0.27100568824521654, Predicted: 0.37004712224006653\n",
            "True: 0.6547137796763975, Predicted: 0.43843144178390503\n",
            "True: 0.14200904299842249, Predicted: 0.12807539105415344\n",
            "True: 7.673453567680733, Predicted: 7.71148681640625\n",
            "True: 0.32074523170073843, Predicted: 0.4622070789337158\n",
            "True: 0.2670677019502313, Predicted: 0.45032769441604614\n",
            "True: 0.7950341721665205, Predicted: 0.5023809671401978\n",
            "True: 0.47783806714560684, Predicted: 0.41905704140663147\n",
            "True: 0.12794269000554623, Predicted: 1.1369365453720093\n",
            "True: 4.483037571453021, Predicted: 4.4594855308532715\n",
            "True: 0.4230515609992016, Predicted: 0.6149545311927795\n",
            "True: 0.5618452515408814, Predicted: 0.5558009147644043\n",
            "True: 5.198118581466679, Predicted: 4.03832483291626\n",
            "True: 2.579208647408413, Predicted: 1.6921533346176147\n",
            "True: 0.739341320483593, Predicted: 0.572742223739624\n",
            "True: 2.680801578787301, Predicted: 3.6465489864349365\n",
            "True: 1.7577397312750058, Predicted: 1.142104983329773\n",
            "True: 0.06428154662217578, Predicted: 0.1706291139125824\n",
            "True: 1.9397683530609808, Predicted: 1.7773493528366089\n",
            "True: 0.20773718834502308, Predicted: 0.2660934031009674\n",
            "True: 0.9458688766200776, Predicted: 1.9933741092681885\n",
            "True: 2.019525690370276, Predicted: 2.098172903060913\n",
            "True: 0.41946636137792237, Predicted: 0.5903869271278381\n",
            "True: 2.5372040077165776, Predicted: 2.3555846214294434\n",
            "True: 0.5550048977099282, Predicted: 0.6051151156425476\n",
            "True: 1.2521280771592902, Predicted: 1.28046715259552\n",
            "True: 2.4341738498342207, Predicted: 2.974661111831665\n",
            "True: 0.6250932744837504, Predicted: 0.5042325258255005\n",
            "True: 0.7444852600078037, Predicted: 0.535009503364563\n",
            "True: 0.5796724129611991, Predicted: 0.562423586845398\n",
            "True: 0.41447057247310626, Predicted: 0.7158470153808594\n",
            "True: 7.772103613514697, Predicted: 8.272473335266113\n",
            "True: 6.384162693881186, Predicted: 6.310390949249268\n",
            "True: 5.291888919817656, Predicted: 4.5041375160217285\n",
            "True: 0.359994770963619, Predicted: 0.2862073481082916\n",
            "True: 8.652074987998292, Predicted: 8.532221794128418\n",
            "True: 0.3511361263154865, Predicted: 0.23742496967315674\n",
            "True: 0.4980600865762325, Predicted: 0.5544074773788452\n",
            "True: 4.178885666225251, Predicted: 3.8015198707580566\n",
            "True: 4.95305595393685, Predicted: 6.380785942077637\n",
            "True: 0.4642382597938116, Predicted: 0.7518391609191895\n",
            "True: 1.5758097853795636, Predicted: 1.9177242517471313\n",
            "True: 0.23765814065152213, Predicted: 0.326322078704834\n",
            "True: 3.1900910443281236, Predicted: 3.2562429904937744\n",
            "True: 2.266627454645012, Predicted: 2.133251667022705\n",
            "True: 0.5388612451675279, Predicted: 0.5491178035736084\n",
            "True: 1.3073677143196103, Predicted: 0.8092517852783203\n",
            "True: 0.3722652897729649, Predicted: 0.4078516662120819\n",
            "True: 0.5738575448002738, Predicted: 0.467090368270874\n",
            "True: 9.165118396740253, Predicted: 9.154993057250977\n",
            "True: 3.1102287213898103, Predicted: 4.5240159034729\n",
            "True: 4.6682406762455395, Predicted: 3.6700217723846436\n",
            "True: 0.5684389935631771, Predicted: 0.9792346358299255\n",
            "True: 0.2717701230283274, Predicted: 0.26902949810028076\n",
            "True: 0.45400844398069484, Predicted: 0.4824944734573364\n",
            "True: 0.27720281452333373, Predicted: 0.5704006552696228\n",
            "True: 4.0473826044518475, Predicted: 4.291629791259766\n",
            "True: 0.16773362745302065, Predicted: 0.16853128373622894\n",
            "True: 5.545894509756898, Predicted: 5.681743621826172\n",
            "True: 0.4742886448007765, Predicted: 0.5357794165611267\n",
            "True: 5.311796003142577, Predicted: 5.431057929992676\n",
            "True: 0.6438945241436527, Predicted: 0.5980254411697388\n",
            "True: 0.06632133722441884, Predicted: -0.007713846862316132\n",
            "True: 0.1910378496822164, Predicted: 0.28466084599494934\n",
            "True: 0.015457160326099367, Predicted: 0.35877150297164917\n",
            "True: 0.3245105832060602, Predicted: 0.3212893009185791\n",
            "True: 0.25944569684178886, Predicted: 0.3384975492954254\n",
            "True: 1.2673740358151349, Predicted: 0.9455417990684509\n",
            "True: 0.5037087179728736, Predicted: 0.4437471032142639\n",
            "True: 0.6505586724839267, Predicted: 0.4605242908000946\n",
            "True: 8.723543764184436, Predicted: 8.44559097290039\n",
            "True: 0.3940152896254502, Predicted: 0.3918582499027252\n",
            "True: 0.3770826245309036, Predicted: 0.4088532328605652\n",
            "True: 0.021924932362542865, Predicted: 0.14320385456085205\n",
            "True: 0.014667866144222396, Predicted: 0.005982302129268646\n",
            "True: 0.5150904826807163, Predicted: 0.5007001161575317\n",
            "True: 0.27452119182139, Predicted: 0.26815178990364075\n",
            "True: 0.11286570930078542, Predicted: 0.17572224140167236\n",
            "True: 0.990021284970368, Predicted: 0.36392179131507874\n",
            "True: 0.19459711098439114, Predicted: 0.2955697476863861\n",
            "True: 2.508327828964704, Predicted: 1.8781260251998901\n",
            "True: 0.07937013866373291, Predicted: 0.16684022545814514\n",
            "True: 0.30979154554349764, Predicted: 0.45296213030815125\n",
            "True: 1.6351470609865213, Predicted: 0.7414433360099792\n",
            "True: 1.890843839275265, Predicted: 1.4407541751861572\n",
            "True: 0.5104170506642892, Predicted: 0.31884530186653137\n",
            "True: 3.0986996791121326, Predicted: 3.56895112991333\n",
            "True: 0.9562679037918054, Predicted: 0.8868608474731445\n",
            "True: 2.411377772348293, Predicted: 2.1091222763061523\n",
            "True: 0.3174356762105891, Predicted: 0.25855278968811035\n",
            "True: 0.2293242170382274, Predicted: 0.3084005117416382\n",
            "True: 1.231395636745652, Predicted: 1.4917265176773071\n",
            "True: 0.7383237109553067, Predicted: 0.6930295825004578\n",
            "True: 0.6363592274947762, Predicted: 0.6261732578277588\n",
            "True: 0.5762158914977618, Predicted: 0.3924810290336609\n",
            "True: 2.6563873703233316, Predicted: 2.4855403900146484\n",
            "True: 3.3938342461220508, Predicted: 2.354642629623413\n",
            "True: 0.3443395880088983, Predicted: 0.29611992835998535\n",
            "True: 8.499817306591142, Predicted: 8.471965789794922\n",
            "True: 4.138569182314856, Predicted: 3.494900941848755\n",
            "True: 0.5801226854127713, Predicted: 0.6237576603889465\n",
            "True: 0.8124530105821072, Predicted: 0.6119357943534851\n",
            "True: 0.24669804424354713, Predicted: 0.34791088104248047\n",
            "True: 0.4282931433901902, Predicted: 0.44586673378944397\n",
            "True: 0.32615433838272384, Predicted: 0.2668706476688385\n",
            "True: 1.4795533016172346, Predicted: 1.2350752353668213\n",
            "True: 0.9634240583306815, Predicted: 1.218013882637024\n",
            "True: 0.5701580406480989, Predicted: 0.7792260646820068\n",
            "True: 6.864083696573075, Predicted: 5.8879852294921875\n",
            "True: 8.421479526745262, Predicted: 8.196660041809082\n",
            "True: 1.0376804895165577, Predicted: 1.0756652355194092\n",
            "True: 5.067573429325444, Predicted: 5.945110321044922\n",
            "True: 0.42925375731658055, Predicted: 0.44972220063209534\n",
            "True: 7.237764938988275, Predicted: 8.38308334350586\n",
            "True: 11.012222906364551, Predicted: 9.141275405883789\n",
            "True: 0.4162629461848537, Predicted: 0.41387099027633667\n",
            "True: 0.23162806405503436, Predicted: 0.2969456911087036\n",
            "True: 0.5814112285326063, Predicted: 0.5268065333366394\n",
            "True: 0.311747121381557, Predicted: 0.2630067467689514\n",
            "True: 0.9394692030566153, Predicted: 1.3691686391830444\n",
            "True: 0.1394025403860954, Predicted: 0.12172047048807144\n",
            "True: 0.3159476692306526, Predicted: 0.4694356918334961\n",
            "True: 7.395939433094057, Predicted: 7.586246967315674\n",
            "True: 0.17162809708323754, Predicted: 0.22022640705108643\n",
            "True: 0.6992696174266154, Predicted: 0.4159724712371826\n",
            "True: 0.02943765835530297, Predicted: 0.14506781101226807\n",
            "True: 1.6807867569001531, Predicted: 1.8286902904510498\n",
            "True: 3.0596283831177664, Predicted: 3.461853504180908\n",
            "True: 0.23769334919164867, Predicted: 0.27917829155921936\n",
            "True: 0.5754792952631094, Predicted: 0.4636051654815674\n",
            "True: 6.768460781618021, Predicted: 5.907283782958984\n",
            "True: 1.2641737327957434, Predicted: 1.6027523279190063\n",
            "True: 4.380346016850027, Predicted: 3.5732533931732178\n",
            "True: 0.5586425552712229, Predicted: 0.5183873176574707\n",
            "True: 1.9591593654261374, Predicted: 2.5071563720703125\n",
            "True: 0.34158334466885376, Predicted: 0.3451797664165497\n",
            "True: 0.10408348377543251, Predicted: 0.14784520864486694\n",
            "True: 0.802218173180324, Predicted: 1.0979371070861816\n",
            "True: 0.33578724932342263, Predicted: 0.3073454797267914\n",
            "True: 0.3593873154538628, Predicted: 0.4427855312824249\n",
            "True: 0.023974081953866445, Predicted: 0.31318604946136475\n",
            "True: 0.3832585509924623, Predicted: 0.34413763880729675\n",
            "True: 0.28424611524366017, Predicted: 0.5696414709091187\n",
            "True: 0.549838050714761, Predicted: 0.4550158381462097\n",
            "True: 0.2817622254748136, Predicted: 0.40087708830833435\n",
            "True: 3.190870604185516, Predicted: 3.4258248805999756\n",
            "True: 0.016145282196274643, Predicted: 0.4391120672225952\n",
            "True: 5.460434918330014, Predicted: 4.308373928070068\n",
            "True: 2.120067760136278, Predicted: 2.6890275478363037\n",
            "True: 0.6589577235923532, Predicted: 0.5676134824752808\n",
            "True: 0.1628716689333281, Predicted: 0.23575660586357117\n",
            "True: 1.4633967805171568, Predicted: 1.959558367729187\n",
            "True: 0.48933194436330313, Predicted: 0.618252158164978\n",
            "True: 0.43517207102650307, Predicted: 0.5415380597114563\n",
            "True: 0.3078521563580059, Predicted: 0.277319997549057\n",
            "True: 1.6778835491239612, Predicted: 1.2014845609664917\n",
            "True: 0.9946046624321114, Predicted: 1.040167212486267\n",
            "True: 0.014393389170933864, Predicted: 0.1253400444984436\n",
            "True: 0.3623665527761383, Predicted: 0.37997621297836304\n",
            "True: 2.705783712636352, Predicted: 3.982659339904785\n",
            "True: 0.8719504940653958, Predicted: 1.0030981302261353\n",
            "True: 5.650976254895869, Predicted: 6.445977210998535\n",
            "True: 0.07613353371254641, Predicted: 0.03866448253393173\n",
            "True: 0.2318411080950653, Predicted: 0.43123728036880493\n",
            "True: 8.601720929640896, Predicted: 8.374824523925781\n",
            "True: 1.0505324043432303, Predicted: 1.0052363872528076\n",
            "True: 0.21405418753495442, Predicted: 0.20994091033935547\n",
            "True: 5.861496306888451, Predicted: 5.985249042510986\n",
            "True: 0.6880641288923302, Predicted: 0.5299696326255798\n",
            "True: 0.10885628218693222, Predicted: 0.1931089162826538\n",
            "True: 0.33402678874907593, Predicted: 0.2713836431503296\n",
            "True: 0.3052184733738136, Predicted: 0.3565104603767395\n",
            "True: 0.5483414103517232, Predicted: 0.5477463603019714\n",
            "True: 7.8621855462675105, Predicted: 7.325106620788574\n",
            "True: 0.6389680145094762, Predicted: 0.9373368620872498\n",
            "True: 0.20123769290690768, Predicted: 0.3586265444755554\n",
            "True: 2.274625646228931, Predicted: 2.3253185749053955\n",
            "True: 2.4651771814149006, Predicted: 1.4288581609725952\n",
            "True: 0.30260424352761045, Predicted: 0.23224660754203796\n",
            "True: 6.498714540415576, Predicted: 6.29754638671875\n",
            "True: 0.5555049025323969, Predicted: 0.5481580495834351\n",
            "True: 0.4102674332450437, Predicted: 0.42639681696891785\n",
            "True: 0.17883344115737354, Predicted: 0.5184423327445984\n",
            "True: 9.268580191809022, Predicted: 8.265948295593262\n",
            "True: 5.976492774599905, Predicted: 3.4604029655456543\n",
            "True: 0.5972001381167925, Predicted: 0.6661596298217773\n",
            "True: 1.4628747093971972, Predicted: 0.7331132292747498\n",
            "True: 0.22462681894886066, Predicted: 0.42020753026008606\n",
            "True: 0.31243463403648875, Predicted: 0.6939104199409485\n",
            "True: 0.1433148233699647, Predicted: 0.20395419001579285\n",
            "True: 0.33321836844645975, Predicted: 0.2641616761684418\n",
            "True: 7.907385079440682, Predicted: 8.022143363952637\n",
            "True: 0.7323198104628478, Predicted: 1.3183292150497437\n",
            "True: 0.4478443867078187, Predicted: 0.5008923411369324\n",
            "True: 5.564982501418283, Predicted: 4.651431083679199\n",
            "True: 0.4997724703701285, Predicted: 0.27349716424942017\n",
            "True: 0.29344299685526504, Predicted: 0.5572779774665833\n",
            "True: 0.68446743480974, Predicted: 0.6387078166007996\n",
            "True: 0.3235498821167266, Predicted: 0.38036978244781494\n",
            "True: 6.759847485041613, Predicted: 7.755145072937012\n",
            "True: 0.5369200494081616, Predicted: 0.5978958010673523\n",
            "True: 0.2062727817316056, Predicted: 0.2782006859779358\n",
            "True: 5.395600227640953, Predicted: 6.252718925476074\n",
            "True: 4.000106147054663, Predicted: 4.111686706542969\n",
            "True: 0.6740189701816173, Predicted: 0.8996038436889648\n",
            "True: 9.920202529318846, Predicted: 10.023449897766113\n",
            "True: 0.43007262000045676, Predicted: 0.36419379711151123\n",
            "True: 4.03360707738004, Predicted: 3.419187068939209\n",
            "True: 0.7642047809058341, Predicted: 1.0173407793045044\n",
            "True: 0.3776903204627393, Predicted: 0.26891961693763733\n",
            "True: 0.5441284990202054, Predicted: 0.6181700229644775\n",
            "True: 0.42750582055253067, Predicted: 1.0856965780258179\n",
            "True: 9.029411057495153, Predicted: 9.393383026123047\n",
            "True: 2.576634592181107, Predicted: 2.502596378326416\n",
            "True: 0.2730935164114927, Predicted: 0.2959366738796234\n",
            "True: 0.46964028832071447, Predicted: 0.4935194253921509\n",
            "True: 1.1918183517448098, Predicted: 1.693153977394104\n",
            "True: 0.8371422256525299, Predicted: 0.558895468711853\n",
            "True: 3.6212789235480787, Predicted: 3.366297483444214\n",
            "True: 0.7143820222456088, Predicted: 0.4624197781085968\n",
            "True: 0.30264149510921146, Predicted: 0.24302613735198975\n",
            "True: 0.545783524309082, Predicted: 0.5745471119880676\n",
            "True: 0.2975413105473704, Predicted: 0.3608121871948242\n",
            "True: 3.9333729068296575, Predicted: 4.832193851470947\n",
            "True: 5.910539047933486, Predicted: 6.882330417633057\n",
            "True: 0.09797495787705554, Predicted: 0.04323076456785202\n",
            "True: 0.4261987908432577, Predicted: 0.7019633650779724\n",
            "True: 0.5980349809740279, Predicted: 0.6743550896644592\n",
            "True: 0.6068356812875833, Predicted: 0.8392901420593262\n",
            "True: 0.2217873484261854, Predicted: 1.2408992052078247\n",
            "True: 6.141026287707978, Predicted: 6.095033645629883\n",
            "True: 0.3210316501492994, Predicted: 0.3186385929584503\n",
            "True: 0.3492224579727155, Predicted: 0.3632212281227112\n",
            "True: 0.47641971801057803, Predicted: 0.427639365196228\n",
            "True: 0.9323372018169734, Predicted: 1.2599585056304932\n",
            "True: 0.0161848659688208, Predicted: 0.16057643294334412\n",
            "True: 2.2343507250446555, Predicted: 2.4152259826660156\n",
            "True: 0.07543554700136916, Predicted: 0.16806456446647644\n",
            "True: 0.42245316900654273, Predicted: 0.5374663472175598\n",
            "True: 0.22049503788928557, Predicted: 0.25285083055496216\n",
            "True: 1.1916709582690619, Predicted: 1.2551628351211548\n",
            "True: 1.8441109674431484, Predicted: 1.765252709388733\n",
            "True: 0.4134335520691875, Predicted: 0.39359012246131897\n",
            "True: 0.011753962981934398, Predicted: -0.02240677922964096\n",
            "True: 4.101948824887537, Predicted: 4.152399063110352\n",
            "True: 0.5647988907927999, Predicted: 0.5012969374656677\n",
            "True: 0.44101927396429375, Predicted: 0.3371341824531555\n",
            "True: 0.02905593552701697, Predicted: 0.20679181814193726\n",
            "True: 0.43268774256791775, Predicted: 0.38683027029037476\n",
            "True: 9.10430801539746, Predicted: 9.366965293884277\n",
            "True: 0.18204908185790183, Predicted: 0.4381755292415619\n",
            "True: 5.542372406521781, Predicted: 5.7677836418151855\n",
            "True: 0.19687397493047246, Predicted: 0.2509099245071411\n",
            "True: 0.47450520824029724, Predicted: 0.6179569363594055\n",
            "True: 2.7840544073041142, Predicted: 2.276982069015503\n",
            "True: 6.656267208426262, Predicted: 5.345102787017822\n",
            "True: 0.5707490963213414, Predicted: 0.6192008852958679\n",
            "True: 0.04838161231998245, Predicted: -0.00694245845079422\n",
            "True: 1.173575863552055, Predicted: 1.4428160190582275\n",
            "True: 0.5428790947881956, Predicted: 0.426305890083313\n",
            "True: 0.29688904259421256, Predicted: 0.22370201349258423\n",
            "True: 0.7394955413958528, Predicted: 0.5184251070022583\n",
            "True: 6.098132903752533, Predicted: 6.272862911224365\n",
            "True: 8.369126297234786, Predicted: 8.525605201721191\n",
            "True: 0.868478798387301, Predicted: 1.7792774438858032\n",
            "True: 0.21167787947159222, Predicted: 0.32775384187698364\n",
            "True: 3.9012741176920875, Predicted: 2.7867300510406494\n",
            "True: 7.834339071209581, Predicted: 6.999338150024414\n",
            "True: 0.7151954712384339, Predicted: 0.633410632610321\n",
            "True: 1.354866657823698, Predicted: 3.4078056812286377\n",
            "True: 0.7711419785563941, Predicted: 0.5643315315246582\n",
            "True: 0.24629795338449656, Predicted: 0.18275943398475647\n",
            "True: 0.6681102008548911, Predicted: 0.4060271084308624\n",
            "True: 0.37154050706856123, Predicted: 0.38744473457336426\n",
            "True: 0.6661300421233657, Predicted: 0.7086588144302368\n",
            "True: 0.1506565550694644, Predicted: 0.20184026658535004\n",
            "True: 7.230983812694452, Predicted: 7.383820533752441\n",
            "True: 4.537800519184028, Predicted: 3.779594659805298\n",
            "True: 0.43134955925381147, Predicted: 0.3381761610507965\n",
            "True: 8.91237242555366, Predicted: 8.139183044433594\n",
            "True: 1.0176777315127017, Predicted: 1.1392711400985718\n",
            "True: 0.38784556979091783, Predicted: 0.3662679195404053\n",
            "True: 6.213754905285402, Predicted: 5.4117512702941895\n",
            "True: 7.937345197697379, Predicted: 8.980147361755371\n",
            "True: 0.3019808954154102, Predicted: 0.2250659167766571\n",
            "True: 0.44653830838504444, Predicted: 0.5433369278907776\n",
            "True: 6.795031352942387, Predicted: 6.252840042114258\n",
            "True: 0.31374450243185076, Predicted: 0.6058172583580017\n",
            "True: 0.5449599903907199, Predicted: 0.35086551308631897\n",
            "True: 1.1038499006302214, Predicted: 1.245282530784607\n",
            "True: 0.9707365345239266, Predicted: 0.6961311101913452\n",
            "True: 0.2770578891388955, Predicted: 0.21351835131645203\n",
            "True: 0.7016104380854787, Predicted: 0.560637354850769\n",
            "True: 1.6994064917668554, Predicted: 1.4204941987991333\n",
            "True: 5.887844773610347, Predicted: 4.8632917404174805\n",
            "True: 0.18216342984249748, Predicted: 0.2205028235912323\n",
            "True: 0.7244027921144647, Predicted: 0.3610522449016571\n",
            "True: 0.301413890503669, Predicted: 0.3706567883491516\n",
            "True: 0.45179245115348393, Predicted: 0.47533899545669556\n",
            "True: 2.65253497234828, Predicted: 2.514197826385498\n",
            "True: 6.463279004834853, Predicted: 6.414310932159424\n",
            "True: 0.2655814260007997, Predicted: 0.25974801182746887\n",
            "True: 5.545561828993321, Predicted: 4.246785640716553\n",
            "True: 0.6583709663570715, Predicted: 0.4491458237171173\n",
            "True: 2.3600617830941055, Predicted: 2.62379789352417\n",
            "True: 4.59693948623168, Predicted: 5.115748405456543\n",
            "True: 0.38704619570973775, Predicted: 0.39572614431381226\n",
            "True: 1.6235487408061382, Predicted: 2.4412591457366943\n",
            "True: 0.39198609177853944, Predicted: 0.5159480571746826\n",
            "True: 0.45744691825517003, Predicted: 0.7280126810073853\n",
            "True: 1.2305197097041285, Predicted: 3.2917323112487793\n",
            "True: 0.012687766298003729, Predicted: 0.05076330155134201\n",
            "True: 0.837471381856336, Predicted: 0.5510823130607605\n",
            "True: 0.3971700974790521, Predicted: 0.2528872489929199\n",
            "True: 0.6366242684435268, Predicted: 0.6332032084465027\n",
            "True: 0.019133184756798954, Predicted: 0.1426694393157959\n",
            "True: 0.33838708041516063, Predicted: 0.33841148018836975\n",
            "True: 0.2909763410127158, Predicted: 0.4714817404747009\n",
            "True: 0.33251855068109143, Predicted: 0.24873778223991394\n",
            "True: 2.1069609342022986, Predicted: 1.8796254396438599\n",
            "True: 0.3527651784821514, Predicted: 0.26661843061447144\n",
            "True: 6.096267891171412, Predicted: 5.653647422790527\n",
            "True: 0.43743756921731136, Predicted: 0.44173291325569153\n",
            "True: 0.24209164193906638, Predicted: 0.2539878189563751\n",
            "True: 0.09529477298828057, Predicted: 0.2368023693561554\n",
            "True: 0.8665004090869572, Predicted: 1.5619266033172607\n",
            "True: 1.819068140013912, Predicted: 1.612460732460022\n",
            "True: 1.3751869386617392, Predicted: 1.1524447202682495\n",
            "True: 1.7969527272408679, Predicted: 0.8268331289291382\n",
            "True: 0.41886223115151067, Predicted: 0.5040059089660645\n",
            "True: 0.4806584759718718, Predicted: 0.7779392600059509\n",
            "True: 0.5605732578449811, Predicted: 0.6754124164581299\n",
            "True: 0.3475032305166691, Predicted: 0.5986098051071167\n",
            "True: 0.3237027363767469, Predicted: 0.5839894413948059\n",
            "True: 0.5087220656834543, Predicted: 0.38610130548477173\n",
            "True: 0.7401451278645234, Predicted: 1.0378978252410889\n",
            "True: 0.6036191852089895, Predicted: 0.5551427602767944\n",
            "True: 0.47177535745572485, Predicted: 0.5153226256370544\n",
            "True: 0.35332956582064534, Predicted: 0.3418176770210266\n",
            "True: 0.49420292056904125, Predicted: 0.40808266401290894\n",
            "True: 0.40457573277917774, Predicted: 0.2644968628883362\n",
            "True: 2.5738962644258883, Predicted: 2.718700408935547\n",
            "True: 4.259161199647604, Predicted: 4.088826656341553\n",
            "True: 0.3731632698440007, Predicted: 0.31687450408935547\n",
            "True: 0.5102725457482303, Predicted: 0.5984683036804199\n",
            "True: 0.38143661630767567, Predicted: 0.36218613386154175\n",
            "True: 4.64840785388265, Predicted: 4.352787971496582\n",
            "True: 0.28693894826752214, Predicted: 0.26773086190223694\n",
            "True: 7.878929689405402, Predicted: 8.19542407989502\n",
            "True: 0.31092330971977544, Predicted: 0.3350825309753418\n",
            "True: 1.9831306688409827, Predicted: 1.6752583980560303\n",
            "True: 5.051955921699458, Predicted: 2.634552478790283\n",
            "True: 0.017548110975476573, Predicted: 0.1327655017375946\n",
            "True: 0.9971196697972191, Predicted: 0.7828684449195862\n",
            "True: 2.3356492787658576, Predicted: 2.4852004051208496\n",
            "True: 0.6181817181952264, Predicted: 0.39091891050338745\n",
            "True: 0.5956216940857346, Predicted: 0.7155197858810425\n",
            "True: 11.561916386688027, Predicted: 11.002760887145996\n",
            "True: 0.015929328725817163, Predicted: -0.055764757096767426\n",
            "True: 0.35244653312923185, Predicted: 0.25759270787239075\n",
            "True: 0.6699111682285444, Predicted: 0.5692698359489441\n",
            "True: 3.3486995778959736, Predicted: 3.2912347316741943\n",
            "True: 0.36482815098370824, Predicted: 0.41553041338920593\n",
            "True: 0.6867108844994041, Predicted: 0.5780154466629028\n",
            "True: 0.7890415664198007, Predicted: 1.0411149263381958\n",
            "True: 0.2673537606348456, Predicted: 0.35014617443084717\n",
            "True: 2.0879559818471836, Predicted: 1.0481019020080566\n",
            "True: 4.450396307854343, Predicted: 3.749037504196167\n",
            "True: 8.788853724725003, Predicted: 7.587516784667969\n",
            "True: 5.118183786634325, Predicted: 5.057870864868164\n",
            "True: 1.6602217499747947, Predicted: 2.061870813369751\n",
            "True: 7.587898270261023, Predicted: 6.078176021575928\n",
            "True: 8.793530050725902, Predicted: 10.318353652954102\n",
            "True: 2.3060621642348123, Predicted: 2.704327344894409\n",
            "True: 0.3389343960835894, Predicted: 0.35276371240615845\n",
            "True: 0.43237863097306867, Predicted: 0.4344788193702698\n",
            "True: 0.19824695470306208, Predicted: 0.288263201713562\n",
            "True: 4.4694873890604185, Predicted: 3.662137508392334\n",
            "True: 4.048729080303094, Predicted: 2.8021960258483887\n",
            "True: 9.08318697076986, Predicted: 8.535650253295898\n",
            "True: 0.24013942865124094, Predicted: 0.32506632804870605\n",
            "True: 5.707777498069502, Predicted: 6.099903583526611\n",
            "True: 0.2036034102793996, Predicted: 0.20263098180294037\n",
            "True: 0.3788361066779336, Predicted: 0.3113746643066406\n",
            "True: 1.3526630864661209, Predicted: 1.7267297506332397\n",
            "True: 2.701849282924714, Predicted: 2.933342456817627\n",
            "True: 0.2012377691870522, Predicted: 0.22674661874771118\n",
            "True: 0.16541259327170232, Predicted: 0.23836767673492432\n",
            "True: 0.07618523658722813, Predicted: 0.0006879344582557678\n"
          ]
        }
      ]
    }
  ]
}